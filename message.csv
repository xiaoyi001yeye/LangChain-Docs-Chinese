msgid,msgstr
Deployments,部署
So you've made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?,所以你已经做了一个非常酷的链条——现在呢？您如何部署它并使其易于与全世界共享？
"This section covers several options for that. Note that these are meant as quick deployment options for prototypes and demos, and not for production systems. If you are looking for help with deployment of a production system, please contact us directly.",本节将介绍这方面的几个选项。请注意，这些是原型和演示的快速部署选项，而不是生产系统。如果您正在寻求生产系统部署方面的帮助，请直接联系我们。
"What follows is a list of template GitHub repositories aimed that are intended to be very easy to fork and modify to use your chain. This is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.",下面是一个模板GitHub存储库列表，旨在非常容易地派生和修改以使用您的链。这远不是一份详尽的选择清单，我们非常愿意在这里发表意见。
[Streamlit](https://github.com/hwchase17/langchain-streamlit-template),[Streamlit](https://github.com/hwchase 17/langchain-streamlit-template）
This repo serves as a template for how to deploy a LangChain with Streamlit. It implements a chatbot interface. It also contains instructions for how to deploy this app on the Streamlit platform.,这个repo作为如何使用Streamlit部署LangChain的模板。它实现了一个聊天机器人界面。它还包含如何在Streamlit平台上部署此应用程序的说明。
[Gradio (on Hugging Face)](https://github.com/hwchase17/langchain-gradio-template),[Gradio（拥抱脸）](https://github.com/hwchase 17/langchain-gradio-template）
"This repo serves as a template for how deploy a LangChain with Gradio. It implements a chatbot interface, with a ""Bring-Your-Own-Token"" approach (nice for not wracking up big bills). It also contains instructions for how to deploy this app on the Hugging Face platform. This is heavily influenced by James Weaver's [excellent examples](https://huggingface.co/JavaFXpert).",这个repo作为如何使用Gradio部署LangChain的模板。它实现了一个聊天机器人界面，采用了“自带令牌”的方法（这对于避免大笔账单来说很好）。它还包含如何在拥抱脸平台上部署该应用程序的说明。这在很大程度上受到了James Weaver的[优秀示例]（https://huggingface.co/JavaFXpert）的影响。
[Beam](https://github.com/slai-labs/get-beam/tree/main/examples/langchain-question-answering),[Beam](https://github.com/slai-labs/get-beam/tree/main/examples/langchain-question-answering）
This repo serves as a template for how deploy a LangChain with [Beam](https://beam.cloud).,这个repo作为如何使用【Beam】（https：//Beam）部署LangChain的模板。云）。
It implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.,它实现了一个问答应用程序，并包含将该应用程序部署为无服务器REST API的说明。
[Vercel](https://github.com/homanp/vercel-langchain),[Vercel](https://github.com/homanp/vercel-langchain）
A minimal example on how to run LangChain on Vercel using Flask.,关于如何使用Flask在Vercel上运行LangChain的最小示例。
[Fly.io](https://github.com/fly-apps/hello-fly-langchain),[Fly.io](https://github.com/fly-apps/hello-fly-langchain）
A minimal example of how to deploy LangChain to [Fly.io](https://fly.io/) using Flask.,如何使用Flask将LangChain部署到【fly.io】（https：//fly.io/）的最小示例。
[Digitalocean App Platform](https://github.com/homanp/digitalocean-langchain),[Digitalocean应用平台](https://github.com/homanp/digitalocean-langchain）
A minimal example on how to deploy LangChain to DigitalOcean App Platform.,如何将LangChain部署到DigitalOcean应用平台的最小示例。
[Google Cloud Run](https://github.com/homanp/gcp-langchain),[Google Cloud Run](https://github.com/homanp/gcp-langchain）
A minimal example on how to deploy LangChain to Google Cloud Run.,一个关于如何将LangChain部署到Google Cloud Run的最小示例。
[SteamShip](https://github.com/steamship-core/steamship-langchain/),[SteamShip](https://github.com/steamship-core/steamship-langchain/）
"This repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship. This includes: production ready endpoints, horizontal scaling across dependencies, persistant storage of app state, multi-tenancy support, etc.",这个存储库包含用于Steamship的LangChain适配器，使LangChain开发人员能够在Steamship上快速部署他们的应用程序。这包括：生产就绪端点、跨依赖项的水平扩展、应用状态的持久存储、多租户支持等。
[Langchain-serve](https://github.com/jina-ai/langchain-serve),[Langchain-serve](https://github.com/jina-ai/Langchain-serve）
"This repository allows users to serve local chains and agents as RESTful, gRPC, or Websocket APIs thanks to [Jina](https://docs.jina.ai/). Deploy your chains & agents with ease and enjoy independent scaling, serverless and autoscaling APIs, as well as a Streamlit playground on Jina AI Cloud.",由于【纪娜】（https：//docs），这个存储库允许用户将本地链和代理作为RESTful、gRPC或Websocket APIs提供服务。纪娜。ai/）。轻松部署您的链和代理，享受独立扩展、无服务器和自动扩展API，以及纪娜人工智能云上的Streamlit游乐场。
[BentoML](https://github.com/ssheng/BentoChain),[BentoML](https://github.com/ssheng/BentoChain）
"This repository provides an example of how to deploy a LangChain application with [BentoML](https://github.com/bentoml/BentoML). BentoML is a framework that enables the containerization of machine learning applications as standard OCI images. BentoML also allows for the automatic generation of OpenAPI and gRPC endpoints. With BentoML, you can integrate models from all popular ML frameworks and deploy them as microservices running on the most optimal hardware and scaling independently.",这个存储库提供了一个如何使用【BentoML】（https：//github.com/BentoML/BentoML）部署LangChain应用程序的示例。BentoML是一个框架，能够将机器学习应用程序容器化为标准的OCI图像。BentoML还允许自动生成OpenAPI和gRPC端点。使用BentoML，您可以集成所有流行的ML框架中的模型，并将它们部署为运行在最佳硬件上并独立扩展的微服务。
[Databutton](https://databutton.com/home?new-data-app=true),[Databutton](https://databutton.com/home？new-data-app=true）
"These templates serve as examples of how to build, deploy, and share LangChain applications using Databutton. You can create user interfaces with Streamlit, automate tasks by scheduling Python code, and store files and data in the built-in store. Examples include Chatbot interface with conversational memory, Personal search engine, and a starter template for LangChain apps. Deploying and sharing is one click.",这些模板是如何使用Databutton构建、部署和共享LangChain应用程序的示例。您可以使用Streamlit创建用户界面，通过调度Python代码来自动化任务，并在内置存储中存储文件和数据。例子包括带有对话记忆的聊天机器人界面、个人搜索引擎和LangChain应用程序的入门模板。部署和共享一键完成。
LangChain Ecosystem,LangChain生态系统
Guides for how other companies/products can be used with LangChain,其他公司/产品如何与LangChain配合使用的指南
Groups,组
LangChain provides integration with many LLMs and systems:,LangChain提供与许多LLM和系统的集成：
`LLM Providers <./modules/models/llms/integrations.html>`_,`LLM提供程序<./modules/models/llms/integrations.html>`_
`Chat Model Providers <./modules/models/chat/integrations.html>`_,`聊天模型提供程序<./modules/models/chat/integrations.html>`_
`Text Embedding Model Providers <./modules/models/text_embedding.html>`_,`文本嵌入模型提供程序<./modules/models/text_embedding.html>`_
`Document Loader Integrations <./modules/indexes/document_loaders.html>`_,`文档加载器集成<./modules/indexes/document_loaders.html>`_
`Text Splitter Integrations <./modules/indexes/text_splitters.html>`_,`文本拆分器集成<./modules/indexes/text_splitters.html>`_
`Vectorstore Providers <./modules/indexes/vectorstores.html>`_,`Vectorstore提供程序<./modules/indexes/vectorstores.html>`_
`Retriever Providers <./modules/indexes/retrievers.html>`_,`检索器提供程序<./modules/indexes/retrievers.html>`_
`Tool Providers <./modules/agents/tools.html>`_,`工具提供程序<./modules/agents/tools.html>`_
`Toolkit Integrations <./modules/agents/toolkits.html>`_,`Toolkit集成<./modules/agents/toolkits.html>`_
Companies / Products,公司╱产品
AI21 Labs,AI 21实验室
"This page covers how to use the AI21 ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific AI21 wrappers.",本页介绍了如何在LangChain中使用AI 21生态系统。它分为两部分：安装和设置，然后引用特定的AI 21包装器。
Installation and Setup,安装和设置
Get an AI21 api key and set it as an environment variable (`AI21_API_KEY`),获取AI 21 api密钥并将其设置为环境变量（`AI 21_API_KEY`）
Wrappers,包装纸
LLM,法学硕士
"There exists an AI21 LLM wrapper, which you can access with",有一个AI 21 LLM包装器，您可以使用
Aim,瞄准
"Aim makes it super easy to visualize and debug LangChain executions. Aim tracks inputs and outputs of LLMs and tools, as well as actions of agents.",Aim使得可视化和调试LangChain执行变得非常容易。Aim跟踪LLM和工具的输入和输出，以及代理的操作。
"With Aim, you can easily debug and examine an individual execution:",使用Aim，您可以轻松地调试和检查单个执行：
![](https://user-images.githubusercontent.com/13848158/227784778-06b806c7-74a1-4d15-ab85-9ece09b458aa.png),！[](https://user-images.githubusercontent.com/13848158/227784778-06 b 806 c 7-74 a 1-4 d 15-ab 85-9 ece 09 b 458 aa.png）
"Additionally, you have the option to compare multiple executions side by side:",此外，您还可以选择并排比较多个执行：
![](https://user-images.githubusercontent.com/13848158/227784994-699b24b7-e69b-48f9-9ffa-e6a6142fd719.png),！[](https://user-images.githubusercontent.com/13848158/227784994-699 b 24 b 7-e 69 b-48 f 9-9 ffa-e 6 a 6142 fd 719.png）
"Aim is fully open source, [learn more](https://github.com/aimhubio/aim) about Aim on GitHub.",Aim是完全开源的，【了解更多】（https：//github.com/aimhubio/Aim）关于GitHub上的Aim。
Let's move forward and see how to enable and configure Aim callback.,让我们继续，看看如何启用和配置Aim回调。
"In this notebook we will explore three usage scenarios. To start off, we will install the necessary packages and import certain modules. Subsequently, we will configure two environment variables that can be established either within the Python script or through the terminal.",在本笔记本中，我们将探讨三种使用场景。首先，我们将安装必要的软件包并导入某些模块。随后，我们将配置两个可以在Python脚本中或通过终端建立的环境变量。
"Our examples use a GPT model as the LLM, and OpenAI offers an API for this purpose. You can obtain the key from the following link: https://platform.openai.com/account/api-keys .",我们的例子使用GPT模型作为LLM，OpenAI为此提供了一个API。您可以从以下链接获取密钥：https://platform.openai.com/account/api-keys。
"We will use the SerpApi to retrieve search results from Google. To acquire the SerpApi key, please go to https://serpapi.com/manage-api-key .",我们将使用SerpApi从Google检索搜索结果。要获取SerpApi密钥，请访问https://serpapi.com/manage-api-key。
"The event methods of `AimCallbackHandler` accept the LangChain module or agent as input and log at least the prompts and generated results, as well as the serialized version of the LangChain module, to the designated Aim run.",“AimCallbackHandler”的事件方法接受LangChain模块或代理作为输入，并至少将提示和生成的结果以及LangChain模块的序列化版本记录到指定的Aim运行中。
"The `flush_tracker` function is used to record LangChain assets on Aim. By default, the session is reset rather than being terminated outright.",“flush_tracker”函数用于记录Aim上的LangChain资产。默认情况下，会话会被重置，而不是直接终止。
AnalyticDB,分析数据库
This page covers how to use the AnalyticDB ecosystem within LangChain.,本页介绍了如何在LangChain中使用AnalyticDB生态系统。
VectorStore,向量存储
"There exists a wrapper around AnalyticDB, allowing you to use it as a vectorstore, whether for semantic search or example selection.",AnalyticDB周围有一个包装器，允许您将其用作vectorstore，无论是用于语义搜索还是示例选择。
To import this vectorstore:,要导入此向量存储：
"For a more detailed walkthrough of the AnalyticDB wrapper, see [this notebook](../modules/indexes/vectorstores/examples/analyticdb.ipynb)",有关AnalyticDB包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/analyticdb.ipynb）
Apify,增高
This page covers how to use [Apify](https://apify.com) within LangChain.,本页介绍如何在LangChain中使用[Apify]（https：//apify.com）。
Overview,概览
"Apify is a cloud platform for web scraping and data extraction, which provides an [ecosystem](https://apify.com/store) of more than a thousand ready-made apps called *Actors* for various scraping, crawling, and extraction use cases.",Apify是一个用于web抓取和数据提取的云平台，它提供了一个【生态系统】（https：//apify.com/store）的一千多个名为*Actors*的现成应用程序，用于各种抓取、抓取和提取用例。
[![Apify Actors](../_static/ApifyActors.png)](https://apify.com/store),[！[Apify Actors](../_static/ApifyActors.png)]（https：//apify.com/store）
Apify Actors,Apify演员
"This integration enables you run Actors on the Apify platform and load their results into LangChain to feed your vector indexes with documents and data from the web, e.g. to generate answers from websites with documentation, blogs, or knowledge bases.",这种集成使您能够在Apify平台上运行Actors，并将其结果加载到LangChain中，以向量索引提供来自web的文档和数据，例如，从带有文档、博客或知识库的网站生成答案。
Install the Apify API client for Python with `pip install apify-client`,使用“pip install apify-client”为Python安装Apify API客户端
Get your [Apify API token](https://console.apify.com/account/integrations) and either set it as an environment variable (`APIFY_API_TOKEN`) or pass it to the `ApifyWrapper` as `apify_api_token` in the constructor.,获取您的[Apify API token](https://console.apify.com/account/integrations)，并将其设置为环境变量(`apify_api_token`)，或者将其作为构造函数中的`apify_api_token`传递给‘ApifyWrapper’。
Utility,效用
You can use the `ApifyWrapper` to run Actors on the Apify platform.,您可以使用“ApifyWrapper”在Apify平台上运行参与者。
"For a more detailed walkthrough of this wrapper, see [this notebook](../modules/agents/tools/examples/apify.ipynb).",有关此包装器的更详细演练，请参见【本笔记本】（../模块/代理/工具/示例/apify.ipynb）。
Loader,装载机
You can also use our `ApifyDatasetLoader` to get data from Apify dataset.,您也可以使用我们的“ApifyDatasetLoader”从Apify数据集中获取数据。
"For a more detailed walkthrough of this loader, see [this notebook](../modules/indexes/document_loaders/examples/apify_dataset.ipynb).",有关此加载程序的更详细演练，请参见[本笔记本]（../modules/indexes/document_loaders/examples/apify_dataset.ipynb）。
AtlasDB,AtlasDB
"This page covers how to use Nomic's Atlas ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Atlas wrappers.",本页介绍了如何在LangChain中使用Nomic的Atlas生态系统。它分为两部分：安装和设置，然后是对特定Atlas包装器的引用。
Install the Python package with `pip install nomic`,使用“pip install nomic”安装Python包
Nomic is also included in langchains poetry extras `poetry install -E all`,Nomic还包含在langchains诗歌附加节目“诗歌安装-E all”中
"There exists a wrapper around the Atlas neural database, allowing you to use it as a vectorstore. This vectorstore also gives you full access to the underlying AtlasProject object, which will allow you to use the full range of Atlas map interactions, such as bulk tagging and automatic topic modeling. Please see [the Atlas docs](https://docs.nomic.ai/atlas_api.html) for more detailed information.",Atlas神经数据库周围有一个包装器，允许您将其用作向量存储。这个vectorstore还允许您完全访问底层AtlasProject对象，这将允许您使用所有的Atlas地图交互，例如批量标记和自动主题建模。有关更多详细信息，请参见[地图集文档](https://docs.nomic.ai/atlas_api.html）。
"For a more detailed walkthrough of the AtlasDB wrapper, see [this notebook](../modules/indexes/vectorstores/examples/atlas.ipynb)",有关AtlasDB包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/atlas.ipynb）
Banana,香蕉
"This page covers how to use the Banana ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Banana wrappers.",本页介绍了如何在LangChain中使用香蕉生态系统。它分为两部分：安装和设置，然后是对特定香蕉包装器的引用。
Install with `pip install banana-dev`,使用“pip Install banana dev”安装
Get an Banana api key and set it as an environment variable (`BANANA_API_KEY`),获取Banana api密钥并将其设置为环境变量（`BANANA_API_KEY`）
Define your Banana Template,定义Banana模板
If you want to use an available language model template you can find one [here](https://app.banana.dev/templates/conceptofmind/serverless-template-palmyra-base). This template uses the Palmyra-Base model by [Writer](https://writer.com/product/api/). You can check out an example Banana repository [here](https://github.com/conceptofmind/serverless-template-palmyra-base).,如果你想使用一个可用的语言模型模板，你可以在这里找到一个（https：//app.banana.dev/templates/conceptofmind/server less-template-palmyra-base）。此模板使用[Writer]的Palmyra-Base模型（https://writer.com/product/api/）。您可以在这里查看一个示例香蕉存储库（https：//github.com/conceptofmind/serverless-template-palmyra-base）。
Build the Banana app,构建香蕉应用程序
"Banana Apps must include the ""output"" key in the return json.  There is a rigid response structure.",Banana应用程序必须在返回的json中包含“output”键。有一个刚性的响应结构。
An example inference function would be:,推理函数的一个示例是：
You can find a full example of a Banana app [here](https://github.com/conceptofmind/serverless-template-palmyra-base/blob/main/app.py).,您可以在这里找到Banana应用程序的完整示例（https：//github.com/conceptofmind/serverless-template-palmyra-base/blob/main/app.py）。
"There exists an Banana LLM wrapper, which you can access with",存在一个Banana LLM包装器，您可以使用
You need to provide a model key located in the dashboard:,您需要提供位于仪表板中的型号密钥：
CerebriumAI,脑病
"This page covers how to use the CerebriumAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific CerebriumAI wrappers.",本页介绍了如何在LangChain中使用CerebriumAI生态系统。它分为两部分：安装和设置，然后是对特定CerebriumAI包装器的引用。
Install with `pip install cerebrium`,使用“pip install cerebrium”安装
Get an CerebriumAI api key and set it as an environment variable (`CEREBRIUMAI_API_KEY`),获取CerebriumAI api密钥并将其设置为环境变量（`CEREBRIUMAI_API_KEY`）
"There exists an CerebriumAI LLM wrapper, which you can access with",存在一个CerebriumAI LLM包装器，您可以使用
Chroma,色度
"This page covers how to use the Chroma ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Chroma wrappers.",本页介绍了如何在LangChain中使用色度生态系统。它分为两部分：安装和设置，然后引用特定的色度包装器。
Install the Python package with `pip install chromadb`,使用“pip install chromadb”安装Python包
"There exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore, whether for semantic search or example selection.",色度向量数据库周围有一个包装器，允许您将其用作向量存储，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Chroma wrapper, see [this notebook](../modules/indexes/vectorstores/getting_started.ipynb)",有关色度包装器的更详细演练，请参见[本笔记本](../modules/indexes/vectorstores/getting_started.ipynb）
ClearML Integration,ClearML集成
"In order to properly keep track of your langchain experiments and their results, you can enable the ClearML integration. ClearML is an experiment manager that neatly tracks and organizes all your experiment runs.",为了正确跟踪您的langchain实验及其结果，您可以启用ClearML集成。ClearML是一个实验管理器，它可以整齐地跟踪和组织你所有的实验运行。
Getting API Credentials,正在获取API凭据
"We'll be using quite some APIs in this notebook, here is a list and where to get them:",我们将在本笔记本中使用相当多的API，下面是一个列表以及在哪里可以获得它们：
ClearML: https://app.clear.ml/settings/workspace-configuration,ClearML:https://app.clear.ml/settings/workspace-configuration
OpenAI: https://platform.openai.com/account/api-keys,OpenAI:https://platform.openai.com/account/api-keys
SerpAPI (google search): https://serpapi.com/dashboard,SerpAPI（谷歌搜索）：https://serpapi.com/dashboard
Setting Up,设置
Scenario 1: Just an LLM,场景1：只是一个LLM
"First, let's just run a single LLM a few times and capture the resulting prompt-answer conversation in ClearML",首先，让我们运行一个LLM几次，并在ClearML中捕获得到的提示-应答对话
At this point you can already go to https://app.clear.ml and take a look at the resulting ClearML Task that was created.,此时，您已经可以转到https：//app.clear.ml并查看创建的结果ClearML任务。
"Among others, you should see that this notebook is saved along with any git information. The model JSON that contains the used parameters is saved as an artifact, there are also console logs and under the plots section, you'll find tables that represent the flow of the chain.",其中，您应该看到这个笔记本与任何git信息一起保存。包含所用参数的模型JSON被保存为工件，还有控制台日志，在plots部分，您可以找到表示链流的表。
"Finally, if you enabled visualizations, these are stored as HTML files under debug samples.",最后，如果您启用了可视化，这些将作为HTML文件存储在调试示例下。
Scenario 2: Creating an agent with tools,场景2：使用工具创建代理
"To show a more advanced workflow, let's create an agent with access to tools. The way ClearML tracks the results is not different though, only the table will look slightly different as there are other types of actions taken when compared to the earlier, simpler example.",为了显示更高级的工作流，让我们创建一个可以访问工具的代理。但是ClearML跟踪结果的方式没有什么不同，只是表看起来略有不同，因为与前面更简单的示例相比，还采取了其他类型的操作。
"You can now also see the use of the `finish=True` keyword, which will fully close the ClearML Task, instead of just resetting the parameters and prompts for a new conversation.",现在，您还可以看到“finish=True”关键字的使用，这将完全关闭ClearML任务，而不仅仅是重置新对话的参数和提示。
Tips and Next Steps,提示和后续步骤
"Make sure you always use a unique `name` argument for the `clearml_callback.flush_tracker` function. If not, the model parameters used for a run will override the previous run!",确保对“clearml_callback.flush_tracker”函数始终使用唯一的“name”参数。否则，用于运行的模型参数将覆盖上一次运行！
"If you close the ClearML Callback using `clearml_callback.flush_tracker(..., finish=True)` the Callback cannot be used anymore. Make a new one if you want to keep logging.",如果使用`clearml_callback.flush_tracker(...，finish=True)`关闭ClearML回调，则回调将无法再使用。如果你想继续记录，就做一个新的。
"Check out the rest of the open source ClearML ecosystem, there is a data version manager, a remote execution agent, automated pipelines and much more!",看看开源ClearML生态系统的其余部分，有一个数据版本管理器，一个远程执行代理，自动化管道等等！
Cohere,凝聚
"This page covers how to use the Cohere ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Cohere wrappers.",本页介绍了如何在LangChain中使用Cohere生态系统。它分为两部分：安装和设置，然后是对特定Cohere包装器的引用。
Install the Python SDK with `pip install cohere`,使用“pip install cohere”安装Python SDK
Get an Cohere api key and set it as an environment variable (`COHERE_API_KEY`),获取Cohere api密钥并将其设置为环境变量(`COHERE_API_KEY`)
"There exists an Cohere LLM wrapper, which you can access with",存在一个Cohere LLM包装器，您可以使用
Embeddings,嵌入
"There exists an Cohere Embeddings wrapper, which you can access with",存在一个Cohere Embeddings包装器，您可以使用
"For a more detailed walkthrough of this, see [this notebook](../modules/models/text_embedding/examples/cohere.ipynb)",有关此操作的更详细演练，请参阅[本笔记本](../modules/models/text_embedding/examples/cohere.ipynb）
Comet,彗星
![](https://user-images.githubusercontent.com/7529846/230328046-a8b18c51-12e3-4617-9b39-97614a571a2d.png),！[](https://user-images.githubusercontent.com/7529846/230328046-a 8 b 18 c 51-12 e 3-4617-9 b 39-97614 a 571 a 2 d.png）
"In this guide we will demonstrate how to track your Langchain Experiments, Evaluation Metrics, and LLM Sessions with [Comet](https://www.comet.com/site/?utm_source=langchain&utm_medium=referral&utm_campaign=comet_notebook).",在本指南中，我们将演示如何使用[Comet]（https://www.comet.com/site/？utm_source=Langchain&utm_medium=referral&utm_campaign=comet_notebook）跟踪您的Langchain实验、评估指标和LLM会话。
**Example Project:** [Comet with LangChain](https://www.comet.com/examples/comet-example-langchain/view/b5ZThK6OFdhKWVSP3fDfRtrNF/panels?utm_source=langchain&utm_medium=referral&utm_campaign=comet_notebook),**示例项目：**[Comet with LangChain]（https://www.comet.com/examples/comet-example-langchain/view/b 5 zthk 6 ofdhkwvsp 3 fdfrtrnf/panels？utm_source=LangChain&utm_medium=referral&utm_campaign=comet_notebook）
Install Comet and Dependencies,安装Comet和依赖项
Initialize Comet and Set your Credentials,初始化Comet并设置凭证
You can grab your [Comet API Key here](https://www.comet.com/signup?utm_source=langchain&utm_medium=referral&utm_campaign=comet_notebook) or click the link after initializing Comet,您可以在此处获取[Comet API密钥]（https://www.comet.com/signup？utm_source=langchain&utm_medium=referral&utm_campaign=comet_notebook）或在初始化Comet后单击链接
Set OpenAI and SerpAPI credentials,设置OpenAI和SerpAPI凭据
You will need an [OpenAI API Key](https://platform.openai.com/account/api-keys) and a [SerpAPI API Key](https://serpapi.com/dashboard) to run the following examples,您将需要一个[OpenAI API密钥](https://platform.openai.com/account/api-keys）和一个[SerpAPI API密钥](https://serpapi.com/dashboard）来运行以下示例
Scenario 1: Using just an LLM,场景1：只使用LLM
Scenario 2: Using an LLM in a Chain,场景2：在链中使用LLM
Scenario 3: Using An Agent with Tools,场景3：使用带有工具的代理
Scenario 4: Using Custom Evaluation Metrics,场景4：使用自定义评估指标
The `CometCallbackManager` also allows you to define and use Custom Evaluation Metrics to assess generated outputs from your model. Let's take a look at how this works.,“CometCallbackManager”还允许您定义和使用定制的评估指标来评估模型生成的输出。让我们来看看这是如何工作的。
"In the snippet below, we will use the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric to evaluate the quality of a generated summary of an input prompt.",在下面的代码片段中，我们将使用【ROUGE】（https：//huggingface.co/spaces/evaluate-metric/ROUGE）度量来评估生成的输入提示摘要的质量。
Databerry,数据库
This page covers how to use the [Databerry](https://databerry.ai) within LangChain.,本页介绍了如何在LangChain中使用【Databerry】（https：//databerry.ai）。
What is Databerry?,什么是数据浆果？
Databerry is an [open source](https://github.com/gmpetrov/databerry) document retrievial platform that helps to connect your personal data with Large Language Models.,Databerry是一个【开源】（https：//github.com/gmpetrov/Databerry）文档检索平台，有助于将您的个人数据与大型语言模型连接起来。
![Databerry](../_static/DataberryDashboard.png),！[Databerry](../_static/DataberryDashboard.png）
Quick start,快速启动
Retrieving documents stored in Databerry from LangChain is very easy!,从LangChain检索存储在Databerry中的文档非常容易！
DeepInfra,深红外线
"This page covers how to use the DeepInfra ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific DeepInfra wrappers.",本页介绍了如何在LangChain中使用DeepInfra生态系统。它分为两部分：安装和设置，然后是对特定DeepInfra包装器的引用。
Get your DeepInfra api key from this link [here](https://deepinfra.com/).,从此链接【此处】（https：//deepinfra.com/）获取您的DeepInfra api密钥。
Get an DeepInfra api key and set it as an environment variable (`DEEPINFRA_API_TOKEN`),获取DeepInfra api密钥并将其设置为环境变量（`DEEPINFRA_API_TOKEN`）
"There exists an DeepInfra LLM wrapper, which you can access with",存在一个DeepInfra LLM包装器，您可以使用
Deep Lake,深湖
This page covers how to use the Deep Lake ecosystem within LangChain.,本页介绍了如何利用LangChain内的深湖生态系统。
Why Deep Lake?,为什么是深湖？
More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models.,不仅仅是一个（多模态）矢量存储。您以后可以使用数据集来微调您自己的LLM模型。
"Not only stores embeddings, but also the original data with automatic version control.",不仅存储嵌入，还存储具有自动版本控制的原始数据。
"Truly serverless. Doesn't require another service and can be used with major cloud providers (AWS S3, GCS, etc.)",真正的无服务器。不需要其他服务，并且可以与主要的云提供商（AWS S3、GCS等）一起使用
More Resources,更多资源
[Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/),[LangChain&Deep Lake终极指南：构建ChatGPT以回答有关您财务数据的问题](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/）
[Twitter the-algorithm codebase analysis with Deep Lake](../use_cases/code/twitter-the-algorithm-analysis-deeplake.ipynb),[Twitter the-algorithm代码库分析与Deep Lake](../use_cases/code/twitter-the-algorithm-analysis-deeplake.ipynb）
Here is [whitepaper](https://www.deeplake.ai/whitepaper) and [academic paper](https://arxiv.org/pdf/2209.10785.pdf) for Deep Lake,这是[白皮书]（https://www.deeplake.ai/whitepaper）和[学术论文]（https://arxiv.org/pdf/2209.10785.pdf）
"Here is a set of additional resources available for review: [Deep Lake](https://github.com/activeloopai/deeplake), [Getting Started](https://docs.activeloop.ai/getting-started) and [Tutorials](https://docs.activeloop.ai/hub-tutorials)",这里有一组可供查看的其他资源：[Deep Lake](https://github.com/activeloopai/deeplake)、[Getting Started](https://docs.activeloop.ai/getting-started）和[Tutorials](https://docs.activeloop.ai/hub-tutorials）
Install the Python package with `pip install deeplake`,使用“pip install deeplake”安装Python包
"There exists a wrapper around Deep Lake, a data lake for Deep Learning applications, allowing you to use it as a vector store (for now), whether for semantic search or example selection.",Deep Lake周围有一个包装器，这是一个用于深度学习应用程序的数据湖，允许您将其用作向量存储（目前），无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Deep Lake wrapper, see [this notebook](../modules/indexes/vectorstores/examples/deeplake.ipynb)",有关Deep Lake包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/deeplake.ipynb）
ForefrontAI,前沿a
"This page covers how to use the ForefrontAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific ForefrontAI wrappers.",本页介绍了如何在LangChain中使用ForefrontAI生态系统。它分为两部分：安装和设置，然后是对特定ForefrontAI包装器的引用。
Get an ForefrontAI api key and set it as an environment variable (`FOREFRONTAI_API_KEY`),获取ForefrontAIAPI密钥并将其设置为环境变量（`FOREFRONTAI_API_KEY`）
"There exists an ForefrontAI LLM wrapper, which you can access with",存在一个ForefrontAI LLM包装器，您可以使用
Google Search Wrapper,谷歌搜索包装器
"This page covers how to use the Google Search API within LangChain. It is broken into two parts: installation and setup, and then references to the specific Google Search wrapper.",本页介绍了如何在LangChain中使用Google搜索API。它分为两部分：安装和设置，然后引用特定的Google搜索包装器。
Install requirements with `pip install google-api-python-client`,使用“pip安装google api python客户端”安装要求
"Set up a Custom Search Engine, following [these instructions](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search)",按照[这些说明]设置自定义搜索引擎（https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search）
"Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively",从上一步中获取API密钥和自定义搜索引擎ID，并将它们分别设置为环境变量“GOOGLE_API_KEY”和“GOOGLE_CSE_ID”
There exists a GoogleSearchAPIWrapper utility which wraps this API. To import this utility:,存在一个包装此API的GoogleSearchAPIWrapper实用程序。要导入此实用程序，请执行以下操作：
"For a more detailed walkthrough of this wrapper, see [this notebook](../modules/agents/tools/examples/google_search.ipynb).",有关这个包装器的更详细的演练，请参见[this notebook]（../modules/agents/tools/examples/google_search.ipynb）。
Tool,工具
You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with:,您还可以轻松地将此包装器作为工具加载（与代理一起使用）。您可以使用以下方法执行此操作：
"For more information on this, see [this page](../modules/agents/tools/getting_started.md)",有关这方面的详细信息，请参阅[本页](../modules/agents/tools/getting_started.md）
Google Serper Wrapper,谷歌Serper包装器
"This page covers how to use the [Serper](https://serper.dev) Google Search API within LangChain. Serper is a low-cost Google Search API that can be used to add answer box, knowledge graph, and organic results data from Google Search.  It is broken into two parts: setup, and then references to the specific Google Serper wrapper.",本页介绍了如何在LangChain中使用【Serper】（https：//serper.dev）Google搜索API。Serper是一个低成本的Google搜索API，可用于添加来自Google搜索的答案框、知识图和有机结果数据。它分为两部分：设置，然后是对特定Google Serper包装器的引用。
Setup,设置
Go to [serper.dev](https://serper.dev) to sign up for a free account,转到[serper.dev](https://serper.dev)，注册一个免费帐户
Get the api key and set it as an environment variable (`SERPER_API_KEY`),获取api密钥并将其设置为环境变量（`SERPER_API_KEY`）
There exists a GoogleSerperAPIWrapper utility which wraps this API. To import this utility:,存在一个包装此API的GoogleSerperAPIWrapper实用程序。要导入此实用程序，请执行以下操作：
You can use it as part of a Self Ask chain:,您可以将其用作自我提问链的一部分：
Output,输出
"For a more detailed walkthrough of this wrapper, see [this notebook](../modules/agents/tools/examples/google_serper.ipynb).",有关这个包装器的更详细的演练，请参见[this notebook]（../modules/agents/tools/examples/google_serper.ipynb）。
GooseAI,鹅
"This page covers how to use the GooseAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific GooseAI wrappers.",本页介绍了如何在LangChain中使用GooseAI生态系统。它分为两部分：安装和设置，然后是对特定GooseAI包装器的引用。
Install the Python SDK with `pip install openai`,使用“pip install openai”安装Python SDK
Get your GooseAI api key from this link [here](https://goose.ai/).,从这个链接【这里】（https：//goose.ai/）获取您的GooseAI api密钥。
Set the environment variable (`GOOSEAI_API_KEY`).,设置环境变量（`GOOSEAI_API_KEY`）。
"There exists an GooseAI LLM wrapper, which you can access with:",存在一个GooseAI LLM包装器，您可以通过以下方式访问它：
GPT4All,GPT 4全部
"This page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.",本页介绍了如何在LangChain中使用“GPT 4 All”包装器。本教程分为两部分：安装和设置，随后是使用示例。
Install the Python package with `pip install pyllamacpp`,使用“pip install pyllamacpp”安装Python包。
Download a [GPT4All model](https://github.com/nomic-ai/pyllamacpp#supported-model) and place it in your desired directory,下载[GPT 4 All model]（https://github.com/nomic-ai/pyllamacpp#supported-model）并将其放在所需目录中
Usage,用法
"To use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration.",要使用GPT 4 All包装器，您需要提供预训练模型文件的路径和模型的配置。
"You can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.",您还可以自定义生成参数，如n_predict、temp、top_p、top_k等。
"To stream the model's predictions, add in a CallbackManager.",要流式传输模型的预测，请添加一个CallbackManager。
Model File,模型文件
You can find links to model file downloads in the [pyllamacpp](https://github.com/nomic-ai/pyllamacpp) repository.,您可以在[pyllamacpp]（https：//github.com/nomic-ai/pyllamacpp）存储库中找到模型文件下载的链接。
"For a more detailed walkthrough of this, see [this notebook](../modules/models/llms/integrations/gpt4all.ipynb)",有关此操作的更详细演练，请参阅[本笔记本]（../modules/models/llms/integrations/gpt 4 all.ipynb）
Graphsignal,图形信号
"This page covers how to use [Graphsignal](https://app.graphsignal.com) to trace and monitor LangChain. Graphsignal enables full visibility into your application. It provides latency breakdowns by chains and tools, exceptions with full context, data monitoring, compute/GPU utilization, OpenAI cost analytics, and more.",本页介绍了如何使用【Graphsignal】（https：//app.graphsignal.com）来跟踪和监控LangChain。Graphsignal支持对应用程序的全面可见性。它提供了按链和工具划分的延迟故障、完整上下文的异常、数据监控、计算/GPU利用率、OpenAI成本分析等。
Install the Python library with `pip install graphsignal`,使用“pip安装图形信号”安装Python库
Create free Graphsignal account [here](https://graphsignal.com),创建免费的Graphsignal帐户[此处]（https：//graphsignal.com）
Get an API key and set it as an environment variable (`GRAPHSIGNAL_API_KEY`),获取API密钥并将其设置为环境变量（`GRAPHSIGNAL_API_KEY`）
Tracing and Monitoring,追踪和监测
Graphsignal automatically instruments and starts tracing and monitoring chains. Traces and metrics are then available in your [Graphsignal dashboards](https://app.graphsignal.com).,Graphsignal自动测量并开始跟踪和监控链。然后，跟踪和指标可以在您的【Graphsignal仪表板】（https：//app.graphsignal.com）中找到。
Initialize the tracer by providing a deployment name:,通过提供部署名称初始化跟踪程序：
"To additionally trace any function or code, you can use a decorator or a context manager:",要另外跟踪任何函数或代码，可以使用装饰器或上下文管理器：
"Optionally, enable profiling to record function-level statistics for each trace.",或者，启用分析来记录每个跟踪的函数级统计信息。
See the [Quick Start](https://graphsignal.com/docs/guides/quick-start/) guide for complete setup instructions.,有关完整的安装说明，请参见[快速入门](https://graphsignal.com/docs/guides/quick-start/）指南。
Hazy Research,朦胧研究
"This page covers how to use the Hazy Research ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Hazy Research wrappers.",本页介绍了如何在LangChain中使用Hazy研究生态系统。它分为两部分：安装和设置，然后引用特定的模糊研究包装。
"To use the `manifest`, install it with `pip install manifest-ml`",要使用“清单”，请使用“pip install manifest-ml”安装它
"There exists an LLM wrapper around Hazy Research's `manifest` library.  `manifest` is a python library which is itself a wrapper around many model providers, and adds in caching, history, and more.",Hazy Research的“清单”库周围有一个LLM包装器。“manifest”是一个python库，它本身是许多模型提供者的包装器，并添加了缓存、历史记录等。
To use this wrapper:,要使用此包装：
Helicone,螺旋酮
This page covers how to use the [Helicone](https://helicone.ai) ecosystem within LangChain.,本页介绍了如何在LangChain中使用【Helicone】（https：//helicone.ai）生态系统。
What is Helicone?,什么是螺旋酮？
"Helicone is an [open source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.",Helicone是一个【开源】（https：//github.com/Helicone/Helicone）可观察性平台，它代理您的OpenAI流量，并为您提供关于您的支出、延迟和使用的关键见解。
![Helicone](../_static/HeliconeDashboard.png),！[Helicone](../_static/HeliconeDashboard.png）
With your LangChain environment you can just add the following parameter.,在您的LangChain环境中，您只需添加以下参数。
"Now head over to [helicone.ai](https://helicone.ai/onboarding?step=2) to create your account, and add your OpenAI API key within our dashboard to view your logs.",现在前往【helicone.ai】（https：//helicone.ai/onboarding？步骤=2）创建您的帐户，并在我们的仪表板中添加您的OpenAI API密钥以查看您的日志。
![Helicone](../_static/HeliconeKeys.png),！[Helicone](../_static/HeliconeKeys.png）
How to enable Helicone caching,如何启用Helicone缓存
[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching),[Helicone缓存文档](https://docs.helicone.ai/advanced-usage/caching）
How to use Helicone custom properties,如何使用Helicone自定义属性
[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties),[Helicone属性文档](https://docs.helicone.ai/advanced-usage/custom-properties）
Hugging Face,拥抱脸
"This page covers how to use the Hugging Face ecosystem (including the [Hugging Face Hub](https://huggingface.co)) within LangChain. It is broken into two parts: installation and setup, and then references to specific Hugging Face wrappers.",本页介绍了如何在LangChain中使用拥抱脸生态系统（包括【拥抱脸中心】（https：//huggingface.co））。它分为两部分：安装和设置，然后参考特定的拥抱面包装。
If you want to work with the Hugging Face Hub:,如果您想使用拥抱脸集线器：
Install the Hub client library with `pip install huggingface_hub`,使用“pip install huggingface_hub”安装集线器客户端库
Create a Hugging Face account (it's free!),创建一个拥抱脸帐户（这是免费的！）
Create an [access token](https://huggingface.co/docs/hub/security-tokens) and set it as an environment variable (`HUGGINGFACEHUB_API_TOKEN`),创建[访问令牌](https://huggingface.co/docs/hub/security-tokens）并将其设置为环境变量(`HUGGINGFACEHUB_API_TOKEN`）
If you want work with the Hugging Face Python libraries:,如果您想使用拥抱脸Python库：
Install `pip install transformers` for working with models and tokenizers,安装“pip install transformers”以使用模型和标记器
Install `pip install datasets` for working with datasets,安装“pip install datasets”以处理数据集
"There exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: [`text2text-generation`](https://huggingface.co/models?library=transformers&pipeline_tag=text2text-generation&sort=downloads), [`text-generation`](https://huggingface.co/models?library=transformers&pipeline_tag=text-classification&sort=downloads)",存在两个拥抱脸LLM包装器，一个用于本地管道，一个用于托管在拥抱脸中心上的模型。请注意，这些包装器仅适用于支持以下任务的模型：[`text 2 text-generation`](https://huggingface.co/models？library=transformers&pipeline_tag=text 2 text-generation&sort=downloads），[`text-generation`](https://huggingface.co/models？library=transformers&pipeline_tag=text-classification&sort=downloads）
To use the local pipeline wrapper:,要使用本地管道包装器：
To use a the wrapper for a model hosted on Hugging Face Hub:,要对托管在拥抱脸集线器上的模型使用包装器：
"For a more detailed walkthrough of the Hugging Face Hub wrapper, see [this notebook](../modules/models/llms/integrations/huggingface_hub.ipynb)",有关Hugging Face Hub包装器的更详细演练，请参阅[本笔记本](../modules/models/llms/integrations/huggingface_hub.ipynb）
"There exists two Hugging Face Embeddings wrappers, one for a local model and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for [`sentence-transformers` models](https://huggingface.co/models?library=sentence-transformers&sort=downloads).",存在两个拥抱脸嵌入包装器，一个用于本地模型，一个用于托管在拥抱脸中心上的模型。请注意，这些包装器只适用于【`句子变形金刚`模型】（https：//huggingface.co/models？library=句子变形金刚&sort=下载）。
"For a more detailed walkthrough of this, see [this notebook](../modules/models/text_embedding/examples/huggingfacehub.ipynb)",有关此操作的更详细演练，请参阅[本笔记本](../modules/models/text_embedding/examples/huggingfacehub.ipynb）
Tokenizer,标记器
"There are several places you can use tokenizers available through the `transformers` package. By default, it is used to count tokens for all LLMs.",通过“transformers”包，您可以在几个地方使用标记器。默认情况下，它用于计算所有LLM的令牌。
You can also use it to count tokens when splitting documents with,在拆分文档时，还可以使用它来计算标记数
"For a more detailed walkthrough of this, see [this notebook](../modules/indexes/text_splitters/examples/huggingface_length_function.ipynb)",有关此操作的更详细演练，请参阅[本笔记本](../modules/indexes/text_splitters/examples/huggingface_length_function.ipynb）
Datasets,数据集
The Hugging Face Hub has lots of great [datasets](https://huggingface.co/datasets) that can be used to evaluate your LLM chains.,拥抱脸中心有很多很棒的【数据集】（https：//huggingface.co/datasets），可以用来评估你的LLM链。
"For a detailed walkthrough of how to use them to do so, see [this notebook](../use_cases/evaluation/huggingface_datasets.ipynb)",有关如何使用它们的详细演练，请参见[本笔记本](../use_cases/evaluation/huggingface_datasets.ipynb）
Jina,纪娜
"This page covers how to use the Jina ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Jina wrappers.",本页介绍了如何在LangChain中使用纪娜生态系统。它分为两部分：安装和设置，然后引用特定的纪娜包装器。
Install the Python SDK with `pip install jina`,使用“pip安装吉娜”安装Python SDK`
Get a Jina AI Cloud auth token from [here](https://cloud.jina.ai/settings/tokens) and set it as an environment variable (`JINA_AUTH_TOKEN`),从[此处](https://cloud.jina.ai/settings/tokens）获取Jina AI Cloud auth令牌，并将其设置为环境变量(`JINA_AUTH_TOKEN`）
"There exists a Jina Embeddings wrapper, which you can access with",存在一个Jina嵌入包装器，您可以使用
"For a more detailed walkthrough of this, see [this notebook](../modules/models/text_embedding/examples/jina.ipynb)",有关此操作的更详细演练，请参阅[本笔记本](../modules/models/text_embedding/examples/jina.ipynb）
Llama.cpp,羊驼。cpp
"This page covers how to use [llama.cpp](https://github.com/ggerganov/llama.cpp) within LangChain. It is broken into two parts: installation and setup, and then references to specific Llama-cpp wrappers.",本页介绍了如何在LangChain中使用[llama.cpp]（https：//github.com/ggerganov/llama.cpp）。它分为两部分：安装和设置，然后是对特定Llama-cpp包装器的引用。
Install the Python package with `pip install llama-cpp-python`,使用“pip install llama-cpp-python”安装Python包
Download one of the [supported models](https://github.com/ggerganov/llama.cpp#description) and convert them to the llama.cpp format per the [instructions](https://github.com/ggerganov/llama.cpp),下载其中一个[支持的模型]（https://github.com/ggerganov/llama.cpp#description），并按照[说明]（https://github.com/ggerganov/llama.cpp）将其转换为llama.cpp格式
"There exists a LlamaCpp LLM wrapper, which you can access with",存在一个LlamaCpp LLM包装器，您可以使用
"For a more detailed walkthrough of this, see [this notebook](../modules/models/llms/integrations/llamacpp.ipynb)",有关此操作的更详细演练，请参阅[本笔记本]（../modules/models/llms/integrations/llamacpp.ipynb）
"There exists a LlamaCpp Embeddings wrapper, which you can access with",存在一个LlamaCpp嵌入包装器，您可以使用
"For a more detailed walkthrough of this, see [this notebook](../modules/models/text_embedding/examples/llamacpp.ipynb)",有关此操作的更详细演练，请参阅[本笔记本](../modules/models/text_embedding/examples/llamacpp.ipynb）
Metal,金属
This page covers how to use [Metal](https://getmetal.io) within LangChain.,本页介绍了如何在LangChain中使用【Metal】（https：//getmetal.io）。
What is Metal?,什么是金属？
Metal is a  managed retrieval & memory platform built for production. Easily index your data into `Metal` and run semantic search and retrieval on it.,Metal是一个为生产而构建的托管检索和内存平台。轻松地将您的数据索引到“Metal”中，并在其上运行语义搜索和检索。
![Metal](../_static/MetalDash.png),！[Metal](../_static/MetalDash.png）
Get started by [creating a Metal account](https://app.getmetal.io/signup).,从【创建金属账户】开始（https：//app.getmetal.io/signup）。
"Then, you can easily take advantage of the `MetalRetriever` class to start retrieving your data for semantic search, prompting context, etc. This class takes a `Metal` instance and a dictionary of parameters to pass to the Metal API.",然后，您可以很容易地利用“MetalRetriever”类开始检索您的数据，用于语义搜索、提示上下文等。这个类接受一个“Metal”实例和一个参数字典来传递给Metal API。
Milvus,米尔沃斯
"This page covers how to use the Milvus ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Milvus wrappers.",本页介绍了如何在LangChain中使用Milvus生态系统。它分为两部分：安装和设置，然后是对特定Milvus包装器的引用。
Install the Python SDK with `pip install pymilvus`,使用“pip install pymilvus”安装Python SDK
"There exists a wrapper around Milvus indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection.",Milvus索引周围有一个包装器，允许您将它用作向量存储，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Miluvs wrapper, see [this notebook](../modules/indexes/vectorstores/examples/milvus.ipynb)",有关Miluvs包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/milvus.ipynb）
Modal,模态的
"This page covers how to use the Modal ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Modal wrappers.",本页介绍了如何在LangChain中使用模态生态系统。它分为两部分：安装和设置，然后是对特定模式包装器的引用。
Install with `pip install modal-client`,使用“pip安装模式客户端”安装
Run `modal token new`,运行“模式令牌新建”
Define your Modal Functions and Webhooks,定义你的模态函数和Webhooks
You must include a prompt. There is a rigid response structure.,您必须包含提示。有一个刚性的响应结构。
An example with GPT2:,GPT 2示例：
"There exists an Modal LLM wrapper, which you can access with",存在一个模态LLM包装器，您可以使用
MyScale,我的比例尺
"This page covers how to use MyScale vector database within LangChain. It is broken into two parts: installation and setup, and then references to specific MyScale wrappers.",本页介绍了如何在LangChain中使用MyScale矢量数据库。它分为两部分：安装和设置，然后是对特定MyScale包装器的引用。
"With MyScale, you can manage both structured and unstructured (vectorized) data, and perform joint queries and analytics on both types of data using SQL. Plus, MyScale's cloud-native OLAP architecture, built on top of ClickHouse, enables lightning-fast data processing even on massive datasets.",使用MyScale，您可以管理结构化和非结构化（矢量化）数据，并使用SQL对这两种类型的数据执行联合查询和分析。此外，MyScale的云原生OLAP架构构建在ClickHouse之上，即使在大规模数据集上也能实现闪电般的数据处理。
Introduction,导言
[Overview to MyScale and High performance vector search](https://docs.myscale.com/en/overview/),[MyScale和高性能矢量搜索概述](https://docs.myscale.com/en/Overview/）
You can now register on our SaaS and [start a cluster now!](https://docs.myscale.com/en/quickstart/),您现在可以在我们的SaaS上注册[立即启动集群！](https://docs.myscale.com/en/quickstart/）
"If you are also interested in how we managed to integrate SQL and vector, please refer to [this document](https://docs.myscale.com/en/vector-reference/) for further syntax reference.",如果您也对我们如何集成SQL和vector感兴趣，请参考【本文档】（https：//docs.myscale.com/en/vector-reference/）以获得进一步的语法参考。
We also deliver with live demo on huggingface! Please checkout our [huggingface space](https://huggingface.co/myscale)! They search millions of vector within a blink!,我们还在huggingface上提供现场演示！请查看我们的【拥抱脸空间】（https：//huggingface.co/myscale）！他们一眨眼就搜索了数百万个矢量！
Install the Python SDK with `pip install clickhouse-connect`,使用“pip安装clickhouse连接”安装Python SDK`
Setting up envrionments,设置环境
There are two ways to set up parameters for myscale index.,有两种方法可以为myscale索引设置参数。
Environment Variables,环境变量
"Before you run the app, please set the environment variable with `export`:  `export MYSCALE_URL='<your-endpoints-url>' MYSCALE_PORT=<your-endpoints-port> MYSCALE_USERNAME=<your-username> MYSCALE_PASSWORD=<your-password> ...`",在运行应用程序之前，请使用“export”设置环境变量：`export MYSCALE_URL=’<your-endpoints-url>’MYSCALE_PORT=<your-endpoints-port>MYSCALE_USERNAME=<your-username>MYSCALE_PASSWORD=<your-password>...`
"You can easily find your account, password and other info on our SaaS. For details please refer to [this document](https://docs.myscale.com/en/cluster-management/)  Every attributes under `MyScaleSettings` can be set with prefix `MYSCALE_` and is case insensitive.",您可以在我们的SaaS上轻松找到您的帐户、密码和其他信息。有关详细信息，请参考【本文档】（https：//docs.myscale.com/en/cluster-management/）“MyScaleSettings”下的每个属性都可以设置前缀“myscale_”，并且不区分大小写。
Create `MyScaleSettings` object with parameters,使用参数创建“MyScaleSettings”对象
supported functions:,支持的功能：
`add_texts`,“添加文本”
`add_documents`,`add_documents’
`from_texts`,“发件人_文本”
`from_documents`,`from_documents’
`similarity_search`,“相似性搜索”
`asimilarity_search`,“asimilarity_search”
`similarity_search_by_vector`,“相似性搜索向量”
`asimilarity_search_by_vector`,“asimilarity_search_by_vector”
`similarity_search_with_relevance_scores`,“相似性搜索与相关性得分”
"There exists a wrapper around MyScale database, allowing you to use it as a vectorstore, whether for semantic search or similar example retrieval.",MyScale数据库周围有一个包装器，允许您将它用作vectorstore，无论是用于语义搜索还是类似的示例检索。
"For a more detailed walkthrough of the MyScale wrapper, see [this notebook](../modules/indexes/vectorstores/examples/myscale.ipynb)",有关MyScale包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/myscale.ipynb）
NLPCloud,NLPCloud
"This page covers how to use the NLPCloud ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific NLPCloud wrappers.",本页介绍了如何在LangChain中使用NLPCloud生态系统。它分为两部分：安装和设置，然后是对特定NLPCloud包装器的引用。
Install the Python SDK with `pip install nlpcloud`,使用“pip安装nlpcloud”安装Python SDK
Get an NLPCloud api key and set it as an environment variable (`NLPCLOUD_API_KEY`),获取NLPCloud api密钥并将其设置为环境变量（`NLPCLOUD_API_KEY`）
"There exists an NLPCloud LLM wrapper, which you can access with",存在一个NLPCloud LLM包装器，您可以使用
OpenAI,OpenAI
"This page covers how to use the OpenAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific OpenAI wrappers.",本页介绍了如何在LangChain中使用OpenAI生态系统。它分为两部分：安装和设置，然后是对特定OpenAI包装器的引用。
Get an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`),获取OpenAI api密钥并将其设置为环境变量（`OPENAI_API_KEY`）
"If you want to use OpenAI's tokenizer (only available for Python 3.9+), install it with `pip install tiktoken`",如果您想使用OpenAI的标记器（仅适用于Python 3.9+），请使用“pip install tiktoken”安装它
"There exists an OpenAI LLM wrapper, which you can access with",有一个OpenAI LLM包装器，您可以使用
"If you are using a model hosted on Azure, you should use different wrapper for that:",如果您使用的是Azure上托管的模型，则应该使用不同的包装器：
"For a more detailed walkthrough of the Azure wrapper, see [this notebook](../modules/models/llms/integrations/azure_openai_example.ipynb)",有关Azure包装器的更详细演练，请参见[本笔记本]（../modules/models/llms/integrations/azure_openai_example.ipynb）
"There exists an OpenAI Embeddings wrapper, which you can access with",有一个OpenAI嵌入包装器，您可以使用
"For a more detailed walkthrough of this, see [this notebook](../modules/models/text_embedding/examples/openai.ipynb)",要获得更详细的演练，请参阅[本笔记本](../modules/models/text_embedding/examples/openai.ipynb）
"There are several places you can use the `tiktoken` tokenizer. By default, it is used to count tokens for OpenAI LLMs.",有几个地方可以使用“tiktoken”标记器。默认情况下，它用于计算OpenAI LLMs的令牌数。
"For a more detailed walkthrough of this, see [this notebook](../modules/indexes/text_splitters/examples/tiktoken.ipynb)",有关此操作的更详细演练，请参阅[本笔记本](../modules/indexes/text_splitters/examples/tiktoken.ipynb）
Moderation,适度
You can also access the OpenAI content moderation endpoint with,您还可以通过以下方式访问OpenAI内容审核端点
"For a more detailed walkthrough of this, see [this notebook](../modules/chains/examples/moderation.ipynb)",要获得更详细的演练，请参阅[本笔记本]（../modules/chains/examples/moderation.ipynb）
OpenSearch,开放搜索
"This page covers how to use the OpenSearch ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific OpenSearch wrappers.",本页介绍了如何在LangChain中使用OpenSearch生态系统。它分为两部分：安装和设置，然后是对特定OpenSearch包装器的引用。
Install the Python package with `pip install opensearch-py`,使用“pip安装opensearch-py”安装Python包
"There exists a wrapper around OpenSearch vector databases, allowing you to use it as a vectorstore  for semantic search using approximate vector search powered by lucene, nmslib and faiss engines  or using painless scripting and script scoring functions for bruteforce vector search.",OpenSearch向量数据库周围有一个包装器，允许您使用它作为语义搜索的向量存储，使用由lucene、nmslib和faiss引擎支持的近似向量搜索，或者使用无痛脚本和脚本评分功能进行暴力向量搜索。
"For a more detailed walkthrough of the OpenSearch wrapper, see [this notebook](../modules/indexes/vectorstores/examples/opensearch.ipynb)",有关OpenSearch包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/opensearch.ipynb）
Petals,花瓣
"This page covers how to use the Petals ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Petals wrappers.",本页介绍了如何在LangChain中使用花瓣生态系统。它分为两部分：安装和设置，然后引用特定的花瓣包装器。
Install with `pip install petals`,使用“pip安装花瓣”安装
Get a Hugging Face api key and set it as an environment variable (`HUGGINGFACE_API_KEY`),获取拥抱脸api密钥并将其设置为环境变量（`HUGGINGFACE_API_KEY`）
"There exists an Petals LLM wrapper, which you can access with",存在一个花瓣LLM包装器，您可以使用
PGVector,PGVector
"This page covers how to use the Postgres [PGVector](https://github.com/pgvector/pgvector) ecosystem within LangChain It is broken into two parts: installation and setup, and then references to specific PGVector wrappers.",本页介绍了如何在LangChain中使用Postgres【PGVector】（https：//github.com/PGVector/PGVector）生态系统。它分为两部分：安装和设置，然后是对特定PGVector包装器的引用。
Installation,安装
Install the Python package with `pip install pgvector`,使用“pip install pgvector”安装Python包
The first step is to create a database with the `pgvector` extension installed.,第一步是创建一个安装了“pgvector”扩展的数据库。
Follow the steps at [PGVector Installation Steps](https://github.com/pgvector/pgvector#installation) to install the database and the extension. The docker image is the easiest way to get started.,按照[PGVector安装步骤]（https：//github.com/PGVector/PGVector#Installation）中的步骤安装数据库和扩展。docker映像是最简单的入门方式。
"There exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore, whether for semantic search or example selection.",Postgres向量数据库周围有一个包装器，允许您将其用作向量存储，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the PGVector Wrapper, see [this notebook](../modules/indexes/vectorstores/examples/pgvector.ipynb)",有关PGVector包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vector stores/examples/pgvector.ipynb）
Pinecone,松果
"This page covers how to use the Pinecone ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Pinecone wrappers.",本页介绍了如何在LangChain中使用松果生态系统。它分为两部分：安装和设置，然后引用特定的松果包装器。
Install the Python SDK with `pip install pinecone-client`,使用“pip安装松果客户端”安装Python SDK`
"There exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection.",松果索引周围有一个包装器，允许您将其用作向量存储，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Pinecone wrapper, see [this notebook](../modules/indexes/vectorstores/examples/pinecone.ipynb)",有关松果包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/pinecone.ipynb）
Prediction Guard,预测保护
"This page covers how to use the Prediction Guard ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Prediction Guard wrappers.",本页介绍了如何在LangChain中使用预测保护生态系统。它分为两部分：安装和设置，然后是对特定预测保护包装器的引用。
Install the Python SDK with `pip install predictionguard`,使用“pip install predictionguard”安装Python SDK
Get an Prediction Guard access token (as described [here](https://docs.predictionguard.com/)) and set it as an environment variable (`PREDICTIONGUARD_TOKEN`),获取Prediction Guard访问令牌（如[此处]所述）(https://docs.predictionguard.com/))，并将其设置为环境变量(`PREDICTIONGUARD_TOKEN`）
LLM Wrapper,LLM包装器
"There exists a Prediction Guard LLM wrapper, which you can access with",存在一个预测保护LLM包装器，您可以使用
"You can provide the name of your Prediction Guard ""proxy"" as an argument when initializing the LLM:",在初始化LLM时，可以提供预测保护“代理”的名称作为参数：
"Alternatively, you can use Prediction Guard's default proxy for SOTA LLMs:",或者，您可以使用预测卫士的SOTA LLM默认代理：
You can also provide your access token directly as an argument:,您也可以直接提供访问令牌作为参数：
Example usage,示例用法
Basic usage of the LLM wrapper:,LLM包装器的基本用法：
Basic LLM Chaining with the Prediction Guard wrapper:,使用预测保护包装器的基本LLM链接：
PromptLayer,提示层
"This page covers how to use [PromptLayer](https://www.promptlayer.com) within LangChain. It is broken into two parts: installation and setup, and then references to specific PromptLayer wrappers.",本页介绍如何在LangChain中使用[PromptLayer](https://www.promptlayer.com）。它分为两部分：安装和设置，然后是对特定PromptLayer包装器的引用。
If you want to work with PromptLayer:,如果要使用PromptLayer：
Install the promptlayer python library `pip install promptlayer`,安装promptlayer python库“pip install promptlayer”
Create a PromptLayer account,创建PromptLayer帐户
Create an api token and set it as an environment variable (`PROMPTLAYER_API_KEY`),创建api令牌并将其设置为环境变量（`PROMPTLAYER_API_KEY`）
"There exists an PromptLayer OpenAI LLM wrapper, which you can access with",存在一个PromptLayer OpenAI LLM包装器，您可以使用
"To tag your requests, use the argument `pl_tags` when instanializing the LLM",要标记请求，请在实例化LLM时使用参数“pl_tags”
"To get the PromptLayer request id, use the argument `return_pl_id` when instanializing the LLM",若要获取PromptLayer请求id，请在实例化LLM时使用参数“return_pl_id”
This will add the PromptLayer request ID in the `generation_info` field of the `Generation` returned when using `.generate` or `.agenerate`,这将在使用“.generate”或“.agenerate”时返回的“generation”的“generation_info”字段中添加PromptLayer请求ID
For example:,例如：
"You can use the PromptLayer request ID to add a prompt, score, or other metadata to your request. [Read more about it here](https://magniv.notion.site/Track-4deee1b1f7a34c1680d085f82567dab9).",您可以使用PromptLayer请求ID向您的请求添加提示、分数或其他元数据。[点击此处了解更多信息]（https://magniv.conception.site/track-4 DEEE 1 B 1 F 7 A 34 C 1680 D 085 F 82567 DAB 9）。
"This LLM is identical to the [OpenAI LLM](./openai.md), except that",此LLM与[OpenAI LLM]（。/openai.md）相同，只是
all your requests will be logged to your PromptLayer account,您的所有请求都将记录到您的PromptLayer帐户中
you can add `pl_tags` when instantializing to tag your requests on PromptLayer,您可以在实例化时添加“pl_tags”来标记PromptLayer上的请求
you can add `return_pl_id` when instantializing to return a PromptLayer request id to use [while tracking requests](https://magniv.notion.site/Track-4deee1b1f7a34c1680d085f82567dab9).,您可以在实例化时添加“return_pl_id”以返回PromptLayer请求id来使用[同时跟踪请求]（https：//magniv.idea.site/track-4 DEEE 1 B 1 F 7 a 34 C 1680 D 085 F 82567 DAB 9）。
PromptLayer also provides native wrappers for [`PromptLayerChatOpenAI`](../modules/models/chat/integrations/promptlayer_chatopenai.ipynb) and `PromptLayerOpenAIChat`,PromptLayer还为[`PromptLayerChatOpenAI`](../modules/models/chat/integrations/promptlayer_chatopenai.ipynb）和`PromptLayerOpenAIChat`提供本机包装器
Qdrant,Qdrant
"This page covers how to use the Qdrant ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Qdrant wrappers.",本页介绍了如何在LangChain中使用Qdrant生态系统。它分为两部分：安装和设置，然后是对特定Qdrant包装器的引用。
Install the Python SDK with `pip install qdrant-client`,使用“pip安装qdrant客户端”安装Python SDK`
"There exists a wrapper around Qdrant indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection.",Qdrant索引周围有一个包装器，允许您将它用作向量存储，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Qdrant wrapper, see [this notebook](../modules/indexes/vectorstores/examples/qdrant.ipynb)",有关Qdrant包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/qdrant.ipynb）
Replicate,复制
This page covers how to run models on Replicate within LangChain.,本页介绍了如何在LangChain中运行复制模型。
Create a [Replicate](https://replicate.com) account. Get your API key and set it as an environment variable (`REPLICATE_API_TOKEN`),创建一个[Replicate](https://replicate.com）帐户。获取API密钥并将其设置为环境变量（`REPLICATE_API_TOKEN`）
Install the [Replicate python client](https://github.com/replicate/replicate-python) with `pip install replicate`,使用“pip install replicate”安装[Replicate python客户端]（https://github.com/Replicate/replicate-python）
Calling a model,呼叫模型
"Find a model on the [Replicate explore page](https://replicate.com/explore), and then paste in the model name and version in this format: `owner-name/model-name:version`",在[复制浏览页](https://replicate.com/explore）上找到一个模型，然后按以下格式粘贴模型名称和版本：`owner-name/model-name:version’
"For example, for this [dolly model](https://replicate.com/replicate/dolly-v2-12b), click on the API tab. The model name/version would be: `""replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5""`",例如，对于这个[dolly模型]（https://replicate.com/replicate/dolly-v 2-12 b），单击API选项卡。型号名称/版本为：`“复制/移动-V 2-12 B：EF 0 E 1 AEFC 61 F 8 E 096 EBE 4 DB 6 B 2 BACC 297 DAF 2 EF 6899 F 0 F 7 E 001 EC 445893500 E 5”`
"Only the `model` param is required, but any other model parameters can also be passed in with the format `input={model_param: value, ...}`",只需要“model”参数，但也可以使用格式“input={model_param:value，...}”传入任何其他模型参数
"For example, if we were running stable diffusion and wanted to change the image dimensions:",例如，如果我们正在运行稳定扩散并希望更改图像尺寸：
"*Note that only the first output of a model will be returned.* From here, we can initialize our model:",*请注意，只返回模型的第一个输出。*从这里，我们可以初始化我们的模型：
And run it:,然后运行它：
"We can call any Replicate model (not just LLMs) using this syntax. For example, we can call [Stable Diffusion](https://replicate.com/stability-ai/stable-diffusion):",我们可以使用这种语法调用任何复制模型（不仅仅是LLM）。例如，我们可以调用[Stable Diffusion]（https://replicate.com/stability-ai/stable-diffusion）：
Runhouse,起落架
"This page covers how to use the [Runhouse](https://github.com/run-house/runhouse) ecosystem within LangChain. It is broken into three parts: installation and setup, LLMs, and Embeddings.",本页介绍了如何在LangChain中使用【Runhouse】（https：//github.com/run-house/Runhouse）生态系统。它分为三个部分：安装和设置、LLMs和嵌入。
Install the Python SDK with `pip install runhouse`,使用“pip install runhouse”安装Python SDK
"If you'd like to use on-demand cluster, check your cloud credentials with `sky check`",如果您想使用按需集群，请使用“sky check”检查您的云凭据
Self-hosted LLMs,自托管LLMs
"For a basic self-hosted LLM, you can use the `SelfHostedHuggingFaceLLM` class. For more custom LLMs, you can use the `SelfHostedPipeline` parent class.",对于基本的自托管LLM，您可以使用“SelfHostedHuggingFaceLLM”类。对于更多的自定义LLM，您可以使用“SelfHostedPipeline”父类。
"For a more detailed walkthrough of the Self-hosted LLMs, see [this notebook](../modules/models/llms/integrations/runhouse.ipynb)",有关自托管LLM的更详细演练，请参见[本笔记本]（../modules/models/LLMs/integrations/runhouse.ipynb）
Self-hosted Embeddings,自托管嵌入
There are several ways to use self-hosted embeddings with LangChain via Runhouse.,有几种方法可以通过Runhouse将自托管嵌入与LangChain一起使用。
"For a basic self-hosted embedding from a Hugging Face Transformers model, you can use  the `SelfHostedEmbedding` class.",对于来自拥抱脸变形金刚模型的基本自托管嵌入，您可以使用“SelfHostedEmbedding”类。
"For a more detailed walkthrough of the Self-hosted Embeddings, see [this notebook](../modules/models/text_embedding/examples/self-hosted.ipynb)",有关自托管嵌入的更详细演练，请参见[本笔记本](../modules/models/text_embedding/examples/self-hosted.ipynb）
RWKV-4,RWKV-4
"This page covers how to use the `RWKV-4` wrapper within LangChain. It is broken into two parts: installation and setup, and then usage with an example.",本页介绍了如何在LangChain中使用“RWKV-4”包装器。它分为两部分：安装和设置，然后是使用示例。
Install the Python package with `pip install rwkv`,使用“pip install rwkv”安装Python包
Install the tokenizer Python package with `pip install tokenizer`,使用“pip install tokenizer”安装标记器Python包
Download a [RWKV model](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) and place it in your desired directory,下载一个[RWKV模型](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main）并将其放在所需的目录中
Download the [tokens file](https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/20B_tokenizer.json),下载[令牌文件](https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/20 b_tokenizer.json）
RWKV,RWKV
"To use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer's configuration.",要使用RWKV包装器，您需要提供预训练模型文件的路径和标记器的配置。
You can find links to model file downloads at the [RWKV-4-Raven](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) repository.,您可以在【RWKV-4-Raven】（https：//huggingface.co/BlinkDL/RWKV-4-Raven/tree/main）存储库中找到模型文件下载的链接。
Rwkv-4 models -> recommended VRAM,Rwkv-4型号->推荐的VRAM
"See the [rwkv pip](https://pypi.org/project/rwkv/) page for more information about strategies, including streaming and cuda support.",有关策略的更多信息，包括流和cuda支持，请参见[rwkv pip](https://pypi.org/project/rwkv/）页面。
SearxNG Search API,搜索API
"This page covers how to use the SearxNG search API within LangChain. It is broken into two parts: installation and setup, and then references to the specific SearxNG API wrapper.",本页介绍了如何在LangChain中使用SearxNG搜索API。它分为两部分：安装和设置，然后是对特定SearxNG API包装器的引用。
While it is possible to utilize the wrapper in conjunction with  [public searx instances](https://searx.space/) these instances frequently do not permit API access (see note on output format below) and have limitations on the frequency of requests. It is recommended to opt for a self-hosted instance instead.,虽然可以将包装器与【public searx instances】（https：//searx.space/）结合使用，但这些实例通常不允许API访问（参见下面关于输出格式的注释），并且对请求频率有限制。建议选择自托管实例。
Self Hosted Instance:,自托管实例：
See [this page](https://searxng.github.io/searxng/admin/installation.html) for installation instructions.,有关安装说明，请参见【本页】（https：//searxng.github.io/searxng/admin/installation.html）。
"When you install SearxNG, the only active output format by default is the HTML format. You need to activate the `json` format to use the API. This can be done by adding the following line to the `settings.yml` file:",安装SearxNG时，默认情况下唯一活动的输出格式是HTML格式。您需要激活“json”格式来使用API。这可以通过在“settings.yml”文件中添加以下行来完成：
You can make sure that the API is working by issuing a curl request to the API endpoint:,您可以通过向API端点发出curl请求来确保API正在工作：
`curl -kLX GET --data-urlencode q='langchain' -d format=json http://localhost:8888`,`curl klx get--data-urlencode q=’langchain’-d格式=json http://本地主机：8888`
This should return a JSON object with the results.,这应该返回一个JSON对象和结果。
To use the wrapper we need to pass the host of the SearxNG instance to the wrapper with:     1. the named parameter `searx_host` when creating the instance.     2. exporting the environment variable `SEARXNG_HOST`.,要使用包装器，我们需要将SearxNG实例的主机传递给包装器：1。创建实例时的命名参数“searx_host”。2.导出环境变量“SEARXNG_HOST”。
You can use the wrapper to get results from a SearxNG instance.,您可以使用包装器从SearxNG实例中获取结果。
You can also load this wrapper as a Tool (to use with an Agent).,您还可以将此包装器作为工具加载（与代理一起使用）。
You can do this with:,您可以使用以下方法执行此操作：
Note that we could _optionally_ pass custom engines to use.,请注意，我们可以_可选地_传递要使用的定制引擎。
If you want to obtain results with metadata as *json* you can use:,如果要获得元数据为*json*的结果，可以使用：
"For more information on tools, see [this page](../modules/agents/tools/getting_started.md)",有关工具的详细信息，请参阅[本页](../modules/agents/tools/getting_started.md）
SerpAPI,塞尔帕皮
"This page covers how to use the SerpAPI search APIs within LangChain. It is broken into two parts: installation and setup, and then references to the specific SerpAPI wrapper.",本页介绍了如何在LangChain中使用SerpAPI搜索APIs。它分为两部分：安装和设置，然后是对特定SerpAPI包装器的引用。
Install requirements with `pip install google-search-results`,使用“pip安装谷歌搜索结果”安装要求
Get a SerpAPI api key and either set it as an environment variable (`SERPAPI_API_KEY`),获取一个SerpAPI api密钥，并将其设置为环境变量（`SERPAPI_API_KEY`）
There exists a SerpAPI utility which wraps this API. To import this utility:,有一个SerpAPI实用程序包装了这个API。要导入此实用程序，请执行以下操作：
"For a more detailed walkthrough of this wrapper, see [this notebook](../modules/agents/tools/examples/serpapi.ipynb).",有关此包装器的更详细演练，请参见【本笔记本】（../模块/代理/工具/示例/serpapi.ipynb）。
StochasticAI,随机性
"This page covers how to use the StochasticAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific StochasticAI wrappers.",本页介绍了如何在LangChain中使用随机生态系统。它分为两部分：安装和设置，然后是对特定随机包装器的引用。
Install with `pip install stochasticx`,使用“pip install stochasticx”安装
Get an StochasticAI api key and set it as an environment variable (`STOCHASTICAI_API_KEY`),获取随机api密钥并将其设置为环境变量（`STOCHASTICAI_API_KEY`）
"There exists an StochasticAI LLM wrapper, which you can access with",存在一个随机LLM包装器，您可以使用
Unstructured,无结构的
This page covers how to use the [`unstructured`](https://github.com/Unstructured-IO/unstructured) ecosystem within LangChain. The `unstructured` package from [Unstructured.IO](https://www.unstructured.io/) extracts clean text from raw source documents like PDFs and Word documents.,本页介绍了如何使用【`非结构化`】（https：//github.com/Unstructured-IO/unstructureded）生态系统。来自【非结构化】的“非结构化”包。io】（https：//www。非结构化。io/）从原始源文档（如pdf和Word文档）中提取干净的文本。
"This page is broken into two parts: installation and setup, and then references to specific `unstructured` wrappers.",此页面分为两部分：安装和设置，然后是对特定“非结构化”包装器的引用。
"Install the Python SDK with `pip install ""unstructured[local-inference]""`",使用“pip install”安装Python SDK“非结构化【本地推理】”
"Install the following system dependencies if they are not already available on your system. Depending on what document types you're parsing, you may not need all of these.",如果您的系统上还没有以下系统依赖项，请安装它们。根据您正在解析的文档类型，您可能不需要所有这些。
`libmagic-dev` (filetype detection),`libmagic-dev`（文件类型检测）
`poppler-utils` (images and PDFs),“波普勒实用程序”（图像和pdf）
`tesseract-ocr`(images and PDFs),“宇宙魔方ocr”（图像和PDF）
`libreoffice` (MS Office docs),“libreoffice”(MS Office文档）
`pandoc` (EPUBs),“pandoc”(EPUBs）
"If you are parsing PDFs using the `""hi_res""` strategy, run the following to install the `detectron2` model, which `unstructured` uses for layout detection:",如果您使用“hi_res”策略解析PDF，请运行以下命令安装“Detectron 2”模型，该模型“非结构化”用于布局检测：
"`pip install ""detectron2@git+https://github.com/facebookresearch/detectron2.git@e2ce8dc#egg=detectron2""`",“pip安装”detectron 2@git+https://github.com/facebookresearch/detectron 2.git@e 2 ce 8 dc#egg=detectron 2”`
"If `detectron2` is not installed, `unstructured` will fallback to processing PDFs using the `""fast""` strategy, which uses `pdfminer` directly and doesn't require `detectron2`.",如果未安装“检测器2”，“非结构化”将退回到使用“快速”策略处理pdf，该策略直接使用“pdfminer”，不需要“检测器2”。
Data Loaders,数据加载器
The primary `unstructured` wrappers within `langchain` are data loaders. The following shows how to use the most basic unstructured data loader. There are other file-specific data loaders available in the `langchain.document_loaders` module.,“langchain”中的主要“非结构化”包装器是数据加载器。下面显示了如何使用最基本的非结构化数据加载器。在“langchain.document_loaders”模块中还有其他特定于文件的数据加载器。
"If you instantiate the loader with `UnstructuredFileLoader(mode=""elements"")`, the loader will track additional metadata like the page number and text type (i.e. title, narrative text) when that information is available.",如果您使用“UnstructuredFileLoader（mode=”elements”）”实例化加载器，当这些信息可用时，加载器将跟踪附加元数据，如页码和文本类型（即标题、叙述性文本）。
Weights & Biases,权重和偏差
This notebook goes over how to track your LangChain experiments into one centralized Weights and Biases dashboard. To learn more about prompt engineering and the callback please refer to this Report which explains both alongside the resultant dashboards you can expect to see.,本笔记本介绍了如何在一个集中的权重和偏差仪表板中跟踪您的LangChain实验。要了解有关提示工程和回调的更多信息，请参考此报告，该报告解释了这两个问题以及您可能会看到的结果仪表板。
Run in Colab: https://colab.research.google.com/drive/1DXH4beT4HFaRKy_Vm4PoxhXVDRf7Ym8L?usp=sharing,在colab中运行：https://colab.research.google.com/drive/1 dxh 4 bet 4 hfarky_vm 4 poxhxvdrf 7 ym 8 l？usp=共享
View Report: https://wandb.ai/a-sh0ts/langchain_callback_demo/reports/Prompt-Engineering-LLMs-with-LangChain-and-W-B--VmlldzozNjk1NTUw#👋-how-to-build-a-callback-in-langchain-for-better-prompt-engineering,查看报告：https://wandb.ai/a-sh 0 ts/langchain_callback_demo/reports/prompt-engineering-llms-with-langchain-and-w-b--vmlldzoznjk 1 ntuw#👋-how-to-build-a-callback-in-langchain-for-better-prompt-engineering
NOTE: For beta workflows we have made the default analysis based on textstat and the visualizations based on spacy,注意：对于测试版工作流，我们基于textstat进行了默认分析，基于spacy进行了可视化
"The `flush_tracker` function is used to log LangChain sessions to Weights & Biases. It takes in the LangChain module or agent, and logs at minimum the prompts and generations alongside the serialized form of the LangChain module to the specified Weights & Biases project. By default we reset the session as opposed to concluding the session outright.",“flush_tracker”函数用于记录权重和偏差的LangChain会话。它接受LangChain模块或代理，并至少将提示和生成以及LangChain模块的序列化形式记录到指定的Weights&Biases项目中。默认情况下，我们重置会话，而不是直接结束会话。
Weaviate,编织的
This page covers how to use the Weaviate ecosystem within LangChain.,本页介绍了如何在LangChain中使用Weaviate生态系统。
What is Weaviate?,什么是编织？
**Weaviate in a nutshell:**,**一言以蔽之：**
Weaviate is an open-source ​database of the type ​vector search engine.,Weaviate是一个向量搜索引擎类型的开源数据库。
Weaviate allows you to store JSON documents in a class property-like fashion while attaching machine learning vectors to these documents to represent them in vector space.,Weaviate允许您以类似类属性的方式存储JSON文档，同时将机器学习向量附加到这些文档，以在向量空间中表示它们。
Weaviate can be used stand-alone (aka bring your vectors) or with a variety of modules that can do the vectorization for you and extend the core capabilities.,Weaviate可以独立使用（也称为带来你的矢量），也可以与各种模块一起使用，这些模块可以为你进行矢量化并扩展核心功能。
Weaviate has a GraphQL-API to access your data easily.,Weaviate有一个GraphQL-API可以轻松访问您的数据。
We aim to bring your vector search set up to production to query in mere milliseconds (check our [open source benchmarks](https://weaviate.io/developers/weaviate/current/benchmarks/) to see if Weaviate fits your use case).,我们的目标是将您的矢量搜索设置到生产环境中，以便在几毫秒内进行查询（查看我们的【开源基准】（https：//weaviate.io/developers/Weaviate/current/benchmarks/），看看Weaviate是否适合您的用例）。
Get to know Weaviate in the [basics getting started guide](https://weaviate.io/developers/weaviate/current/core-knowledge/basics.html) in under five minutes.,在【基础入门指南】（https：//weaviate.io/developers/Weaviate/current/core-knowledge/basics.html）中了解Weaviate。
**Weaviate in detail:**,**详细编织：**
"Weaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), etc. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering and the fault tolerance of a cloud-native database. It is all accessible through GraphQL, REST, and various client-side programming languages.",Weaviate是一个低延迟矢量搜索引擎，支持不同的媒体类型（文本、图像等）。）.它提供语义搜索、问答提取、分类、可定制模型（PyTorch/TensorFlow/Keras）等。Weaviate在Go中从头开始构建，存储对象和向量，允许将向量搜索与结构化过滤和云原生数据库的容错相结合。这一切都可以通过GraphQL、REST和各种客户端编程语言来访问。
Install the Python SDK with `pip install weaviate-client`,使用“pip安装weaviate客户端”安装Python SDK`
"There exists a wrapper around Weaviate indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection.",Weaviate索引周围有一个包装器，允许您将它用作向量存储，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Weaviate wrapper, see [this notebook](../modules/indexes/vectorstores/examples/weaviate.ipynb)",有关Weaviate包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/weaviate.ipynb）
Wolfram Alpha Wrapper,Wolfram Alpha包装器
"This page covers how to use the Wolfram Alpha API within LangChain. It is broken into two parts: installation and setup, and then references to specific Wolfram Alpha wrappers.",本页介绍了如何在LangChain中使用Wolfram Alpha API。它分为两部分：安装和设置，然后是对特定Wolfram Alpha包装器的引用。
Install requirements with `pip install wolframalpha`,使用“pip install wolframalpha”安装要求
Go to wolfram alpha and sign up for a developer account [here](https://developer.wolframalpha.com/),转到wolfram alpha并注册一个开发者帐户[此处](https://developer.wolframalpha.com/）
Create an app and get your APP ID,创建应用程序并获取应用程序ID
Set your APP ID as an environment variable `WOLFRAM_ALPHA_APPID`,将应用程序ID设置为环境变量“WOLFRAM_ALPHA_APPID”
There exists a WolframAlphaAPIWrapper utility which wraps this API. To import this utility:,存在一个WolframAlphaAPIWrapper实用程序来包装这个API。要导入此实用程序，请执行以下操作：
"For a more detailed walkthrough of this wrapper, see [this notebook](../modules/agents/tools/examples/wolfram_alpha.ipynb).",有关此包装器的更详细演练，请参见[本笔记本]（../modules/agents/tools/examples/wolfram_alpha.ipynb）。
Writer,作家
"This page covers how to use the Writer ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Writer wrappers.",本页介绍了如何在LangChain中使用Writer生态系统。它分为两部分：安装和设置，然后是对特定Writer包装器的引用。
Get an Writer api key and set it as an environment variable (`WRITER_API_KEY`),获取一个编写器api密钥并将其设置为环境变量（`WRITER_API_KEY`）
"There exists an Writer LLM wrapper, which you can access with",存在一个Writer LLM包装器，您可以使用
Yeager.ai,耶格尔·艾
This page covers how to use [Yeager.ai](https://yeager.ai) to generate LangChain tools and agents.,本页介绍了如何使用【Yeager.ai】（https：//Yeager.ai）来生成LangChain工具和代理。
What is Yeager.ai?,什么是耶格尔。艾？
Yeager.ai is an ecosystem designed to simplify the process of creating AI agents and tools.,耶格尔。人工智能是一个生态系统，旨在简化创建人工智能代理和工具的过程。
"It features yAgents, a No-code LangChain Agent Builder, which enables users to build, test, and deploy AI solutions with ease. Leveraging the LangChain framework, yAgents allows seamless integration with various language models and resources, making it suitable for developers, researchers, and AI enthusiasts across diverse applications.",它以yAgents为特色，这是一个无代码的LangChain代理构建器，使用户能够轻松构建、测试和部署人工智能解决方案。利用LangChain框架，yAgents允许与各种语言模型和资源无缝集成，使其适合不同应用程序的开发人员、研究人员和人工智能爱好者。
yAgents,yAgents
"Low code generative agent designed to help you build, prototype, and deploy Langchain tools with ease.",低代码生成代理，旨在帮助您轻松构建、原型化和部署Langchain工具。
How to use?,怎么用？
Go to http://127.0.0.1:7860,转到http://127.0.0.1:7860
"This will install the necessary dependencies and set up yAgents on your system. After the first run, yAgents will create a .env file where you can input your OpenAI API key. You can do the same directly from the Gradio interface under the tab ""Settings"".",这将在您的系统上安装必要的依赖项并设置yAgents。第一次运行后，yAgents将创建一个。env文件，您可以在其中输入OpenAI API密钥。您可以直接从Gradio界面的“设置”选项卡下执行相同的操作。
`OPENAI_API_KEY=<your_openai_api_key_here>`,`openai_api_key=<your_openai_api_key_here>`
"We recommend using GPT-4,. However, the tool can also work with GPT-3 if the problem is broken down sufficiently.",我们建议使用GPT-4。然而，如果问题被充分分解，该工具也可以与GPT-3一起工作。
Creating and Executing Tools with yAgents,使用yAgents创建和执行工具
yAgents makes it easy to create and execute AI-powered tools. Here's a brief overview of the process:,yAgents使创建和执行人工智能工具变得容易。以下是流程的简要概述：
"Create a tool: To create a tool, provide a natural language prompt to yAgents. The prompt should clearly describe the tool's purpose and functionality. For example: `create a tool that returns the n-th prime number`",创建工具：要创建工具，请向yAgents提供自然语言提示。提示应该清楚地描述工具的用途和功能。例如：`创建一个返回第n个质数的工具`
"Load the tool into the toolkit: To load a tool into yAgents, simply provide a command to yAgents that says so. For example: `load the tool that you just created it into your toolkit`",将工具加载到工具包中：要将工具加载到yAgents，只需向yAgents提供一个命令。例如：`将刚刚创建的工具加载到工具包中`
"Execute the tool: To run a tool or agent, simply provide a command to yAgents that includes the name of the tool and any required parameters. For example: `generate the 50th prime number`",执行工具：要运行工具或代理，只需向yAgents提供一个命令，其中包括工具的名称和任何必需的参数。例如：“生成第50个质数”
You can see a video of how it works [here](https://www.youtube.com/watch?v=KA5hCM3RaWE).,你可以在这里看到它是如何工作的视频（https：//www.youtube.com/watch？v=Ka 5 HCM 3 Rawe）。
"As you become more familiar with yAgents, you can create more advanced tools and agents to automate your work and enhance your productivity.",随着您对yAgents越来越熟悉，您可以创建更高级的工具和代理来自动化您的工作并提高您的工作效率。
"For more information, see [yAgents' Github](https://github.com/yeagerai/yeagerai-agent) or our [docs](https://yeagerai.gitbook.io/docs/general/welcome-to-yeager.ai)",欲了解更多信息，请参阅[yAgents'Github](https://github.com/yeagerai/yeagerai-agent）或我们的[docs](https://yeagerai.gitbook.io/docs/general/welcome-to-yeager.ai）
Zilliz,齐利兹
"This page covers how to use the Zilliz Cloud ecosystem within LangChain. Zilliz uses the Milvus integration.  It is broken into two parts: installation and setup, and then references to specific Milvus wrappers.",本页介绍了如何在LangChain中使用Zilliz云生态系统。Zilliz使用Milvus集成。它分为两部分：安装和设置，然后是对特定Milvus包装器的引用。
"There exists a wrapper around Zilliz indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection.",Zilliz索引周围有一个包装器，允许您将它用作vectorstore，无论是用于语义搜索还是示例选择。
"For a more detailed walkthrough of the Miluvs wrapper, see [this notebook](../modules/indexes/vectorstores/examples/zilliz.ipynb)",有关Miluvs包装器的更详细演练，请参见[本笔记本]（../modules/indexes/vectorstores/examples/zilliz.ipynb）
LangChain Gallery,LangChain画廊
"Lots of people have built some pretty awesome stuff with LangChain. This is a collection of our favorites. If you see any other demos that you think we should highlight, be sure to let us know!",很多人用LangChain构建了一些非常棒的东西。这是我们最喜欢的收藏。如果您看到任何其他您认为我们应该强调的演示，请务必告诉我们！
Open Source,开放源码
"This is an experiment in building a large-language-model-backed chatbot. It can hold a conversation, remember previous comments/questions, and answer all types of queries (history, web search, movie data, weather, news, and more).",这是一个构建大型语言模型支持的聊天机器人的实验。它可以进行对话，记住以前的评论/问题，并回答所有类型的查询（历史、网络搜索、电影数据、天气、新闻等）。
"An end-to-end example of doing question answering on YouTube transcripts, returning the timestamps as sources to legitimize the answer.",在YouTube抄本上做问题回答的端到端示例，返回时间戳作为来源以使答案合法化。
This application is a Slack Bot that uses Langchain and OpenAI's GPT3 language model to provide domain specific answers. You provide the documents.,这个应用程序是一个Slack Bot，它使用Langchain和OpenAI的GPT 3语言模型来提供特定领域的答案。你提供文件。
"A central, open resource and community around data and tools related to chain-of-thought reasoning in large language models.",围绕大型语言模型中与思维链推理相关的数据和工具的中心、开放资源和社区。
"This Python package adds a decorator llm_strategy that connects to an LLM (such as OpenAI’s GPT-3) and uses the LLM to ""implement"" abstract methods in interface classes. It does this by forwarding requests to the LLM and converting the responses back to Python data using Python's @dataclasses.",这个Python包添加了一个decorator llm_strategy，它连接到一个LLM（比如OpenAI的GPT-3），并使用LLM在接口类中“实现”抽象方法。它通过将请求转发到LLM并使用Python的@dataclasses将响应转换回Python数据来实现这一点。
A notebook showing how to use GPT to help with the work of a corporate lobbyist.,一本展示如何利用GPT帮助企业游说者工作的笔记本。
A jupyter notebook demonstrating how you could create a semantic search engine on documents in one of your Google Folders,jupyter笔记本演示了如何在Google文件夹中的文档上创建语义搜索引擎
"Build a GitHub support bot with GPT3, LangChain, and Python.",用GPT 3、LangChain和Python构建一个GitHub支持机器人。
"Record sounds of anything (birds, wind, fire, train station) and chat with it.",记录任何东西的声音（鸟、风、火、火车站）并与之聊天。
"This simple application demonstrates a conversational agent implemented with OpenAI GPT-3.5 and LangChain. When necessary, it leverages tools for complex math, searching the internet, and accessing news and weather.",这个简单的应用程序演示了一个用OpenAI GPT-3.5和LangChain实现的会话代理。必要时，它利用复杂的数学工具，搜索互联网，访问新闻和天气。
A Hugging Face spaces project showing off the benefits of using PAL for math problems.,一个拥抱面部空间项目展示了使用PAL解决数学问题的好处。
Measure the political compass of GPT.,衡量GPT的政治指南针。
Open source GitHub project shows how to use LangChain to create a chatbot that can answer questions about an arbitrary Notion database.,开源GitHub项目展示了如何使用LangChain创建一个聊天机器人，它可以回答关于任意概念数据库的问题。
LlamaIndex (formerly GPT Index) is a project consisting of a set of data structures that are created using GPT-3 and can be traversed using GPT-3 in order to answer queries.,LlamaIndex（以前的GPT索引）是一个由一组使用GPT-3创建的数据结构组成的项目，可以使用GPT-3遍历这些数据结构以回答查询。
"Leveraging Qiskit, OpenAI and LangChain to demonstrate Grover's algorithm",利用Qiskit、OpenAI和LangChain演示Grover算法
"A chat UI to play Nim, where a player can select an opponent, either a quantum computer or an AI",一个玩Nim的聊天界面，玩家可以选择一个对手，一个量子计算机或者一个AI
Leveraging the ReActTextWorldAgent to play TextWorld with an LLM!,利用ReActTextWorldAgent用LLM玩文本世界！
This repo is a simple demonstration of using LangChain to do fact-checking with prompt chaining.,这个repo是使用LangChain通过提示链接进行事实检查的简单演示。
Answer questions about the documentation of any project,回答有关任何项目文档的问题
Misc. Colab Notebooks,杂项Colab笔记本
Give ChatGPT a WolframAlpha neural implant,给ChatGPT植入WolframAlpha神经
Agent improvements (6th Jan 2023),代理改进（2023年1月6日）
Langchain AGI (23rd Dec 2022),Langchain AGI（2022年12月23日）
Proprietary,专有的
A chat-based AI personal assistant with long-term memory about you.,一个基于聊天的人工智能个人助理，对你有长期记忆。
"Summarize not only long docs, interview audio or video files quickly, but also entire websites and YouTube videos. Share or download your generated summaries to collaborate with others, or revisit them at any time! Bonus: `@anysummary <https://twitter.com/anysummary>`_ on Twitter will also summarize any thread it is tagged in.",不仅可以总结长文档，快速采访音频或视频文件，还可以总结整个网站和YouTube视频。共享或下载您生成的摘要以与他人协作，或随时重温它们！额外奖励：Twitter上的`@anysummary<https：//twitter.com/anysummary>`_还会总结它被标记的任何线程。
"An app to write SQL using natural language, and execute against real DB.",一个使用自然语言编写SQL并对真实数据库执行的应用程序。
Stack Tracing QA Bot to help debug complex stack tracing (especially the ones that go multi-function/file deep).,堆栈跟踪QA Bot帮助调试复杂的堆栈跟踪（尤其是多功能/文件深度的堆栈跟踪）。
"By Raza Habib, this demo utilizes LangChain + SerpAPI + HumanLoop to write sales emails. Give it a company name and a person, this application will use Google Search (via SerpAPI) to get more information on the company and the person, and then write them a sales message.",由Raza Habib制作，这个演示利用LangChain+SerpAPI+HumanLoop来编写销售电子邮件。给它一个公司名称和一个人，这个应用程序将使用谷歌搜索（通过SerpAPI）来获得更多关于公司和这个人的信息，然后给他们写一条销售信息。
"By Zahid Khawaja, this demo utilizes question answering to answer questions about a given website. A followup added this for `YouTube videos <https://twitter.com/chillzaza_/status/1593739682013220865?s=20&t=EhU8jl0KyCPJ7vE9Rnz-cQ>`_, and then another followup added it for `Wikipedia <https://twitter.com/chillzaza_/status/1594847151238037505?s=20&t=EhU8jl0KyCPJ7vE9Rnz-cQ>`_.",由Zahid Khawaja，这个演示利用问答来回答关于给定网站的问题。一个后续为“YouTube视频<https：//twitter”添加了这个。com/chill zaza_/status/1593739682013220865？s=20&t=ehu 8 jl 0 kycpj 7 ve 9 rnz-cq>`_，然后另一个后续为`Wikipedia<https：//twitter.com/chill zaza_/status/1594847151238037505？s=20&t=ehu 8 jl 0 kycpj 7 ve 9 rnz-cq>`_添加了这个。
A journaling app for self-care that uses AI to uncover insights and patterns over time.,一个用于自我保健的日志应用程序，使用人工智能来揭示一段时间内的见解和模式。
Quickstart Guide,快速入门指南
This tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.,本教程为您提供了一个关于使用LangChain构建端到端语言模型应用程序的快速演练。
"To get started, install LangChain with the following command:",若要开始，请使用以下命令安装LangChain：
Environment Setup,环境设置
"Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.",使用LangChain通常需要与一个或多个模型提供者、数据存储、API等集成。
"For this example, we will be using OpenAI's APIs, so we will first need to install their SDK:",对于本例，我们将使用OpenAI的API，因此我们首先需要安装他们的SDK：
We will then need to set the environment variable in the terminal.,然后，我们需要在终端中设置环境变量。
"Alternatively, you could do this from inside the Jupyter notebook (or Python script):",或者，您可以在Jupyter笔记本（或Python脚本）中执行此操作：
Building a Language Model Application: LLMs,构建语言模型应用程序：LLMs
"Now that we have installed LangChain and set up our environment, we can start building our language model application.",现在我们已经安装了LangChain并设置了我们的环境，我们可以开始构建我们的语言模型应用程序了。
"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.",LangChain提供了许多可用于构建语言模型应用程序的模块。模块可以组合起来创建更复杂的应用程序，也可以单独用于简单的应用程序。
LLMs: Get predictions from a language model,LLMs：从语言模型中获取预测
"The most basic building block of LangChain is calling an LLM on some input. Let's walk through a simple example of how to do this.  For this purpose, let's pretend we are building a service that generates a company name based on what the company makes.",LangChain最基本的构建模块是对一些输入调用LLM。让我们通过一个简单的例子来说明如何做到这一点。为此，让我们假设我们正在构建一个基于公司产品生成公司名称的服务。
"In order to do this, we first need to import the LLM wrapper.",为此，我们首先需要导入LLM包装器。
"We can then initialize the wrapper with any arguments. In this example, we probably want the outputs to be MORE random, so we'll initialize it with a HIGH temperature.",然后，我们可以用任何参数初始化包装器。在本例中，我们可能希望输出更加随机，因此我们将用高温初始化它。
We can now call it on some input!,我们现在可以根据一些输入调用它了！
"For more details on how to use LLMs within LangChain, see the [LLM getting started guide](../modules/models/llms/getting_started.ipynb).",有关如何在LangChain中使用LLM的更多详细信息，请参见[LLM入门指南]（../modules/models/LLMs/getting_started.ipynb）。
Prompt Templates: Manage prompts for LLMs,提示模板：管理LLMs的提示
"Calling an LLM is a great first step, but it's just the beginning. Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.",打电话给LLM是很好的第一步，但这仅仅是开始。通常，当您在应用程序中使用LLM时，您不会将用户输入直接发送到LLM。相反，您可能会接受用户输入并构造一个提示，然后将其发送到LLM。
"For example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks. In this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.",例如，在前面的示例中，我们传入的文本被硬编码，以询问一家生产彩色袜子的公司的名称。在这个假想的服务中，我们想要做的是只获取描述公司工作的用户输入，然后用该信息格式化提示。
This is easy to do with LangChain!,用LangChain很容易做到这一点！
First lets define the prompt template:,首先让我们定义提示模板：
Let's now see how this works! We can call the `.format` method to format it.,现在让我们看看这是如何工作的！我们可以调用`.format’方法来格式化它。
"[For more details, check out the getting started guide for prompts.](../modules/prompts/chat_prompt_template.ipynb)",[有关更多详细信息，请查看提示入门指南。](../modules/prompts/chat_prompt_template.ipynb）
Chains: Combine LLMs and prompts in multi-step workflows,链：在多步骤工作流中组合LLM和提示
"Up until now, we've worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.",到目前为止，我们已经单独使用了PromptTemplate和LLM原语。但是当然，一个真正的应用程序不仅仅是一个原语，而是它们的组合。
"A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.",LangChain中的链由链接组成，链接可以是LLMs之类的原语，也可以是其他链。
"The most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.",最核心的链类型是LLMChain，它由PromptTemplate和LLM组成。
"Extending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.",扩展前面的例子，我们可以构造一个LLMChain，它接受用户输入，用PromptTemplate格式化它，然后将格式化的响应传递给LLM。
"We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:",我们现在可以创建一个非常简单的链，它将接受用户输入，用它格式化提示符，然后将其发送到LLM：
Now we can run that chain only specifying the product!,现在我们可以只指定产品来运行该链了！
"There we go! There's the first chain - an LLM Chain. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.",我们走吧！这是第一条链LLM链。这是较简单的链类型之一，但是了解它的工作原理将使您能够很好地处理更复杂的链。
"[For more details, check out the getting started guide for chains.](../modules/chains/getting_started.ipynb)",[有关更多详细信息，请查看链入门指南。](../modules/chains/getting_started.ipynb）
Agents: Dynamically Call Chains Based on User Input,代理：基于用户输入动态调用链
So far the chains we've looked at run in a predetermined order.,到目前为止，我们看到的链是按照预定的顺序运行的。
"Agents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.",代理不再这样做：他们使用LLM来决定采取哪些行动以及采取什么顺序。动作可以是使用工具并观察其输出，也可以是返回给用户。
"When used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.",如果使用得当，代理可以非常强大。在本教程中，我们将向您展示如何通过最简单、最高级别的API轻松使用代理。
"In order to load agents, you should understand the following concepts:",为了加载代理，您应该了解以下概念：
"Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.",工具：执行特定职责的功能。这可以是：谷歌搜索，数据库查找，Python REPL，其他链。工具的接口目前是一个函数，它期望有一个字符串作为输入，一个字符串作为输出。
LLM: The language model powering the agent.,LLM：为代理提供动力的语言模型。
"Agent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see the documentation for custom agents (coming soon).",代理：要使用的代理。这应该是一个引用支持代理类的字符串。因为本笔记本侧重于最简单、最高级别的API，所以这只包括使用标准支持的代理。如果您想要实现自定义代理，请参见自定义代理的文档（即将推出）。
"**Agents**: For a list of supported agents and their specifications, see [here](../modules/agents/agents.md).",**代理**：有关支持的代理及其规范的列表，请参见[此处]（../modules/agents/agents.md）。
"**Tools**: For a list of predefined tools and their specifications, see [here](../modules/agents/tools.md).",**工具**：有关预定义工具及其规格的列表，请参见[此处]（../modules/agents/tools.md）。
"For this example, you will also need to install the SerpAPI Python package.",对于本例，您还需要安装SerpAPI Python包。
And set the appropriate environment variables.,并设置适当的环境变量。
Now we can get started!,现在我们可以开始了！
Memory: Add State to Chains and Agents,内存：向链和代理添加状态
"So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of ""memory"" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of ""short-term memory"". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of ""long-term memory"". For more concrete ideas on the latter, see this [awesome paper](https://memprompt.com/).",到目前为止，我们经历过的所有连锁店和代理商都是无国籍的。但通常，您可能希望链或代理具有一些“记忆”的概念，以便它可以记住关于其先前交互的信息。这方面最清晰和简单的例子是在设计聊天机器人时——你希望它记住以前的消息，这样它就可以使用上下文进行更好的对话。这将是一种“短期记忆”。在更复杂的方面，你可以想象一个链/代理随着时间的推移记住关键信息——这将是一种“长期记忆”的形式。关于后者的更多具体想法，请参见这篇【awesome paper】（https：//memprompt.com/）。
LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the `ConversationChain`) with two different types of memory.,LangChain专门为此提供了几个专门创建的链。这款笔记本使用其中一个链（“ConversationChain”）和两种不同类型的内存。
"By default, the `ConversationChain` has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let's take a look at using this chain (setting `verbose=True` so we can see the prompt).",默认情况下，“ConversationChain”有一个简单类型的内存，它记住所有以前的输入/输出，并将它们添加到传递的上下文中。让我们看看如何使用这个链（设置“verbose=True”，这样我们就可以看到提示）。
Building a Language Model Application: Chat Models,构建语言模型应用程序：聊天模型
"Similarly, you can use chat models instead of LLMs. Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a ""text in, text out"" API, they expose an interface where ""chat messages"" are the inputs and outputs.",同样，您可以使用聊天模型来代替LLM。聊天模型是语言模型的变体。虽然聊天模型在引擎盖下使用语言模型，但它们公开的接口有点不同：它们不是公开“文本输入，文本输出”API，而是公开一个接口，其中“聊天消息”是输入和输出。
"Chat model APIs are fairly new, so we are still figuring out the correct abstractions.",聊天模型API相当新，所以我们仍在找出正确的抽象。
Get Message Completions from a Chat Model,从聊天模型获取消息完成
"You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`.",您可以通过向聊天模型传递一条或多条消息来完成聊天。响应将是一条消息。LangChain目前支持的消息类型有“AIMessage”、“HumanMessage”、“SystemMessage”和“ChatMessage”-“ChatMessage”接受任意角色参数。大多数时候，你只是在处理“人类信息”、“目标信息”和“系统信息”。
You can get completions by passing in a single message.,您可以通过传入一条消息来完成。
You can also pass in multiple messages for OpenAI's gpt-3.5-turbo and gpt-4 models.,你也可以为OpenAI的gpt-3.5-turbo和gpt-4型号传递多条消息。
You can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter:,您可以更进一步，使用“generate”为多组消息生成完成。这将返回一个带有附加“message”参数的“LLMResult”：
You can recover things like token usage from this LLMResult:,您可以从这个LLMResult恢复令牌使用情况：
Chat Prompt Templates,聊天提示模板
"Similar to LLMs, you can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplate`s. You can use `ChatPromptTemplate`'s `format_prompt` -- this returns a `PromptValue`, which you can convert to a string or `Message` object, depending on whether you want to use the formatted value as input to an llm or chat model.",与LLMs类似，您可以通过使用“MessagePromptTemplate”来利用模板。您可以从一个或多个“消息提示模板”构建“聊天提示模板”。您可以使用“ChatPromptTemplate”的“format_prompt”——这将返回一个“PromptValue”，您可以将其转换为字符串或“Message”对象，这取决于您是想使用格式化值作为llm还是聊天模型的输入。
"For convience, there is a `from_template` method exposed on the template. If you were to use this template, this is what it would look like:",为了方便起见，在模板上公开了一个“from_template”方法。如果要使用此模板，它将如下所示：
Chains with Chat Models,带有聊天模型的链
The `LLMChain` discussed in the above section can be used with chat models as well:,上一节中讨论的“LLMChain”也可以用于聊天模型：
Agents with Chat Models,具有聊天模型的座席
"Agents can also be used with chat models, you can initialize one using `AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION` as the agent type.",代理也可以与聊天模型一起使用，您可以使用“AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION”作为代理类型来初始化一个代理。
"You can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object.",您可以将内存用于用聊天模型初始化的链和代理。这与LLM的内存之间的主要区别在于，我们可以将它们作为自己唯一的内存对象，而不是试图将所有以前的消息压缩到一个字符串中。
Glossary,词汇表
"This is a collection of terminology commonly used when developing LLM applications. It contains reference to external papers or sources where the concept was first introduced, as well as to places in LangChain where the concept is used.",这是开发LLM应用程序时常用的术语集。它包含对首次引入该概念的外部论文或来源的引用，以及对LangChain中使用该概念的地方的引用。
Chain of Thought Prompting,思维链提示
A prompting technique used to encourage the model to generate a series of intermediate reasoning steps. A less formal way to induce this behavior is to include “Let’s think step-by-step” in the prompt.,一种提示技术，用于鼓励模型生成一系列中间推理步骤。诱导这种行为的一种不太正式的方式是在提示中包含“让我们一步一步地思考”。
Resources:,资源：
[Chain-of-Thought Paper](https://arxiv.org/pdf/2201.11903.pdf),[思维链文件](https://arxiv.org/pdf/2201.11903.pdf）
[Step-by-Step Paper](https://arxiv.org/abs/2112.00114),[分步文件](https://arxiv.org/abs/2112.00114）
Action Plan Generation,行动计划生成
A prompt usage that uses a language model to generate actions to take. The results of these actions can then be fed back into the language model to generate a subsequent action.,使用语言模型生成要采取的操作的提示用法。然后，这些动作的结果可以反馈到语言模型中，以生成后续的动作。
[WebGPT Paper](https://arxiv.org/pdf/2112.09332.pdf),[WebGPT文件](https://arxiv.org/pdf/2112.09332.pdf）
[SayCan Paper](https://say-can.github.io/assets/palm_saycan.pdf),[SayCan Paper](https://say-can.github.io/assets/palm_saycan.pdf）
ReAct Prompting,反应提示
"A prompting technique that combines Chain-of-Thought prompting with action plan generation. This induces the to model to think about what action to take, then take it.",一种将思维链提示与行动计划生成相结合的提示技术。这导致to模型考虑采取什么行动，然后采取行动。
[Paper](https://arxiv.org/pdf/2210.03629.pdf),[论文](https://arxiv.org/pdf/2210.03629.pdf）
[LangChain Example](modules/agents/agents/examples/react.ipynb),[LangChain示例]（模块/代理/代理/示例/react.ipynb）
Self-ask,自问
"A prompting method that builds on top of chain-of-thought prompting. In this method, the model explicitly asks itself follow-up questions, which are then answered by an external search engine.",一种建立在思维链提示之上的提示方法。在这种方法中，模型明确地向自己询问后续问题，然后由外部搜索引擎回答这些问题。
[Paper](https://ofir.io/self-ask.pdf),[论文](https://ofir.io/self-ask.pdf）
[LangChain Example](modules/agents/agents/examples/self_ask_with_search.ipynb),[LangChain示例](modules/agents/agents/examples/self_ask_with_search.ipynb）
Prompt Chaining,提示链接
"Combining multiple LLM calls together, with the output of one-step being the input to the next.",将多个LLM调用组合在一起，一步的输出是下一步的输入。
[PromptChainer Paper](https://arxiv.org/pdf/2203.06566.pdf),[PromptChainer Paper](https://arxiv.org/pdf/2203.06566.pdf）
[Language Model Cascades](https://arxiv.org/abs/2207.10342),[语言模型级联](https://arxiv.org/abs/2207.10342）
[ICE Primer Book](https://primer.ought.org/),[ICE入门手册](https://primer.ought.org/)
[Socratic Models](https://socraticmodels.github.io/),[苏格拉底模型](https://socraticmodels.github.io/）
Memetic Proxy,模因代理
"Encouraging the LLM to respond in a certain way framing the discussion in a context that the model knows of and that will result in that type of response. For example, as a conversation between a student and a teacher.",鼓励LLM以某种方式做出回应，在模型知道的背景下构建讨论，并产生这种类型的回应。例如，作为学生和老师之间的对话。
[Paper](https://arxiv.org/pdf/2102.07350.pdf),[论文](https://arxiv.org/pdf/2102.0735.pdf）
Self Consistency,自我一致性
A decoding strategy that samples a diverse set of reasoning paths and then selects the most consistent answer. Is most effective when combined with Chain-of-thought prompting.,一种解码策略，对一组不同的推理路径进行采样，然后选择最一致的答案。当与思维链提示相结合时最有效。
[Paper](https://arxiv.org/pdf/2203.11171.pdf),[论文](https://arxiv.org/pdf/2203.11171.pdf）
Inception,开始
Also called “First Person Instruction”. Encouraging the model to think a certain way by including the start of the model’s response in the prompt.,也称为“第一人称教学”。通过在提示中包含模型响应的开始，鼓励模型以某种方式思考。
[Example](https://twitter.com/goodside/status/1583262455207460865?s=20&t=8Hz7XBnK1OF8siQrxxCIGQ),[示例](https://twitter.com/goodside/status/1583262455207460865？s=20&t=8 Hz 7 xBNK 1/8 SIQRxxCIGQ）
MemPrompt,记忆提示符
"MemPrompt maintains a memory of errors and user feedback, and uses them to prevent repetition of mistakes.",MemPrompt维护错误和用户反馈的内存，并使用它们来防止错误的重复。
[Paper](https://memprompt.com/),[论文](https://memprompt.com/）
Getting Started,入门
Modules,模块
Use Cases,用例
Reference,参考
Ecosystem,生态系统
LangChainHub,长链轮毂
Discord,不和谐
Production Support,生产支持
Additional Resources,额外资源
Welcome to LangChain,欢迎来到LangChain
"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:",LangChain是一个开发由语言模型驱动的应用程序的框架。我们相信，最强大、最差异化的应用程序不仅会通过API调用语言模型，还会：
*Be data-aware*: connect a language model to other sources of data,*了解数据*：将语言模型连接到其他数据源
*Be agentic*: allow a language model to interact with its environment,*代理*：允许语言模型与其环境交互
The LangChain framework is designed with the above principles in mind.,LangChain框架的设计考虑了上述原则。
"This is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see `here <https://docs.langchain.com/docs/>`_. For the JavaScript documentation, see `here <https://js.langchain.com/docs/>`_.",这是文档中特定于Python的部分。有关LangChain的纯概念性指南，请参见`here<https://docs.langchain.com/docs/>`_。有关JavaScript文档，请参见`here<https://js.langchain.com/docs/>`_。
Checkout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.,查看下面的指南，了解如何开始使用LangChain创建语言模型应用程序。
`Getting Started Documentation <./getting_started/getting_started.html>`_,`入门文档<./getting_started/getting_started.html>`_
"There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity:",LangChain支持几个主要模块。对于每个模块，我们都提供了一些入门示例、操作指南、参考文档和概念指南。这些模块按复杂程度递增的顺序排列：
`Models <./modules/models.html>`_: The various model types and model integrations LangChain supports.,`models<./modules/models.html>`_:LangChain支持的各种模型类型和模型集成。
"`Prompts <./modules/prompts.html>`_: This includes prompt management, prompt optimization, and prompt serialization.",`prompts<./modules/prompts.html>`_:这包括提示管理、提示优化和提示序列化。
"`Memory <./modules/memory.html>`_: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.",`Memory<./modules/memory.html>`_:Memory是在链/代理的调用之间保持状态的概念。LangChain提供了内存的标准接口、内存实现的集合以及使用内存的链/代理的示例。
`Indexes <./modules/indexes.html>`_: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.,`indexes<./modules/indexes.html>`_:当与您自己的文本数据结合时，语言模型通常更强大—本模块涵盖了实现这一点的最佳实践。
"`Chains <./modules/chains.html>`_: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.",`Chains<./modules/chains.html>`_:Chains不仅仅是一个LLM调用，而是一系列调用（无论是对LLM还是其他实用程序）。LangChain为链提供了一个标准接口，与其他工具的大量集成，以及常见应用程序的端到端链。
"`Agents <./modules/agents.html>`_: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.",`Agents<./modules/agents.html>`_:Agents涉及到一个LLM决定采取哪些行动，采取那个行动，看到一个观察结果，并重复这个操作直到完成。LangChain为代理提供了一个标准接口、一系列可供选择的代理以及端到端代理的示例。
The above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.,上述模块可以以多种方式使用。LangChain也在这方面提供指导和帮助。以下是LangChain支持的一些常见用例。
`Autonomous Agents <./use_cases/autonomous_agents.html>`_: Autonomous agents are long running agents that take many steps in an attempt to accomplish an objective. Examples include AutoGPT and BabyAGI.,`Autonomous Agents<./use_cases/autonomous_agents.html>`_：自治Agent是长时间运行的Agent，它采取许多步骤来尝试完成一个目标。例子包括AutoGPT和BabyAGI。
`Agent Simulations <./use_cases/agent_simulations.html>`_: Putting agents in a sandbox and observing how they interact with each other or to events can be an interesting way to observe their long-term memory abilities.,`Agent Simulations<./use_cases/agent_simulations.html>`_：将Agent放入沙盒中，观察它们如何相互交互或与事件交互，这是观察它们长期记忆能力的一种有趣方法。
"`Personal Assistants <./use_cases/personal_assistants.html>`_: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.",`Personal Assistants<./use_cases/personal_assistants.html>`_:主要的LangChain用例。个人助理需要采取行动，记住互动，并了解你的数据。
"`Question Answering <./use_cases/question_answering.html>`_: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.",`Question Answering<./use_cases/question_answering.html>`_：第二个大型LangChain用例。回答特定文档上的问题，仅利用这些文档中的信息来构建答案。
"`Chatbots <./use_cases/chatbots.html>`_: Since language models are good at producing text, that makes them ideal for creating chatbots.",`chatbots<./use_cases/chatbots.html>`_：由于语言模型擅长生成文本，因此非常适合创建聊天机器人。
"`Querying Tabular Data <./use_cases/tabular.html>`_: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.",“查询表格数据<./use_cases/tabular.html>`_：如果您想了解如何使用LLMs查询以表格格式存储的数据（CSV、SQL、数据帧等），您应该阅读本页。
"`Code Understanding <./use_cases/code.html>`_: If you want to understand how to use LLMs to query source code from github, you should read this page.",“代码理解<./use_cases/code.html>`_：如果你想了解如何使用LLMs从github查询源代码，你应该阅读这个页面。
`Interacting with APIs <./use_cases/apis.html>`_: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.,`与API交互<./use_cases/apis.html>`_：使LLM能够与API交互是非常强大的，以便为它们提供更多最新的信息并允许它们采取行动。
`Extraction <./use_cases/extraction.html>`_: Extract structured information from text.,`extraction<./use_cases/extraction.html>`_:从文本中提取结构化信息。
"`Summarization <./use_cases/summarization.html>`_: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.",`summarization<./use_cases/summarization.html>`_：将较长的文档汇总为更短、更浓缩的信息块。一种数据增强生成。
`Evaluation <./use_cases/evaluation.html>`_: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.,`evaluation<./use_cases/evaluation.html>`_：众所周知，生成模型很难用传统的度量标准进行评估。评估它们的一种新方法是使用语言模型本身来进行评估。LangChain提供了一些提示/链来帮助实现这一点。
Reference Docs,参考单据
"All of LangChain's reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.",LangChain的所有参考文档都在一个地方。关于LangChain的所有方法、类、安装方法和集成设置的完整文档。
`Reference Documentation <./reference.html>`_,`参考文档<./reference.html>`_
`LangChain Ecosystem <./ecosystem.html>`_,`LangChain生态系统<./ecystology.html>`_
Additional collection of resources we think may be useful as you develop your application!,我们认为在您开发应用程序时可能有用的其他资源集合！
"`LangChainHub <https://github.com/hwchase17/langchain-hub>`_: The LangChainHub is a place to share and explore other prompts, chains, and agents.",`LangChainHub<https://github.com/hwchase 17/langchain-hub>`_:LangChainHub是一个共享和探索其他提示、链和代理的地方。
"`Glossary <./glossary.html>`_: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!",`glossary<./glossary.html>`_：所有相关术语、论文、方法等的词汇表。无论是否在LangChain中实现！
`Gallery <./gallery.html>`_: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.,`gallery<./gallery.html>`_：我们最喜欢的使用LangChain的项目的集合。对于寻找灵感或查看其他应用程序中的工作方式非常有用。
"`Deployments <./deployments.html>`_: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.",`deployments<./deployments.html>`_:用于部署LangChain应用程序的指令、代码片段和模板库的集合。
`Tracing <./tracing.html>`_: A guide on using tracing in LangChain to visualize the execution of chains and agents.,`tracing<./tracing.html>`_：关于在LangChain中使用跟踪来可视化链和代理的执行的指南。
"`Model Laboratory <./model_laboratory.html>`_: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.",`Model Laboratory<./model_laboratory.html>`_：试验不同的提示、模型和链是开发最佳应用程序的重要部分。模型实验室使这样做变得容易。
`Discord <https://discord.gg/6adMQxSpJS>`_: Join us on our Discord to discuss all things LangChain!,`Discord<https：//discord.gg/6 admqxspjs>`_:加入我们的Discord，讨论所有关于LangChain的事情！
`YouTube <./youtube.html>`_: A collection of the LangChain tutorials and videos.,`youtube<./youtube.html>`_：语言链教程和视频的集合。
"`Production Support <https://forms.gle/57d8AmXBYp8PP8tZA>`_: As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.",`生产支持<https://forms.gle/57 d 8 amxbyp 8 pp 8 tza>`_:当您将LangChains投入生产时，我们很乐意为您提供更全面的支持。请填写此表格，我们将建立一个专门的支持松弛渠道。
Model Comparison,模型比较
"Constructing your language model application will likely involved choosing between many different options of prompts, models, and even chains to use. When doing so, you will want to compare these different options on different inputs in an easy, flexible, and intuitive way.",构建您的语言模型应用程序可能会涉及到在许多不同的提示、模型甚至要使用的链选项之间进行选择。这样做时，您会希望以简单、灵活和直观的方式比较不同输入上的这些不同选项。
LangChain provides the concept of a ModelLaboratory to test out and try different models.,LangChain提供了一个模型实验室的概念来测试和尝试不同的模型。
API References,API引用
"All of LangChain's reference documentation, in one place. Full documentation on all methods, classes, and APIs in LangChain.",LangChain的所有参考文档都在一个地方。LangChain中所有方法、类和API的完整文档。
Agents,代理
Reference guide for Agents and associated abstractions.,代理和相关抽象参考指南。
Indexes,索引
"Indexes refer to ways to structure documents so that LLMs can best interact with them. LangChain has a number of modules that help you load, structure, store, and retrieve documents.",索引指的是构建文档的方式，以便LLM能够最好地与它们交互。LangChain有许多模块可以帮助您加载、构建、存储和检索文档。
Official Releases,官方发布
"LangChain is available on PyPi, so to it is easily installable with:",LangChain在PyPi上可用，因此可以通过以下方式轻松安装：
"That will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. However, there are two other ways to install LangChain that do bring in those dependencies.",这将安装LangChain的最低要求。当LangChain与各种模型提供者、数据存储等集成时，它的许多价值就来了。默认情况下，不会安装执行此操作所需的依赖项。然而，还有另外两种安装LangChain的方法会带来这些依赖。
"To install modules needed for the common LLM providers, run:",要安装通用LLM提供程序所需的模块，请运行：
"To install all modules needed for all integrations, run:",要安装所有集成所需的所有模块，请运行：
"Note that if you are using `zsh`, you'll need to quote square brackets when passing them as an argument to a command, for example:",请注意，如果您使用的是“zsh”，那么在将方括号作为参数传递给命令时，您需要用方括号括起来，例如：
Installing from source,从源安装
"If you want to install from source, you can do so by cloning the repo and running:",如果要从源代码安装，可以通过克隆repo并运行：
Integrations,集成
"Besides the installation of this python package, you will also need to install packages and set environment variables depending on which chains you want to use.",除了安装这个python包之外，您还需要安装包，并根据您想要使用的链设置环境变量。
"Note: the reason these packages are not included in the dependencies by default is that as we imagine scaling this package, we do not want to force dependencies that are not needed.",注意：默认情况下，这些包不包含在依赖项中的原因是，当我们想象扩展这个包时，我们不想强制不需要的依赖项。
The following use cases require specific installs and api keys:,以下用例需要特定的安装和api密钥：
_OpenAI_:,_OpenAI_:
Install requirements with `pip install openai`,使用“pip install openai”安装要求
Get an OpenAI api key and either set it as an environment variable (`OPENAI_API_KEY`) or pass it to the LLM constructor as `openai_api_key`.,获取一个OpenAI api密钥，并将其设置为环境变量（“OPENAI_API_KEY”），或者将其作为“OPENAI_API_KEY”传递给LLM构造函数。
_Cohere_:,_Cohere_:
Install requirements with `pip install cohere`,使用“pip install cohere”安装要求
Get a Cohere api key and either set it as an environment variable (`COHERE_API_KEY`) or pass it to the LLM constructor as `cohere_api_key`.,获取一个CohereAPI密钥，并将其设置为环境变量（`COHERE_API_KEY`），或者将其作为`COHERE_API_KEY`传递给LLM构造函数。
_GooseAI_:,_GooseAI_:
Get an GooseAI api key and either set it as an environment variable (`GOOSEAI_API_KEY`) or pass it to the LLM constructor as `gooseai_api_key`.,获取一个GooseAI api密钥，并将其设置为环境变量（`GOOSEAI_API_KEY`），或者将其作为`GOOSEAI_API_KEY`传递给LLM构造函数。
_Hugging Face Hub_,拥抱面轮毂_
Install requirements with `pip install huggingface_hub`,使用“pip install huggingface_hub”安装要求
Get a Hugging Face Hub api token and either set it as an environment variable (`HUGGINGFACEHUB_API_TOKEN`) or pass it to the LLM constructor as `huggingfacehub_api_token`.,获取一个拥抱脸集线器api令牌，并将其设置为环境变量（`huggingfacehub_api_token`），或者将其作为`huggingfacehub_api_token’传递给LLM构造函数。
_Petals_:,_Petals_:
Install requirements with `pip install petals`,使用“pip安装花瓣”安装要求
Get an GooseAI api key and either set it as an environment variable (`HUGGINGFACE_API_KEY`) or pass it to the LLM constructor as `huggingface_api_key`.,获取一个GooseAI api密钥，并将其设置为环境变量（“huggingface_api_key”），或者将其作为“huggingface_api_key”传递给LLM构造函数。
_CerebriumAI_:,_CerebriumAI_:
Install requirements with `pip install cerebrium`,使用“pip install cerebrium”安装要求
Get a Cerebrium api key and either set it as an environment variable (`CEREBRIUMAI_API_KEY`) or pass it to the LLM constructor as `cerebriumai_api_key`.,获取一个Cerebrium api密钥，并将其设置为环境变量（“CEREBRIUMAI_API_KEY”），或者将其作为“CEREBRIUMAI_API_KEY”传递给LLM构造函数。
_PromptLayer_:,_PromptLayer_:
Install requirements with `pip install promptlayer` (be sure to be on version 0.1.62 or higher),使用“pip install promptlayer”安装要求（确保版本为0.1.62或更高版本）
Get an API key from [promptlayer.com](http://www.promptlayer.com) and set it using `promptlayer.api_key=<API KEY>`,从[promptlayer.com](http://www.promptlayer.com）获取API密钥，并使用'promptlayer.api_key=<API key>'设置它
_SerpAPI_:,_SerpAPI_：
Get a SerpAPI api key and either set it as an environment variable (`SERPAPI_API_KEY`) or pass it to the LLM constructor as `serpapi_api_key`.,获取一个SerpAPI api密钥，并将其设置为环境变量（`serpapi_api_key`），或者将其作为`serpapi_api_key`传递给LLM构造函数。
_GoogleSearchAPI_:,_GoogleSearchAPI_:
Get a Google api key and either set it as an environment variable (`GOOGLE_API_KEY`) or pass it to the LLM constructor as `google_api_key`. You will also need to set the `GOOGLE_CSE_ID` environment variable to your custom search engine id. You can pass it to the LLM constructor as `google_cse_id` as well.,获取一个Google api密钥，并将其设置为环境变量（`google_api_key`），或者将其作为`google_api_key’传递给LLM构造函数。您还需要将“GOOGLE_CSE_ID”环境变量设置为您的自定义搜索引擎id。您也可以将它作为“google_cse_id”传递给LLM构造函数。
_WolframAlphaAPI_:,_WolframAlphaAPI_:
Get a Wolfram Alpha api key and either set it as an environment variable (`WOLFRAM_ALPHA_APPID`) or pass it to the LLM constructor as `wolfram_alpha_appid`.,获取一个Wolfram Alpha api密钥，并将其设置为环境变量（`wolfram_alpha_appid`），或者将其作为`wolfram_alpha_appid`传递给LLM构造函数。
_NatBot_:,_NatBot_:
Install requirements with `pip install playwright`,使用“pip安装剧作家”安装要求
_Wikipedia_:,_Wikipedia_:
Install requirements with `pip install wikipedia`,使用“pip安装维基百科”安装要求
_Elasticsearch_:,_Elasticsearch_:
Install requirements with `pip install elasticsearch`,使用“pip安装弹性搜索”安装要求
"Set up Elasticsearch backend. If you want to do locally, [this](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started.html) is a good guide.",设置弹性搜索后端。如果你想在本地做，【这个】（https：//www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started.html）是一个很好的指南。
_FAISS_:,_FAISS_:
Install requirements with `pip install faiss` for Python 3.7 and `pip install faiss-cpu` for Python 3.10+.,Python 3.7的安装要求为“pip安装faiss”，Python 3.10+的安装要求为“pip安装faiss-cpu”。
_MyScale_,_MyScale_
"Install requirements with `pip install clickhouse-connect`. For documentations, please refer to [this document](https://docs.myscale.com/en/overview/).",使用“pip安装clickhouse-connect”安装要求。有关文件，请参考【本文件】（https：//docs.myscale.com/en/overview/）。
_Manifest_:,_Manifest_:
Install requirements with `pip install manifest-ml` (Note: this is only available in Python 3.8+ currently).,使用“pip install manifest-ml”安装要求（注意：这目前仅在Python 3.8+中可用）。
_OpenSearch_:,_OpenSearch_:
Install requirements with `pip install opensearch-py`,使用“pip install opensearch-py”安装要求
"If you want to set up OpenSearch on your local, [here](https://opensearch.org/docs/latest/)",如果要在本地设置OpenSearch，请访问[此处](https://opensearch.org/docs/latest/）
_DeepLake_:,_DeepLake_:
Install requirements with `pip install deeplake`,使用“pip install deeplake”安装要求
_LlamaCpp_:,_LlamaCpp_:
Install requirements with `pip install llama-cpp-python`,使用“pip安装llama-cpp-python”安装要求
Download model and convert following [llama.cpp instructions](https://github.com/ggerganov/llama.cpp),下载模型并按照[llama.cpp说明]进行转换(https://github.com/ggerganov/llama.cpp）
_Milvus_:,_Milvus_:
Install requirements with `pip install pymilvus`,使用“pip install pymilvus”安装要求
"In order to setup a local cluster, take a look [here](https://milvus.io/docs).",为了设置本地集群，请看【这里】（https：//milvus.io/docs）。
_Zilliz_:,_Zilliz_:
"To get up and running, take a look [here](https://zilliz.com/doc/quick_start).",要开始运行，请查看[此处]（https://zilliz.com/doc/quick_start）。
"If you are using the `NLTKTextSplitter` or the `SpacyTextSplitter`, you will also need to install the appropriate models. For example, if you want to use the `SpacyTextSplitter`, you will need to install the `en_core_web_sm` model with `python -m spacy download en_core_web_sm`. Similarly, if you want to use the `NLTKTextSplitter`, you will need to install the `punkt` model with `python -m nltk.downloader punkt`.",如果您使用的是“NLTKTextSplitter”或“SpacyTextSplitter”，您还需要安装适当的型号。例如，如果您想要使用“SpacyTextSplitter”，您将需要使用“python-m spacy download en_core_web_sm”安装“en_core_web_sm”模型。类似地，如果您想要使用“NLTKTextSplitter”，您将需要使用“python-m nltk”安装“punkt”模型。下载器punkt”。
Models,模型
LangChain provides interfaces and integrations for a number of different types of models.,LangChain为许多不同类型的模型提供接口和集成。
Agent Toolkits,代理工具包
Chains,链条
Chat Models,聊天模式
Docstore,文档存储
Document Compressors,文档压缩器
Document Loaders,文档加载器
Document Transformers,文档转换器
Example Selector,示例选择器
Experimental Modules,实验模块
This module contains experimental modules and reproductions of existing work using LangChain primitives.,本模块包含实验模块和使用LangChain原语的现有工作的复制品。
Autonomous Agents,自主代理
"Here, we document the BabyAGI and AutoGPT classes from the langchain.experimental module.",在这里，我们记录了langchain中的BabyAGI和AutoGPT类。实验模块。
Generative Agents,生殖因子
"Here, we document the GenerativeAgent and GenerativeAgentMemory classes from the langchain.experimental module.",在这里，我们记录了langchain中的GenerativeAgent和GenerativeAgentMemory类。实验模块。
LLMs,LLMs
Memory,记忆
Output Parsers,输出解析器
PromptTemplates,提示模板
Python REPL,Python REPL
Retrievers,寻回犬
SearxNG Search,SearxNG搜索
Text Splitter,文本拆分器
Tools,工具
Utilities,公用设施
Vector Stores,向量存储
Prompts,提示
The reference guides here all relate to objects for working with Prompts.,此处的参考指南都与使用提示的对象相关。
Tracing,跟踪
"By enabling tracing in your LangChain runs, you’ll be able to more effectively visualize, step through, and debug your chains and agents.",通过在LangChain运行中启用跟踪，您将能够更有效地可视化、单步执行和调试您的链和代理。
"First, you should install tracing and set up your environment properly. You can use either a locally hosted version of this (uses Docker) or a cloud hosted version (in closed alpha). If you're interested in using the hosted platform, please fill out the form [here](https://forms.gle/tRCEMSeopZf6TE3b6).",首先，您应该安装跟踪并正确设置您的环境。您可以使用本地托管版本（使用Docker）或云托管版本（在封闭的alpha中）。如果您对使用托管平台感兴趣，请填写表格【此处】（https：//forms.gle/trcemseopzf 6 te 3 b 6）。
[Locally Hosted Setup](./tracing/local_installation.md),[本地托管安装程序](./tracing/local_installation.md）
[Cloud Hosted Setup](./tracing/hosted_installation.md),[云托管安装程序](./tracing/hosted_installation.md）
Tracing Walkthrough,跟踪演练
"When you first access the UI, you should see a page with your tracing sessions.  An initial one ""default"" should already be created for you.  A session is just a way to group traces together.  If you click on a session, it will take you to a page with no recorded traces that says ""No Runs.""  You can create a new session with the new session form.",当您第一次访问UI时，您应该会看到一个包含跟踪会话的页面。应该已经为您创建了一个初始的“默认”。会话只是将跟踪分组在一起的一种方式。如果你点击一个会话，它会把你带到一个没有记录痕迹的页面，上面写着“没有运行”。您可以使用新会话表单创建新会话。
![](tracing/homepage.png),！[]（跟踪/主页。png）
"If we click on the `default` session, we can see that to start we have no traces stored.",如果我们单击“默认”会话，我们可以看到开始时我们没有存储任何跟踪。
![](tracing/default_empty.png),！[](tracing/default_empty.png）
"If we now start running chains and agents with tracing enabled, we will see data show up here. To do so, we can run [this notebook](tracing/agent_with_tracing.ipynb) as an example. After running it, we will see an initial trace show up.",如果我们现在开始运行启用跟踪的链和代理，我们将看到数据显示在这里。为此，我们可以运行[this notebook]（tracing/agent_with_tracing.ipynb）作为示例。运行后，我们将看到一个初始跟踪显示。
![](tracing/first_trace.png),！[](tracing/first_trace.png）
From here we can explore the trace at a high level by clicking on the arrow to show nested runs. We can keep on clicking further and further down to explore deeper and deeper.,从这里，我们可以通过单击箭头来显示嵌套运行，从而在高层次上探索跟踪。我们可以继续点击越来越深，探索越来越深。
![](tracing/explore.png),！[]（跟踪/explore.png）
"We can also click on the ""Explore"" button of the top level run to dive even deeper.  Here, we can see the inputs and outputs in full, as well as all the nested traces.",我们还可以点击顶级跑步的“探索”按钮，进行更深入的探索。在这里，我们可以看到完整的输入和输出，以及所有嵌套的跟踪。
![](tracing/explore_trace.png),！[](tracing/explore_trace.png）
"We can keep on exploring each of these nested traces in more detail. For example, here is the lowest level trace with the exact inputs/outputs to the LLM.",我们可以继续更详细地探索这些嵌套跟踪中的每一个。例如，这是LLM精确输入/输出的最低电平走线。
![](tracing/explore_llm.png),！[](tracing/explore_llm.png）
Changing Sessions,更改会话
"To initially record traces to a session other than `""default""`, you can set the `LANGCHAIN_SESSION` environment variable to the name of the session you want to record to:",要最初记录到“default”以外的会话的跟踪，可以将“LANGCHAIN_SESSION”环境变量设置为要记录到的会话的名称：
"To switch sessions mid-script or mid-notebook, do NOT set the `LANGCHAIN_SESSION` environment variable. Instead: `langchain.set_tracing_callback_manager(session_name=""my_session"")`",若要在脚本中间或笔记本中间切换会话，请不要设置“LANGCHAIN_SESSION”环境变量。改为：`langchain.set_tracing_callback_manager(session_name=“my_session”)`
Cloud Hosted Setup,云托管设置
We offer a hosted version of tracing at [langchainplus.vercel.app](https://langchainplus.vercel.app/). You can use this to view traces from your run without having to run the server locally.,我们在[langchainplus.vercel.app]（https：//langchainplus.vercel.app/）上提供了一个托管版本的跟踪。您可以使用它来查看运行中的跟踪，而不必在本地运行服务器。
"Note: we are currently only offering this to a limited number of users. The hosted platform is VERY alpha, in active development, and data might be dropped at any time. Don't depend on data being persisted in the system long term and don't log traces that may contain sensitive information. If you're interested in using the hosted platform, please fill out the form [here](https://forms.gle/tRCEMSeopZf6TE3b6).",注意：我们目前只向有限数量的用户提供这项服务。托管平台是非常alpha的，正在积极开发中，数据可能会随时丢失。不要依赖长期保存在系统中的数据，也不要记录可能包含敏感信息的跟踪。如果您对使用托管平台感兴趣，请填写表格【此处】（https：//forms.gle/trcemseopzf 6 te 3 b 6）。
"Login to the system and click ""API Key"" in the top right corner. Generate a new key and keep it safe. You will need it to authenticate with the system.",登录系统，点击右上角的“API键”。生成一个新密钥并保证其安全。您将需要它来向系统进行身份验证。
"After installation, you must now set up your environment to use tracing.",安装后，现在必须将环境设置为使用跟踪。
This can be done by setting an environment variable in your terminal by running `export LANGCHAIN_HANDLER=langchain`.,这可以通过运行“export LANGCHAIN_HANDLER=langchain”在终端中设置一个环境变量来实现。
"You can also do this by adding the below snippet to the top of every script. **IMPORTANT:** this must go at the VERY TOP of your script, before you import anything from `langchain`.",您也可以通过将下面的代码片段添加到每个脚本的顶部来实现这一点。**重要提示：**在从“langchain”导入任何内容之前，这必须放在脚本的最顶端。
You will also need to set an environment variable to specify the endpoint and your API key. This can be done with the following environment variables:,您还需要设置一个环境变量来指定端点和API密钥。这可以通过以下环境变量来实现：
"`LANGCHAIN_ENDPOINT` = ""https://langchain-api-gateway-57eoxz8z.uc.gateway.dev""",`LANGCHAIN_ENDPOINT`=“https：//langchain-api-gateway-57 eoxz 8 z.uc.gateway.dev”
`LANGCHAIN_API_KEY` - set this to the API key you generated during installation.,`LANGCHAIN_API_KEY`-将其设置为您在安装过程中生成的API密钥。
An example of adding all relevant environment variables is below:,添加所有相关环境变量的示例如下：
Locally Hosted Setup,本地托管安装程序
This page contains instructions for installing and then setting up the environment to use the locally hosted version of tracing.,本页包含安装和设置环境以使用本地承载的跟踪版本的说明。
Ensure you have Docker installed (see [Get Docker](https://docs.docker.com/get-docker/)) and that it’s running.,确保您已经安装了Docker（参见【Get Docker】（https：//docs.docker.com/get-docker/））并且它正在运行。
Install the latest version of `langchain`: `pip install langchain` or `pip install langchain -U` to upgrade your existing version.,安装最新版本的“langchain”：“pip install langchain”或“pip Install langchain-u”来升级您现有的版本。
Run `langchain-server`. This command was installed automatically when you ran the above command (`pip install langchain`).,运行“语言链服务器”。当您运行上述命令（“pip install langchain”）时，会自动安装该命令。
"This will spin up the server in the terminal, hosted on port `4137` by default.",这将启动终端中的服务器，默认情况下托管在端口“4137”上。
"Once you see the terminal output `langchain-langchain-frontend-1 | ➜ Local: [http://localhost:4173/](http://localhost:4173/)`, navigate to [http://localhost:4173/](http://localhost:4173/)",一旦您看到终端输出`langchain-langchain-frontend-1 local:[http://localhost:4173/](http://localhost:4173/)`，请导航到[http://localhost:4173/](http://localhost:4173/)
You should see a page with your tracing sessions. See the overview page for a walkthrough of the UI.,您应该会看到一个包含跟踪会话的页面。有关UI的演练，请参见概述页面。
"Currently, trace data is not guaranteed to be persisted between runs of `langchain-server`. If you want to     persist your data, you can mount a volume to the Docker container. See the [Docker docs](https://docs.docker.com/storage/volumes/) for more info.",目前，跟踪数据不能保证在运行“langchain-server”之间保持不变。如果您想要持久化您的数据，您可以将一个卷挂载到Docker容器中。有关更多信息，请参见【Docker文档】（https：//docs.docker.com/storage/volumes/）。
"To stop the server, press `Ctrl+C` in the terminal where you ran `langchain-server`.",要停止服务器，请在运行“langchain-server”的终端中按“Ctrl+C”。
Agent Simulations,代理模拟
Agent simulations involve interacting one of more agents with eachother. Agent simulations generally involve two main components:,代理模拟包括一个或多个代理之间的交互。代理模拟通常包括两个主要部分：
Long Term Memory,长时记忆
Simulation Environment,仿真环境
Specific implementations of agent simulations (or parts of agent simulations) include,代理模拟（或部分代理模拟）的具体实现包括
Simulations with Two Agents,两个智能体的模拟
"[CAMEL](agent_simulations/camel_role_playing.ipynb): an implementation of the CAMEL (Communicative Agents for “Mind” Exploration of Large Scale Language Model Society) paper, where two agents communicate with each other.",[CAMEL](agent_simulations/camel_role_playing.ipynb):CAMEL（用于大规模语言模型社会“思维”探索的通信代理）论文的一个实现，其中两个代理相互通信。
[Two Player D&D](agent_simulations/two_player_dnd.ipynb): an example of how to use a generic simulator for two agents to implement a variant of the popular Dungeons & Dragons role playing game.,[Two Player D&D](agent_simulations/two_player_dnd.ipynb)：一个示例，说明如何为两个代理使用通用模拟器来实现流行的龙与地下城角色扮演游戏的变体。
Simulations with Multiple Agents,多智能体仿真
"[Multi-Player D&D](agent_simulations/multi_player_dnd.ipynb): an example of how to use a generic dialogue simulator for multiple dialogue agents with a custom speaker-ordering, illustrated with a variant of the popular Dungeons & Dragons role playing game.",[Multi-Player D&D](agent_simulations/multi_player_dnd.ipynb)：一个示例，说明了如何使用通用对话模拟器为具有自定义扬声器排序的多个对话代理进行对话，并以流行的龙与地下城角色扮演游戏的变体进行说明。
[Decentralized Speaker Selection](agent_simulations/multiagent_bidding.ipynb): an example of how to implement a multi-agent dialogue without a fixed schedule for who speaks when. Instead the agents decide for themselves who speaks by outputting bids to speak. This example shows how to do this in the context of a fictitious presidential debate.,[分散式发言人选择](agent_simulations/multiagent_bidding.ipynb）：一个如何在没有固定时间表的情况下实现多代理对话的例子。相反，代理通过输出发言出价来自己决定谁发言。这个例子展示了如何在一个虚构的总统辩论中做到这一点。
"[Generative Agents](agent_simulations/characters.ipynb): This notebook implements a generative agent based on the paper [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442) by Park, et. al.",[Generative Agents](agent_simulations/characters.ipynb)：本笔记本基于Park等人的论文[Generative Agents：Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442）实现了一个generative agent。艾尔。
CAMEL Role-Playing Autonomous Cooperative Agents,CAMEL角色扮演自主协作智能体
"This is a langchain implementation of paper: ""CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society"".",这是一篇论文的langchain实现：“骆驼：大规模语言模型社会的“思维”探索的交流代理”。
Overview:,概述：
"The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their ""cognitive"" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond.",会话和基于聊天的语言模型的快速发展导致了复杂任务解决的显著进步。然而，他们的成功在很大程度上依赖于人工输入来引导对话，这可能具有挑战性且耗时。本文探讨了建立可扩展技术的潜力，以促进交流代理之间的自主合作，并提供对其“认知”过程的洞察。为了解决实现自主合作的挑战，我们提出了一种新的通信代理框架，名为角色扮演。我们的方法包括使用初始提示来引导聊天代理完成任务，同时保持与人类意图的一致性。我们展示了如何使用角色扮演来生成用于研究聊天代理的行为和能力的会话数据，为研究会话语言模型提供了有价值的资源。我们的贡献包括引入一种新的通信代理框架，为研究多代理系统的协作行为和能力提供一种可扩展的方法，以及开源我们的库以支持通信代理及其他方面的研究。
The original implementation: https://github.com/lightaime/camel,原始实现：https://github.com/lightaime/camel
Project website: https://www.camel-ai.org/,项目网址：https://www.camel-ai.org/
Arxiv paper: https://arxiv.org/abs/2303.17760,Arxiv文件：https://arxiv.org/abs/2303.17760
Import LangChain related modules,导入LangChain相关模块
Define a CAMEL agent helper class,定义CAMEL代理助手类
Setup OpenAI API key and roles and task for role-playing,为角色扮演设置OpenAI API密钥、角色和任务
Create a task specify agent for brainstorming and get the specified task,创建任务指定头脑风暴代理并获取指定任务
Create inception prompts for AI assistant and AI user for role-playing,为AI助手和AI用户创建角色扮演的初始提示
Create a helper helper to get system messages for AI assistant and AI user from role names and the task,创建一个助手助手，从角色名和任务中获取AI助手和AI用户的系统消息
Create AI assistant agent and AI user agent from obtained system messages,根据获取的系统消息创建AI助理代理和AI用户代理
Start role-playing session to solve the task!,开始角色扮演会话来解决任务！
Generative Agents in LangChain,LangChain中的生成代理
"This notebook implements a generative agent based on the paper [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442) by Park, et. al.",本笔记本基于帕克等人的论文【生成代理：人类行为的交互式拟像】（https：//arxiv.org/abs/2304.03442）实现了一个生成代理。艾尔。
"In it, we leverage a time-weighted Memory object backed by a LangChain Retriever.",在其中，我们利用了一个由LangChain检索器支持的时间加权内存对象。
Generative Agent Memory Components,生成代理存储器组件
This tutorial highlights the memory of generative agents and its impact on their behavior. The memory varies from standard LangChain Chat memory in two aspects:,本教程重点介绍了生殖代理的记忆及其对其行为的影响。该存储器在两个方面不同于标准的LangChain聊天存储器：
**Memory Formation**,**记忆形成**
"Generative Agents have extended memories, stored in a single stream:",生成代理具有扩展的存储器，存储在单个流中：
"Observations - from dialogues or interactions with the virtual world, about self or others",观察——来自与虚拟世界的对话或互动，关于自我或他人
Reflections - resurfaced and summarized core memories,反思-重新浮现和总结核心记忆
**Memory Recall**,**回忆**
"Memories are retrieved using a weighted sum of salience, recency, and importance.",使用显著性、新近性和重要性的加权总和来检索记忆。
"You can review the definitions of the `GenerativeAgent` and `GenerativeAgentMemory` in the [reference documentation](""../../reference/modules/experimental"") for the following imports, focusing on `add_memory` and `summarize_related_memories` methods.",您可以在[参考文档]（“../../reference/modules/experimental”）中查看有关以下导入的“GenerativeAgent”和“GenerativeAgentMemory”的定义，重点是“add_memory”和“summarize_related_memories”方法。
Memory Lifecycle,内存生命周期
Summarizing the key methods in the above: `add_memory` and `summarize_related_memories`.,总结上面的关键方法：“添加_memory”和“总结_related_memories”。
"When an agent makes an observation, it stores the memory:",当代理进行观察时，它会存储内存：
"Language model scores the memory's importance (1 for mundane, 10 for poignant)",语言模型给记忆的重要性打分（1分表示平凡，10分表示辛酸）
"Observation and importance are stored within a document by TimeWeightedVectorStoreRetriever, with a `last_accessed_time`.",观察和重要性由TimeWeightedVectorStoreRetriever存储在文档中，具有“上次访问时间”。
When an agent responds to an observation:,当代理对观察结果做出响应时：
"Generates query(s) for retriever, which fetches documents based on salience, recency, and importance.",为检索器生成查询，检索器根据显著性、新近性和重要性获取文档。
Summarizes the retrieved information,汇总检索到的信息
Updates the `last_accessed_time` for the used documents.,更新所用文档的“上次访问时间”。
Create a Generative Character,创造生成性角色
"Now that we've walked through the definition, we will create two characters named ""Tommie"" and ""Eve"".",现在我们已经完成了定义，我们将创建两个名为“Tommie”和“Eve”的角色。
Pre-Interview with Character,人物面试前
"Before sending our character on their way, let's ask them a few questions.",在送我们的角色上路之前，让我们问他们几个问题。
Step through the day's observations.,一步一步地观察一天。
Interview after the day,一天后的面谈
Adding Multiple Characters,添加多个字符
Let's add a second character to have a conversation with Tommie. Feel free to configure different traits.,让我们添加第二个角色来与Tommie对话。随意配置不同的特征。
Pre-conversation interviews,会话前访谈
"Let's ""Interview"" Eve before she speaks with Tommie.",让我们在伊芙和汤米说话之前“采访”她。
Dialogue between Generative Agents,生成主体之间的对话
"Generative agents are much more complex when they interact with a virtual environment or with each other. Below, we run a simple conversation between Tommie and Eve.",当生成代理与虚拟环境或彼此交互时，它们要复杂得多。下面，我们在汤米和伊芙之间进行一段简单的对话。
Let's interview our agents after their conversation,在我们的代理人谈话之后，让我们采访他们
"Since the generative agents retain their memories from the day, we can ask them about their plans, conversations, and other memoreis.",由于生殖代理人保留了他们当天的记忆，我们可以询问他们的计划、谈话和其他记忆。
Multi-Player Dungeons & Dragons,多人龙与地下城
This notebook shows how the `DialogueAgent` and `DialogueSimulator` class make it easy to extend the [Two-Player Dungeons & Dragons example](https://python.langchain.com/en/latest/use_cases/agent_simulations/two_player_dnd.html) to multiple players.,本笔记本展示了“DialogueAgent”和“DialogueSimulator”类如何轻松地将[双玩家龙与地下城示例]（https://python.langchain.com/en/latest/use_cases/agent_simulations/two_player_dnd.html）扩展到多个玩家。
The main difference between simulating two players and multiple players is in revising the schedule for when each agent speaks,模拟两个玩家和多个玩家的主要区别在于修改每个代理发言的时间表
"To this end, we augment `DialogueSimulator` to take in a custom function that determines the schedule of which agent speaks. In the example below, each character speaks in round-robin fashion, with the storyteller interleaved between each player.",为此，我们增强了“DialogueSimulator”，以引入一个自定义函数来确定哪个代理说话的时间表。在下面的例子中，每个角色都以循环的方式说话，讲故事的人在每个玩家之间穿插。
`DialogueAgent` class,“对话代理”类
The `DialogueAgent` class is a simple wrapper around the `ChatOpenAI` model that stores the message history from the `dialogue_agent`'s point of view by simply concatenating the messages as strings.,“DialogueAgent”类是围绕“ChatOpenAI”模型的简单包装器，它通过简单地将消息连接为字符串来存储来自“dialogue_agent”的消息历史。
It exposes two methods:,它公开了两种方法：
`send()`: applies the chatmodel to the message history and returns the message string,`send()`:将chatmodel应用于消息历史记录并返回消息字符串
"`receive(name, message)`: adds the `message` spoken by `name` to message history",“receive（name，message）”:将“name”所说的“message”添加到消息历史记录中
`DialogueSimulator` class,“对话模拟程序”类
"The `DialogueSimulator` class takes a list of agents. At each step, it performs the following:",“DialogueSimulator”类接受代理列表。在每个步骤中，它执行以下操作：
Select the next speaker,选择下一个演讲者
Calls the next speaker to send a message,呼叫下一个发言者发送消息
Broadcasts the message to all other agents,将消息广播到所有其他座席
"Update the step counter. The selection of the next speaker can be implemented as any function, but in this case we simply loop through the agents.",更新步进计数器。下一个说话者的选择可以实现为任何函数，但在这种情况下，我们只需遍历代理。
Define roles and quest,定义角色和任务
Ask an LLM to add detail to the game description,请LLM为游戏描述添加细节
Use an LLM to create an elaborate quest description,使用LLM创建详细的任务描述
Main Loop,主回路
Multi-agent decentralized speaker selection,多agent分散式说话人选择
This notebook showcases how to implement a multi-agent simulation without a fixed schedule for who speaks when. Instead the agents decide for themselves who speaks. We can implement this by having each agent bid to speak. Whichever agent's bid is the highest gets to speak.,本笔记本展示了如何在没有固定时间表的情况下实现多代理模拟。相反，代理人自己决定谁说话。我们可以通过让每个代理竞标发言来实现这一点。谁的出价最高，谁就可以发言。
We will show how to do this in the example below that showcases a fictitious presidential debate.,我们将在下面的例子中展示如何做到这一点，这个例子展示了一个虚构的总统辩论。
`DialogueAgent` and `DialogueSimulator` classes,“DialogueAgent”和“DialogueSimulator”类
We will use the same `DialogueAgent` and `DialogueSimulator` classes defined in [Multi-Player Dungeons & Dragons](https://python.langchain.com/en/latest/use_cases/agent_simulations/multi_player_dnd.html).,我们将使用[多人龙与地下城]中定义的相同的“DialogueAgent”和“DialogueSimulator”类（https://python.langchain.com/en/latest/use_cases/agent_simulations/multi_player_dnd.html）。
`BiddingDialogueAgent` class,“BiddingDialogueAgent”类
We define a subclass of `DialogueAgent` that has a `bid()` method that produces a bid given the message history and the most recent message.,我们定义了“DialogueAgent”的一个子类，它有一个“bid（）”方法，该方法根据消息历史和最近的消息生成一个bid。
Define participants and debate topic,定义参与者和辩论主题
Generate system messages,生成系统消息
Output parser for bids,投标输出解析器
"We ask the agents to output a bid to speak. But since the agents are LLMs that output strings, we need to",我们要求代理输出一个出价发言。但是由于代理是输出字符串的LLM，我们需要
define a format they will produce their outputs in,定义它们将产生输出的格式
parse their outputs,解析它们的输出
We can subclass the [RegexParser](https://github.com/hwchase17/langchain/blob/master/langchain/output_parsers/regex.py) to implement our own custom output parser for bids.,我们可以对[RegexParser]（https://github.com/hwchase 17/langchain/blob/master/langchain/output_parsers/regex.py）进行子类化，以实现我们自己的用于投标的自定义输出解析器。
Generate bidding system message,生成投标系统消息
This is inspired by the prompt used in [Generative Agents](https://arxiv.org/pdf/2304.03442.pdf) for using an LLM to determine the importance of memories. This will use the formatting instructions from our `BidOutputParser`.,这是受【生成代理】（https：//arxiv.org/pdf/2304.03442.pdf）中使用的提示的启发，该提示使用LLM来确定记忆的重要性。这将使用我们的“BidOutputParser”中的格式化指令。
Use an LLM to create an elaborate on debate topic,使用LLM创建一个详细的辩论主题
Define the speaker selection function,定义扬声器选择功能
Lastly we will define a speaker selection function `select_next_speaker` that takes each agent's bid and selects the agent with the highest bid (with ties broken randomly).,最后，我们将定义一个发言人选择函数“select_next_speaker”，它接受每个代理的出价，并选择出价最高的代理（随机打破平局）。
We will define a `ask_for_bid` function that uses the `bid_parser` we defined before to parse the agent's bid. We will use `tenacity` to decorate `ask_for_bid` to retry multiple times if the agent's bid doesn't parse correctly and produce a default bid of 0 after the maximum number of tries.,我们将定义一个“ask_for_bid”函数，该函数使用我们之前定义的“bid_parser”来解析代理的出价。我们将使用“坚韧”来修饰“ask_for_bid”，以便在代理的出价没有正确解析并在最大尝试次数后产生默认出价0的情况下重试多次。
Two-Player Dungeons & Dragons,双人龙与地下城
"In this notebook, we show how we can use concepts from [CAMEL](https://www.camel-ai.org/) to simulate a role-playing game with a protagonist and a dungeon master. To simulate this game, we create an `DialogueSimulator` class that coordinates the dialogue between the two agents.",在本笔记本中，我们将展示如何使用[CAMEL]（https：//www.camel-ai.org/）中的概念来模拟一个有主角和地下城主的角色扮演游戏。为了模拟这个游戏，我们创建了一个“DialogueSimulator”类来协调两个代理之间的对话。
Protagonist and dungeon master system messages,主角和地下城主系统消息
BabyAGI User Guide,BabyAGI用户指南
This notebook demonstrates how to implement [BabyAGI](https://github.com/yoheinakajima/babyagi/tree/main) by [Yohei Nakajima](https://twitter.com/yoheinakajima). BabyAGI is an AI agent that can generate and pretend to execute tasks based on a given objective.,本笔记本演示了如何通过[Yohei Nakajima](https://twitter.com/yoheinakajima）实现[BabyAGI](https://github.com/yoheinakajima/BabyAGI/tree/main）。BabyAGI是一个人工智能代理，可以根据给定的目标生成并假装执行任务。
This guide will help you understand the components to create your own recursive agents.,本指南将帮助您理解创建自己的递归代理的组件。
"Although BabyAGI uses specific vectorstores/model providers (Pinecone, OpenAI), one of the benefits of implementing it with LangChain is that you can easily swap those out for different options. In this implementation we use a FAISS vectorstore (because it runs locally and is free).",尽管BabyAGI使用特定的向量存储/模型提供者（Pinecone，OpenAI），但是用LangChain实现它的一个好处是，您可以很容易地将它们换成不同的选项。在这个实现中，我们使用FAISS vectorstore（因为它在本地运行并且是免费的）。
Install and Import Required Modules,安装并导入所需模块
Connect to the Vector Store,连接到矢量存储
"Depending on what vectorstore you use, this step may look different.",根据您使用的vectorstore，这一步可能会有所不同。
Define the Chains,定义链
BabyAGI relies on three LLM chains:,BabyAGI依赖于三个LLM链：
Task creation chain to select new tasks to add to the list,任务创建链，用于选择要添加到列表中的新任务
Task prioritization chain to re-prioritize tasks,任务优先级链，用于重新确定任务的优先级
Execution Chain to execute the tasks,执行任务的执行链
Define the BabyAGI Controller,定义BabyAGI控制器
BabyAGI composes the chains defined above in a (potentially-)infinite loop.,BabyAGI在一个（潜在的）无限循环中组成了上面定义的链。
Run the BabyAGI,运行BabyAGI
Now it's time to create the BabyAGI controller and watch it try to accomplish your objective.,现在是时候创建BabyAGI控制器，并观看它尝试完成您的目标。
BabyAGI with Tools,带工具的BabyAGI
"This notebook builds on top of [baby agi](baby_agi.ipynb), but shows how you can swap out the execution chain. The previous execution chain was just an LLM which made stuff up. By swapping it out with an agent that has access to tools, we can hopefully get real reliable information",本笔记本建立在[baby agi]（baby_agi.ipynb）之上，但展示了如何交换执行链。以前的执行链只是一个编造东西的LLM。通过与能够使用工具的代理交换，我们有希望获得真正可靠的信息
"NOTE: in this notebook, the Execution chain will now be an agent.",注意：在本笔记本中，执行链现在将是一个代理。
Custom Agent with PlugIn Retrieval,具有插件检索功能的自定义代理
This notebook combines two concepts in order to build a custom agent that can interact with AI Plugins:,这款笔记本结合了两个概念，以构建一个可以与AI插件交互的自定义代理：
"[Custom Agent with Retrieval](../../modules/agents/agents/custom_agent_with_plugin_retrieval.ipynb): This introduces the concept of retrieving many tools, which is useful when trying to work with arbitrarily many plugins.",[Custom Agent with Retrieval](../../modules/agents/agents/custom_agent_with_plugin_retrieval.ipynb)：这引入了检索许多工具的概念，这在尝试使用任意多个插件时非常有用。
"[Natural Language API Chains](../../modules/chains/examples/openapi.ipynb): This creates Natural Language wrappers around OpenAPI endpoints. This is useful because (1) plugins use OpenAPI endpoints under the hood, (2) wrapping them in an NLAChain allows the router agent to call it more easily.",[自然语言API链]（../../modules/Chains/examples/openapi.ipynb)：这将围绕OpenAPI端点创建自然语言包装器。这很有用，因为（1）插件在幕后使用OpenAPI端点，（2）将它们包装在NLAChain中允许路由器代理更容易地调用它。
"The novel idea introduced in this notebook is the idea of using retrieval to select not the tools explicitly, but the set of OpenAPI specs to use. We can then generate tools from those OpenAPI specs. The use case for this is when trying to get agents to use plugins. It may be more efficient to choose plugins first, then the endpoints, rather than the endpoints directly. This is because the plugins may contain more useful information for selection.",本笔记本中介绍的新颖想法是使用检索来选择要使用的OpenAPI规范集，而不是显式地选择工具。然后我们可以从这些OpenAPI规范中生成工具。这种情况的用例是当试图让代理使用插件时。先选择插件，再选择端点，可能比直接选择端点更有效率。这是因为插件可能包含更多有用的信息供选择。
Set up environment,设置环境
"Do necessary imports, etc.",做必要的进口等。
Setup LLM,安装LLM
Set up plugins,设置插件
Load and index plugins,加载和索引插件
Tool Retriever,工具回收器
"We will use a vectorstore to create embeddings for each tool description. Then, for an incoming query we can create embeddings for that query and do a similarity search for relevant tools.",我们将使用vectorstore为每个工具描述创建嵌入。然后，对于传入的查询，我们可以为该查询创建嵌入，并对相关工具进行相似性搜索。
We can now test this retriever to see if it seems to work.,我们现在可以测试这只寻回犬，看看它是否能工作。
Prompt Template,提示模板
"The prompt template is pretty standard, because we're not actually changing that much logic in the actual prompt template, but rather we are just changing how retrieval is done.",提示模板非常标准，因为我们实际上并没有改变实际提示模板中的太多逻辑，而是改变了检索的方式。
"The custom prompt template now has the concept of a tools_getter, which we call on the input to select the tools to use",自定义提示模板现在有了tools_getter的概念，我们在输入中调用它来选择要使用的工具
Output Parser,输出解析器
"The output parser is unchanged from the previous notebook, since we are not changing anything about the output format.",输出解析器与上一个笔记本没有任何变化，因为我们没有改变输出格式。
"Set up LLM, stop sequence, and the agent",设置LLM、停止序列和代理
Also the same as the previous notebook,也和之前的笔记本一样
Use the Agent,使用代理
Now we can use it!,现在我们可以用它了！
Plug-and-Plai,即插即用
"This notebook builds upon the idea of [tool retrieval](custom_agent_with_plugin_retrieval.html), but pulls all tools from `plugnplai` - a directory of AI Plugins.",这个笔记本基于[工具检索]（custom_agent_with_plugin_retrieval.html）的思想，但是从“plugnplai”-一个人工智能插件目录中提取所有工具。
Install plugnplai lib to get a list of active plugins from https://plugplai.com directory,安装plugnplai库以从https://plugplai.com目录获取活动插件的列表
SalesGPT - Your Context-Aware AI Sales Assistant,SalesGPT-您的上下文感知人工智能销售助理
This notebook demonstrates an implementation of a **Context-Aware** AI Sales agent.,本笔记本演示了**上下文感知**AI销售代理的实现。
This notebook was originally published at [filipmichalsky/SalesGPT](https://github.com/filip-michalsky/SalesGPT) by [@FilipMichalsky](https://twitter.com/FilipMichalsky).,本笔记本最初发表于【filipmichalsky/SalesGPT】（https：//github.com/filip-michalsky/SalesGPT）作者【@filipmichalsky】（https：//twitter.com/filipmichalsky）。
"SalesGPT is context-aware, which means it can understand what section of a sales conversation it is in and act accordingly.",SalesGPT是上下文感知的，这意味着它可以了解自己处于销售对话的哪个部分，并采取相应的行动。
"As such, this agent can have a natural sales conversation with a prospect and behaves based on the conversation stage. Hence, this notebook demonstrates how we can use AI to automate sales development representatives activites, such as outbound sales calls.",因此，该代理可以与潜在客户进行自然的销售对话，并根据对话阶段进行行为。因此，这本笔记本展示了我们如何使用人工智能来自动化销售开发代表的活动，如对外销售电话。
We leverage the [`langchain`](https://github.com/hwchase17/langchain) library in this implementation and are inspired by [BabyAGI](https://github.com/yoheinakajima/babyagi) architecture .,我们在这个实现中利用了【`langchain`】（https：//github.com/hwchase 17/langchain）库，并受到了【BabyAGI】（https：//github.com/yoheinakajima/BabyAGI）架构的启发。
Import Libraries and Set Up Your Environment,导入库并设置环境
SalesGPT architecture,SalesGPT体系结构
Seed the SalesGPT agent,设置SalesGPT代理的种子
Run Sales Agent,运行销售代理
Run Sales Stage Recognition Agent to recognize which stage is the sales agent at and adjust their behaviour accordingly.,运行销售阶段识别代理以识别销售代理处于哪个阶段，并相应地调整他们的行为。
Here is the schematic of the architecture:,以下是架构示意图：
Architecture diagram,体系结构图
Sales conversation stages.,销售对话阶段。
The agent employs an assistant who keeps it in check as in what stage of the conversation it is in. These stages were generated by ChatGPT and can be easily modified to fit other use cases or modes of conversation.,代理雇佣一名助理来检查它处于对话的哪个阶段。这些阶段是由ChatGPT生成的，可以很容易地修改以适应其他用例或对话模式。
Introduction: Start the conversation by introducing yourself and your company. Be polite and respectful while keeping the tone of the conversation professional.,介绍：从介绍你自己和你的公司开始对话。保持礼貌和尊重，同时保持专业的谈话语气。
Qualification: Qualify the prospect by confirming if they are the right person to talk to regarding your product/service. Ensure that they have the authority to make purchasing decisions.,资格：通过确认潜在客户是否是谈论你的产品/服务的合适人选来确定他们的资格。确保他们有权做出购买决定。
Value proposition: Briefly explain how your product/service can benefit the prospect. Focus on the unique selling points and value proposition of your product/service that sets it apart from competitors.,价值主张：简要解释你的产品/服务如何让潜在客户受益。专注于你的产品/服务的独特卖点和价值主张，使其从竞争对手中脱颖而出。
Needs analysis: Ask open-ended questions to uncover the prospect's needs and pain points. Listen carefully to their responses and take notes.,需求分析：提出开放式问题，发现潜在客户的需求和痛点。仔细听他们的回答并做笔记。
"Solution presentation: Based on the prospect's needs, present your product/service as the solution that can address their pain points.",解决方案演示：根据潜在客户的需求，将您的产品/服务演示为能够解决他们棘手问题的解决方案。
Objection handling: Address any objections that the prospect may have regarding your product/service. Be prepared to provide evidence or testimonials to support your claims.,异议处理：解决潜在客户对您的产品/服务的任何异议。准备好提供证据或证明来支持你的主张。
"Close: Ask for the sale by proposing a next step. This could be a demo, a trial or a meeting with decision-makers. Ensure to summarize what has been discussed and reiterate the benefits.",成交：通过提出下一步来要求销售。这可能是演示、试用或与决策者的会面。确保总结讨论的内容并重申好处。
Set up the SalesGPT Controller with the Sales Agent and Stage Analyzer,使用销售代理和阶段分析器设置SalesGPT控制器
Set up the AI Sales Agent and start the conversation,设置AI销售代理并开始对话
Set up the agent,设置代理
Run the agent,运行代理
Wikibase Agent,Wikibase代理
"This notebook demonstrates a very simple wikibase agent that uses sparql generation. Although this code is intended to work against any wikibase instance, we use http://wikidata.org for testing.",本笔记本演示了一个非常简单的wikibase代理，它使用sparql生成。尽管这段代码的目的是针对任何wikibase实例，但我们使用http://wikidata.org进行测试。
"If you are interested in wikibases and sparql, please consider helping to improve this agent. Look [here](https://github.com/donaldziff/langchain-wikibase) for more details and open questions.",如果您对wikibases和sparql感兴趣，请考虑帮助改进这个代理。查看【此处】（https：//github.com/donaldziff/langchain-wikibase）了解更多详细信息和开放问题。
Preliminaries,预赛
API keys and other secrats,API密钥和其他秘密
"We use an `.ini` file, like this:",我们使用“.ini”文件，如下所示：
OpenAI API Key,OpenAI API密钥
An OpenAI API key is required unless you modify the code below to use another LLM provider.,除非您修改下面的代码以使用另一个LLM提供者，否则需要OpenAI API密钥。
Wikidata user-agent header,Wikidata用户代理头
"Wikidata policy requires a user-agent header. See https://meta.wikimedia.org/wiki/User-Agent_policy. However, at present this policy is not strictly enforced.",Wikidata策略需要用户代理标头。参见https://meta.wikimedia.org/wiki/User-Agent_policy。然而，目前这一政策并没有得到严格执行。
Enable tracing if desired,如果需要，启用跟踪
Three tools are provided for this simple agent:,为这个简单的代理提供了三个工具：
`ItemLookup`: for finding the q-number of an item,“项目查找”：用于查找项目的q号
`PropertyLookup`: for finding the p-number of a property,“属性查找”：用于查找属性的p号
`SparqlQueryRunner`: for running a sparql query,“SparqlQueryRunner”：用于运行sparql查询
Item and Property lookup,项和属性查找
"Item and Property lookup are implemented in a single method, using an elastic search endpoint. Not all wikibase instances have it, but wikidata does, and that's where we'll start.",项和属性查找使用弹性搜索端点在单个方法中实现。不是所有的wikibase实例都有，但是wikidata有，这就是我们要开始的地方。
Sparql runner,Sparql运行程序
"This tool runs sparql - by default, wikidata is used.",这个工具运行sparql——默认情况下，使用wikidata。
Agent,代理
Wrap the tools,把工具包起来
Output parser,输出解析器
This is unchanged from langchain docs,这与langchain文档没有变化
Specify the LLM model,指定LLM模型
Agent and agent executor,代理和代理执行者
Run it!,快跑！
Interacting with APIs,与API交互
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/apis),[概念指南](https://docs.langchain.com/docs/use-cases/API）
Lots of data and information is stored behind APIs. This page covers all resources available in LangChain for working with APIs.,大量的数据和信息存储在API后面。本页涵盖了LangChain中所有可用的API资源。
"If you are just getting started, and you have relatively simple apis, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you  understand what is happening better.",如果你刚刚入门，并且你有相对简单的API，你应该从链开始。链是一系列预先确定的步骤，所以它们是很好的开始，因为它们给你更多的控制，让你更好地理解正在发生的事情。
[API Chain](../modules/chains/examples/api.ipynb),[API链]（../模块/链/示例/api.ipynb）
"Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger and more complex schemas.",代理更加复杂，涉及到对LLM的多个查询，以了解要做什么。代理的缺点是你的控制力较弱。好处是它们更强大，这允许您在更大、更复杂的模式上使用它们。
[OpenAPI Agent](../modules/agents/toolkits/examples/openapi.ipynb),[OpenAPI代理]（../modules/agents/toolkits/examples/openapi.ipynb）
"Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.",自治代理是设计为更长时间运行的代理。你给他们一个或多个长期目标，他们独立地朝着这些目标执行。这些应用程序结合了工具的使用和长期记忆。
"At the moment, Autonomous Agents are fairly experimental and based off of other open-source projects. By implementing these open source projects in LangChain primitives we can get the benefits of LangChain -  easy switching an experimenting with multiple LLMs, usage of different vectorstores as memory,  usage of LangChain's collection of tools.",目前，自主代理是相当实验性的，并且基于其他开源项目。通过在LangChain原语中实现这些开源项目，我们可以获得LangChain的好处——轻松切换和试验多个LLM，使用不同的向量存储作为内存，使用LangChain的工具集。
Baby AGI ([Original Repo](https://github.com/yoheinakajima/babyagi)),Baby AGI([原始回购](https://github.com/yoheinakajima/babyagi)）
[Baby AGI](autonomous_agents/baby_agi.ipynb): a notebook implementing BabyAGI as LLM Chains,[Baby AGI](autonomous_agents/baby_agi.ipynb)：将BabyAGI实现为LLM链的笔记本
"[Baby AGI with Tools](autonomous_agents/baby_agi_with_agent.ipynb): building off the above notebook, this example substitutes in an agent with tools as the execution tools, allowing it to actually take actions.",[Baby AGI with Tools]（autonomous_agents/baby_agi_with_agent.ipynb）：在上面的笔记本基础上，这个示例用一个带有工具的代理作为执行工具，允许它实际执行操作。
AutoGPT ([Original Repo](https://github.com/Significant-Gravitas/Auto-GPT)),AutoGPT([原始回购](https://github.com/significat-gravitas/Auto-GPT)）
[AutoGPT](autonomous_agents/autogpt.ipynb): a notebook implementing AutoGPT in LangChain primitives,[AutoGPT](autonomous_agents/autogpt.ipynb):一个用LangChain原语实现AutoGPT的笔记本
[WebSearch Research Assistant](autonomous_agents/marathon_times.ipynb): a notebook showing how to use AutoGPT plus specific tools to act as research assistant that can use the web.,[WebSearch Research Assistant](autonomous_agents/marathon_times.ipynb):一个笔记本，展示了如何使用AutoGPT和特定工具作为可以使用web的研究助理。
MetaPrompt ([Original Repo](https://github.com/ngoodman/metaprompt)),MetaPrompt([原始回购](https://github.com/ngoodman/MetaPrompt)）
[Meta-Prompt](autonomous_agents/meta_prompt.ipynb): a notebook implementing Meta-Prompt in LangChain primitives,[Meta-Prompt](autonomous_agents/meta_prompt.ipynb)：在LangChain原语中实现Meta-Prompt的笔记本
AutoGPT,自动GPT
"Implementation of https://github.com/Significant-Gravitas/Auto-GPT but with LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)",https://github.com/significat-gravitas/Auto-GPT的实现，但使用LangChain原语（LLM、PromptTemplates、VectorStores、嵌入、工具）
Set up tools,设置工具
"We'll set up an AutoGPT with a search tool, and write-file tool, and a read-file tool",我们将设置一个带有搜索工具、写入文件工具和读取文件工具的AutoGPT
Set up memory,设置内存
The memory here is used for the agents intermediate steps,此处的内存用于代理的中间步骤
Setup model and AutoGPT,设置模型和AutoGPT
Initialize everything! We will use ChatOpenAI model,初始化一切！我们将使用ChatOpenAI模型
Run an example,运行一个示例
Here we will make it write a weather report for SF,在这里，我们将让它为旧金山写一份天气报告
AutoGPT example finding Winning Marathon Times,AutoGPT示例查找马拉松获胜次数
Implementation of https://github.com/Significant-Gravitas/Auto-GPT,https://github.com/significat-gravitas/Auto-GPT的实现
"With LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)",使用LangChain原语（llm、提示模板、向量存储、嵌入、工具）
"We'll set up an AutoGPT with a `search` tool, and `write-file` tool, and a `read-file` tool, a web browsing tool, and a tool to interact with a CSV file via a python REPL",我们将设置一个带有“搜索”工具、“写入文件”工具、“读取文件”工具、web浏览工具和通过python REPL与CSV文件交互的工具的AutoGPT
Define any other `tools` you want to use below:,在下面定义您想要使用的任何其他“工具”：
**Browse a web page with PlayWright**,**与剧作家一起浏览网页**
**Q&A Over a webpage**,**网页问答**
Help the model ask more directed questions of web pages to avoid cluttering its memory,帮助模型对网页提出更多有针对性的问题，以避免弄乱它的内存
`Model set-up`,“模型设置”
AutoGPT for Querying the Web,用于查询Web的AutoGPT
I've spent a lot of time over the years crawling data sources and cleaning data. Let's see if AutoGPT can help with this!,这些年来，我花了很多时间抓取数据源和清理数据。让我们看看AutoGPT是否能在这方面有所帮助！
Here is the prompt for looking up recent boston marathon times and converting them to tabular form.,以下是查找最近波士顿马拉松时间并将其转换为表格形式的提示。
Meta-Prompt,元提示符
"This is a LangChain implementation of [Meta-Prompt](https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving), by [Noah Goodman](https://cocolab.stanford.edu/ndg), for building self-improving agents.",这是【Meta-Prompt】（https：//noahgoodman.substack.com/p/meta-prompt-a-simple-self-imprompting）的一个LangChain实现，作者是【Noah Goodman】（https：//cocolab.stanford.edu/ndg），用于构建自我改进代理。
The key idea behind Meta-Prompt is to prompt the agent to reflect on its own performance and modify its own instructions.,元提示背后的关键思想是提示代理反思自己的性能并修改自己的指令。
"![figure](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F468217b9-96d9-47c0-a08b-dbf6b21b9f49_492x384.png)",！[图](https://substackcdn.com/image/fetch/f_auto，q_auto:good，fl_progressive:steep/https%3A%2F%2fsubstack-post-media.s 3.Amazonaws.com%2Fpublic%2Fimages%2f 468217 b 9-96 d 9-47 c 0-a 08 b-dbf 6 b 21 b 9 f 49_492 x 384.png）
figure,图形
Here is a description from the [original blog post](https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving):,以下是[原始博客文章]的描述（https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-impromping）：
The agent is a simple loop that starts with no instructions and follows these steps:,代理是一个简单的循环，开始时没有任何指令，并遵循以下步骤：
"Engage in conversation with a user, who may provide requests, instructions, or feedback.",与用户进行对话，用户可能会提供请求、指示或反馈。
"At the end of the episode, generate self-criticism and a new instruction using the meta-prompt",在这一集结束时，使用元提示产生自我批评和新的指示
Repeat.,重复一遍。
"The only fixed instructions for this system (which I call Meta-prompt) is the meta-prompt that governs revision of the agent’s instructions. The agent has no memory between episodes except for the instruction it modifies for itself each time. Despite its simplicity, this agent can learn over time and self-improve by incorporating useful details into its instructions.",这个系统唯一固定的指令（我称之为元提示）是管理代理指令修订的元提示。除了每次为自己修改的指令之外，代理在剧集之间没有记忆。尽管它很简单，但这种代理可以随着时间的推移学习，并通过将有用的细节纳入其指令来自我提高。
"We define two chains. One serves as the `Assistant`, and the other is a ""meta-chain"" that critiques the `Assistant`'s performance and modifies the instructions to the `Assistant`.",我们定义了两条链。一个充当“助手”，另一个是“元链”，它评论“助手”的表现并修改对“助手”的指令。
Specify a task and interact with the agent,指定任务并与代理交互
Chatbots,聊天机器人
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/chatbots),[概念指南](https://docs.langchain.com/docs/use-cases/chatbots）
"Since language models are good at producing text, that makes them ideal for creating chatbots. Aside from the base prompts/LLMs, an important concept to know for Chatbots is `memory`. Most chat based applications rely on remembering what happened in previous interactions, which `memory` is designed to help with.",由于语言模型擅长生成文本，这使得它们非常适合创建聊天机器人。除了基本提示/LLM，聊天机器人需要知道的一个重要概念是“内存”。大多数基于聊天的应用程序依赖于记住以前互动中发生的事情，而“记忆”就是为了帮助记住这些事情而设计的。
The following resources exist:,存在以下资源：
[ChatGPT Clone](../modules/agents/agent_executors/examples/chatgpt_clone.ipynb): A notebook walking through how to recreate a ChatGPT-like experience with LangChain.,[ChatGPT Clone](../modules/agents/agent_executors/examples/chatgpt_clone.ipynb)：一个笔记本，介绍如何使用LangChain重新创建类似ChatGPT的体验。
[Conversation Memory](../modules/memory/getting_started.ipynb): A notebook walking through how to use different types of conversational memory.,[Conversation Memory](../modules/Memory/getting_started.ipynb)：介绍如何使用不同类型的会话内存的笔记本。
[Conversation Agent](../modules/agents/agents/examples/conversational_agent.ipynb): A notebook walking through how to create an agent optimized for conversation.,[Conversation Agent](../modules/agents/agents/examples/conversational_agent.ipynb)：介绍如何创建针对会话进行优化的代理的笔记本。
Additional related resources include:,其他相关资源包括：
[Memory Key Concepts](../modules/memory.rst): Explanation of key concepts related to memory.,[内存关键概念]（../modules/memory.rst）：与内存相关的关键概念的解释。
[Memory Examples](../modules/memory/how_to_guides.rst): A collection of how-to examples for working with memory.,[Memory Examples](../modules/Memory/how_to_guides.rst)：一组如何使用内存的示例。
More end-to-end examples include:,更多端到端示例包括：
[Voice Assistant](chatbots/voice_assistant.ipynb): A notebook walking through how to create a voice assistant using LangChain.,[Voice Assistant]（chatbots/voice_assistant.ipynb）：介绍如何使用LangChain创建语音助手的笔记本。
Voice Assistant,语音助手
This chain creates a clone of ChatGPT with a few modifications to make it a voice assistant.  It uses the `pyttsx3` and `speech_recognition` libraries to convert text to speech and speech to text respectively. The prompt template is also changed to make it more suitable for voice assistant use.,这个链创建了ChatGPT的一个克隆，并做了一些修改，使其成为一个语音助手。它使用“pyttsx3”和“speiche_recognition”库分别将文本转换为语音和语音转换为文本。提示模板也进行了更改，使其更适合语音助手使用。
Code Understanding,代码理解
"LangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.",LangChain是一个有用的工具，旨在解析GitHub代码库。通过利用VectorStores、对话式检索链和GPT-4，它可以在整个GitHub存储库的上下文中回答问题或生成新代码。该文档页面概述了该系统的基本组件，并指导使用LangChain在GitHub存储库中更好地理解代码、回答上下文问题和生成代码。
Conversational Retriever Chain,对话检索器链
"Conversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.",对话式RetrieverChain是一个以检索为中心的系统，它与存储在VectorStore中的数据进行交互。它利用高级技术，如上下文感知过滤和排名，为给定的用户查询检索最相关的代码片段和信息。对话检索链旨在提供高质量、相关的结果，同时考虑对话历史和上下文。
LangChain Workflow for Code Understanding and Generation,用于代码理解和生成的LangChain工作流
"Index the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.",索引代码库：克隆目标存储库，加载其中的所有文件，分块文件，并执行索引过程。或者，您可以跳过这一步，使用已经索引的数据集。
"Embedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore. Query Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.",嵌入和代码存储：使用代码感知嵌入模型嵌入代码片段，并存储在VectorStore中。查询理解：GPT-4处理用户查询，掌握上下文并提取相关细节。
Construct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.,构造检索器：对话式检索链搜索VectorStore以识别与给定查询最相关的代码片段。
Build the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed.,构建对话链：定制检索器设置，并根据需要定义任何用户定义的过滤器。
"Ask questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.",提问：定义一个关于代码库的提问列表，然后使用ConversationalRetrievalChain生成上下文感知的答案。LLM（GPT-4）基于检索到的代码片段和对话历史生成全面的、上下文感知的答案。
The full tutorial is available below.,完整教程如下。
[Twitter the-algorithm codebase analysis with Deep Lake](code/twitter-the-algorithm-analysis-deeplake.ipynb): A notebook walking through how to parse github source code and run queries conversation.,[Twitter the-algorithm codebase analysis with Deep Lake]（code/twitter-the-algorithm-analysis-deeplake.ipynb）：一个笔记本，介绍如何解析github源代码并运行查询对话。
[LangChain codebase analysis with Deep Lake](code/code-analysis-deeplake.ipynb): A notebook walking through how to analyze and do question answering over THIS code base.,[LangChain codebase analysis with Deep Lake]（code/code-analysis-deeplake.ipynb）：一个笔记本，介绍如何在这个代码库上分析和回答问题。
"Use LangChain, GPT and Deep Lake to work with code base",使用LangChain、GPT和Deep Lake处理代码库
"In this tutorial, we are going to use Langchain + Deep Lake with GPT to analyze the code base of the LangChain itself.",在本教程中，我们将使用Langchain+Deep Lake和GPT来分析Langchain本身的代码库。
Design,设计
Prepare data:,准备数据：
Upload all python project files using the `langchain.document_loaders.TextLoader`. We will call these files the **documents**.,使用“langchain.document_loaders.TextLoader”上传所有python项目文件。我们将这些文件称为**文档**。
Split all documents to chunks using the `langchain.text_splitter.CharacterTextSplitter`.,使用“langchain.text_splitter.CharacterTextSplitter”将所有文档拆分成块。
Embed chunks and upload them into the DeepLake using `langchain.embeddings.openai.OpenAIEmbeddings` and `langchain.vectorstores.DeepLake`,嵌入块并使用“langchain.embeddings.openai.OpenAIEmbeddings”和“langchain.vectorstores.DeepLake”将它们上传到DeepLake中。
Question-Answering:,问答：
Build a chain from `langchain.chat_models.ChatOpenAI` and `langchain.chains.ConversationalRetrievalChain`,从“langchain.chat_models.ChatOpenAI”和“langchain.chains.ConversationalRetrievalChain”构建链
Prepare questions.,准备问题。
Get answers running the chain.,获得运行链的答案。
Implementation,实施
Integration preparations,整合准备工作
We need to set up keys for external services and install necessary python libraries.,我们需要为外部服务设置密钥，并安装必要的python库。
"Set up OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate.",设置OpenAI嵌入，深湖多模态矢量存储api和认证。
For full documentation of Deep Lake please follow https://docs.activeloop.ai/ and API reference https://docs.deeplake.ai/en/latest/,有关Deep Lake的完整文档，请遵循https://docs.activeloop.ai/和API参考https://docs.deeplake.ai/en/latest/
Authenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the platform at [app.activeloop.ai](https://app.activeloop.ai),如果您想要创建自己的数据集并发布它，请在Deep Lake中进行身份验证。您可以从平台[app.activeloop.ai](https://app.activeloop.ai）获取API密钥
Prepare data,准备数据
Load all repository files. Here we assume this notebook is downloaded as the part of the langchain fork and we work with the python files of the `langchain` repo.,加载所有存储库文件。这里，我们假设这个笔记本是作为langchain fork的一部分下载的，我们使用“langchain”repo的python文件。
"If you want to use files from different repo, change `root_dir` to the root dir of your repo.",如果您想使用不同repo中的文件，请将“root_dir”更改为repo的根目录。
"Then, chunk the files",然后，将文件分块
Then embed chunks and upload them to the DeepLake.,然后嵌入块并上传到DeepLake。
This can take several minutes.,这可能需要几分钟。
Question Answering,问答
"First load the dataset, construct the retriever, then construct the Conversational Chain",首先加载数据集，构造检索器，然后构造对话链
You can also specify user defined functions using [Deep Lake filters](https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter),您还可以使用[Deep Lake filters](https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter）指定用户定义的函数
-> **Question**: What is the class hierarchy?,->**问题**：什么是类层次结构？
"**Answer**: There are several class hierarchies in the provided code, so I'll list a few:",**回答**：在提供的代码中有几个类层次结构，所以我将列出几个：
`BaseModel` -> `ConstitutionalPrinciple`: `ConstitutionalPrinciple` is a subclass of `BaseModel`.,“BaseModel”->“ConstitutionalPrinciple”:“ConstitutionalPrinciple”是“BaseModel”的子类。
"`BasePromptTemplate` -> `StringPromptTemplate`, `AIMessagePromptTemplate`, `BaseChatPromptTemplate`, `ChatMessagePromptTemplate`, `ChatPromptTemplate`, `HumanMessagePromptTemplate`, `MessagesPlaceholder`, `SystemMessagePromptTemplate`, `FewShotPromptTemplate`, `FewShotPromptWithTemplates`, `Prompt`, `PromptTemplate`: All of these classes are subclasses of `BasePromptTemplate`.",`BasePromptTemplate`->`StringPromptTemplate`、`AIMessagePromptTemplate`、`ChatMessagePromptTemplate`、`ChatPromptTemplate`、`HumanMessagePromptTemplate`、`MessagePlaceholder`、`SystemMessagePromptTemplate`、`FewShotPromptTemplate`、`FewShotPromptTemplate`、`FewShotPromptTemplate`、`FewShotPromptWithTemplates`、`FewShotPromptTemplate`、`FewShotPromptWithTemplate`、`Promp
"`APIChain`, `Chain`, `MapReduceDocumentsChain`, `MapRerankDocumentsChain`, `RefineDocumentsChain`, `StuffDocumentsChain`, `HypotheticalDocumentEmbedder`, `LLMChain`, `LLMBashChain`, `LLMCheckerChain`, `LLMMathChain`, `LLMRequestsChain`, `PALChain`, `QAWithSourcesChain`, `VectorDBQAWithSourcesChain`, `VectorDBQA`, `SQLDatabaseChain`: All of these classes are subclasses of `Chain`.",“APIChain”、“Chain”、“MapReduceDocumentsChain”、“MapRerankDocumentsChain”、“RefineDocumentsChain”、“StuffDocumentsChain”、“HypotheticalDocumentEmbedder”、“LLMChain”、“llmchacherchain”、“llmchathchain”、“LLMRequestsChain”、“PALChain”、“QAWithSourcesChain”、“VectorDBQA”、“SQLDatabaseChain”:所有这些类都是“Chain”的子类。
`BaseLoader`: `BaseLoader` is a subclass of `ABC`.,“BaseLoader”:“BaseLoader”是“ABC”的子类。
"`BaseTracer` -> `ChainRun`, `LLMRun`, `SharedTracer`, `ToolRun`, `Tracer`, `TracerException`, `TracerSession`: All of these classes are subclasses of `BaseTracer`.",“BaseTracer”->“ChainRun”、“LLMRun”、“SharedTracer”、“ToolRun”、“Tracer”、“TracerException”、“TracerSession”:所有这些类都是“BaseTracer”的子类。
"`OpenAIEmbeddings`, `HuggingFaceEmbeddings`, `CohereEmbeddings`, `JinaEmbeddings`, `LlamaCppEmbeddings`, `HuggingFaceHubEmbeddings`, `TensorflowHubEmbeddings`, `SagemakerEndpointEmbeddings`, `HuggingFaceInstructEmbeddings`, `SelfHostedEmbeddings`, `SelfHostedHuggingFaceEmbeddings`, `SelfHostedHuggingFaceInstructEmbeddings`, `FakeEmbeddings`, `AlephAlphaAsymmetricSemanticEmbedding`, `AlephAlphaSymmetricSemanticEmbedding`: All of these classes are subclasses of `BaseLLM`.",“OpenAIEmbeddings”、“HuggingFaceEmbeddings”、“CohereEmbeddings”、“JinaEmbeddings”、“LlamaCppEmbeddings”、“HuggingFaceHubEmbeddings”、“TensorflowHubEmbeddings”、“sageMakeEndpointembeddings”、“HuggingFaceInstructEmbeddings”、“SelfHostedHuggingFaceEmbeddings”、“SelfHostedHuggingFaceEmbeddings”、“SelfHostedHuggingFaceInstructEmbeddings”、
-> **Question**: What classes are derived from the Chain class?,->**问题**：哪些类是从链类派生的？
**Answer**: There are multiple classes that are derived from the Chain class. Some of them are:,**答案**：有多个类是从Chain类派生的。其中包括：
APIChain,根尖链
AnalyzeDocumentChain,AnalyzeDocumentChain
ChatVectorDBChain,ChatVectorDBChain
CombineDocumentsChain,CombineDocumentsChain
ConstitutionalChain,宪法链
ConversationChain,对话链
GraphQAChain,图形链
HypotheticalDocumentEmbedder,HypotheticalDocumentEmbedder
LLMChain,LLMChain
LLMCheckerChain,LLMCheckerChain
LLMRequestsChain,LLMRequestsChain
LLMSummarizationCheckerChain,LLMSummarizationCheckerChain
MapReduceChain,MapReduceChain
OpenAPIEndpointChain,OpenAPIEndpointChain
PALChain,棕榈链
QAWithSourcesChain,QAWithSourceShain
RetrievalQA,检索值QA
RetrievalQAWithSourcesChain,RetrievalQAWithSourceShain
SequentialChain,顺序链
SQLDatabaseChain,SQLDatabaseChain
TransformChain,转换链
VectorDBQA,矢量BQA
VectorDBQAWithSourcesChain,VectorDBQAWithSourceShain
There might be more classes that are derived from the Chain class as it is possible to create custom classes that extend the Chain class.,可能有更多的类是从链类派生的，因为可以创建扩展链类的自定义类。
-> **Question**: What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?,->**问题**：单元测试不包括。/langchain/utilities/forlder中的哪些类和函数？
**Answer**: All classes and functions in the `./langchain/utilities/` folder seem to have unit tests written for them.,**答案**：`./langchain/utilities/`文件夹中的所有类和函数似乎都为它们编写了单元测试。
"Analysis of Twitter the-algorithm source code with LangChain, GPT4 and Deep Lake",基于LangChain、GPT 4和Deep Lake的Twitter算法源代码分析
"In this tutorial, we are going to use Langchain + Deep Lake with GPT4 to analyze the code base of the twitter algorithm.",在本教程中，我们将使用Langchain+Deep Lake和GPT 4来分析twitter算法的代码库。
"Define OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate. For full documentation of Deep Lake please follow [docs](https://docs.activeloop.ai/) and [API reference](https://docs.deeplake.ai/en/latest/).",定义OpenAI嵌入，深湖多模态矢量存储api和认证。有关深湖的完整文档，请访问【docs】（https：//docs.activeloop.ai/）和【API参考】（https：//docs.deeplake.ai/en/latest/）。
Authenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the [platform](https://app.activeloop.ai),如果您想要创建自己的数据集并发布它，请在Deep Lake中进行身份验证。您可以从[平台](https://app.activeloop.ai）获取API密钥
disallowed_special=() is required to avoid `Exception: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte` from tiktoken for some repositories,disallowed_special=(）是避免“异常”:“utf-8”编解码器无法解码位置0中的字节0 XFF：某些存储库的tiktoken中的无效起始字节”所必需的
1. Index the code base (optional),1.索引代码库（可选）
"You can directly skip this part and directly jump into using already indexed dataset. To begin with, first we will clone the repository, then parse and chunk the code base and use OpenAI indexing.",您可以直接跳过这一部分，直接使用已经索引的数据集。首先，我们将克隆存储库，然后解析和分块代码库，并使用OpenAI索引。
Load all files inside the repository,加载存储库中的所有文件
Execute the indexing. This will take about ~4 mins to compute embeddings and upload to Activeloop. You can then publish the dataset to be public.,执行索引。计算嵌入并上传到Activeloop大约需要4分钟。然后，您可以将数据集发布为公共数据集。
2. Question Answering on Twitter algorithm codebase,2.Twitter算法代码库上的问答
-> **Question**: What does favCountParams do?,->**问题**：favCountParams是做什么的？
"**Answer**: `favCountParams` is an optional ThriftLinearFeatureRankingParams instance that represents the parameters related to the ""favorite count"" feature in the ranking process. It is used to control the weight of the favorite count feature while ranking tweets. The favorite count is the number of times a tweet has been marked as a favorite by users, and it is considered an important signal in the ranking of tweets. By using `favCountParams`, the system can adjust the importance of the favorite count while calculating the final ranking score of a tweet.",**答案**:“favCountParams”是一个可选的ThriftLinearFeatureRankingParams实例，它表示与排名过程中的“最喜欢的计数”功能相关的参数。它用于在对推文进行排名时控制收藏计数功能的权重。最喜欢的计数是一条推文被用户标记为最喜欢的次数，它被认为是推文排名中的一个重要信号。通过使用“favCountParams”，系统可以在计算推文的最终排名分数时调整喜爱计数的重要性。
"-> **Question**: is it Likes + Bookmarks, or not clear from the code?",->**问题**：是喜欢+书签，还是从代码上不清楚？
"**Answer**: From the provided code, it is not clear if the favorite count metric is determined by the sum of likes and bookmarks. The favorite count is mentioned in the code, but there is no explicit reference to how it is calculated in terms of likes and bookmarks.",**答案**：从提供的代码来看，不清楚收藏夹计数指标是否由喜欢和书签的总和决定。代码中提到了最喜欢的计数，但没有明确提到它是如何根据喜欢和书签计算的。
-> **Question**: What are the major negative modifiers that lower your linear ranking parameters?,->**问题**：降低线性排名参数的主要负面修正因素是什么？
"**Answer**: In the given code, major negative modifiers that lower the linear ranking parameters are:",**答案**：在给定的代码中，降低线性排序参数的主要否定修饰符是：
"`scoringData.querySpecificScore`: This score adjustment is based on the query-specific information. If its value is negative, it will lower the linear ranking parameters.",`scoringData.querySpecificScore`：此分数调整基于特定于查询的信息。如果它的值为负，它将降低线性排序参数。
"`scoringData.authorSpecificScore`: This score adjustment is based on the author-specific information. If its value is negative, it will also lower the linear ranking parameters.",`scoringData.authorSpecificScore`：此分数调整基于特定于作者的信息。如果它的值为负，也会降低线性排序参数。
"Please note that I cannot provide more information on the exact calculations of these negative modifiers, as the code for their determination is not provided.",请注意，我不能提供更多关于这些负修饰符的精确计算的信息，因为没有提供确定它们的代码。
-> **Question**: How do you get assigned to SimClusters?,->**问题**：如何分配到SimClusters？
"**Answer**: The assignment to SimClusters occurs through a Metropolis-Hastings sampling-based community detection algorithm that is run on the Producer-Producer similarity graph. This graph is created by computing the cosine similarity scores between the users who follow each producer. The algorithm identifies communities or clusters of Producers with similar followers, and takes a parameter *k* for specifying the number of communities to be detected.",**答案**：分配给SimClusters是通过基于Metropolis-Hastings抽样的社区检测算法进行的，该算法在生产者-生产者相似性图上运行。该图是通过计算关注每个生产商的用户之间的余弦相似性得分来创建的。该算法识别具有相似追随者的生产者的社区或集群，并采用参数*k*来指定要检测的社区的数量。
"After the community detection, different users and content are represented as sparse, interpretable vectors within these identified communities (SimClusters). The resulting SimClusters embeddings can be used for various recommendation tasks.",在社区检测之后，不同的用户和内容在这些识别的社区（模拟集群）中被表示为稀疏的、可解释的向量。由此产生的SimClusters嵌入可用于各种推荐任务。
-> **Question**: What is needed to migrate from one SimClusters to another SimClusters?,->**问题**：从一个SimClusters迁移到另一个SimClusters需要什么？
"**Answer**: To migrate from one SimClusters representation to another, you can follow these general steps:",**回答**：要从一个SimClusters表示迁移到另一个SimClusters表示，可以遵循以下一般步骤：
"**Prepare the new representation**: Create the new SimClusters representation using any necessary updates or changes in the clustering algorithm, similarity measures, or other model parameters. Ensure that this new representation is properly stored and indexed as needed.",**准备新表示**：使用聚类算法、相似性度量或其他模型参数中的任何必要更新或更改来创建新的SimClusters表示。确保根据需要正确存储和索引这个新表示。
"**Update the relevant code and configurations**: Modify the relevant code and configuration files to reference the new SimClusters representation. This may involve updating paths or dataset names to point to the new representation, as well as changing code to use the new clustering method or similarity functions if applicable.",**更新相关代码和配置**：修改相关代码和配置文件以引用新的SimClusters表示。这可能涉及更新路径或数据集名称以指向新的表示形式，以及更改代码以使用新的聚类方法或相似性函数（如果适用）。
"**Test the new representation**: Before deploying the changes to production, thoroughly test the new SimClusters representation to ensure its effectiveness and stability. This may involve running offline jobs like candidate generation and label candidates, validating the output, as well as testing the new representation in the evaluation environment using evaluation tools like TweetSimilarityEvaluationAdhocApp.",**测试新的表示形式**：在将更改部署到生产环境之前，彻底测试新的SimClusters表示形式，以确保其有效性和稳定性。这可能涉及运行离线作业，如候选生成和标记候选，验证输出，以及使用评估工具（如TweetSimilarityEvaluationAdhocApp）在评估环境中测试新的表示。
"**Deploy the changes**: Once the new representation has been tested and validated, deploy the changes to production. This may involve creating a zip file, uploading it to the packer, and then scheduling it with Aurora. Be sure to monitor the system to ensure a smooth transition between representations and verify that the new representation is being used in recommendations as expected.",**部署变更**：一旦新的表示已经过测试和验证，就将变更部署到生产环境中。这可能包括创建一个zip文件，将其上传到打包程序，然后用Aurora对其进行调度。请务必监控系统，以确保表示之间的平稳过渡，并验证新的表示是否如预期的那样在建议中使用。
"**Monitor and assess the new representation**: After the new representation has been deployed, continue to monitor its performance and impact on recommendations. Take note of any improvements or issues that arise and be prepared to iterate on the new representation if needed. Always ensure that the results and performance metrics align with the system's goals and objectives.",**监测和评估新的代表**：在部署新的代表后，继续监测其业绩和对建议的影响。注意出现的任何改进或问题，并准备好在需要时迭代新的表示。始终确保结果和绩效指标与系统的目标一致。
-> **Question**: How much do I get boosted within my cluster?,->**问题**：我在集群中得到多少提升？
"**Answer**: It's not possible to determine the exact amount your content is boosted within your cluster in the SimClusters representation without specific data about your content and its engagement metrics. However, a combination of factors, such as the favorite score and follow score, alongside other engagement signals and SimCluster calculations, influence the boosting of content.",**回答**：如果没有关于您的内容及其参与度指标的具体数据，就不可能确定您的内容在SimClusters表示中的集群中的确切提升量。然而，一系列因素的组合，如最喜欢的分数和关注分数，以及其他参与信号和SimCluster计算，会影响内容的提升。
-> **Question**: How does Heavy ranker work. what are it’s main inputs?,->**问题**：重排名器是如何工作的。它的主要输入是什么？
**Answer**: The Heavy Ranker is a machine learning model that plays a crucial role in ranking and scoring candidates within the recommendation algorithm. Its primary purpose is to predict the likelihood of a user engaging with a tweet or connecting with another user on the platform.,**回答**：Heavy Ranker是一种机器学习模型，在推荐算法中对候选人进行排名和评分时起着至关重要的作用。它的主要目的是预测用户参与推文或与平台上的另一个用户联系的可能性。
Main inputs to the Heavy Ranker consist of:,重型分级器的主要输入包括：
"Static Features: These are features that can be computed directly from a tweet at the time it's created, such as whether it has a URL, has cards, has quotes, etc. These features are produced by the Index Ingester as the tweets are generated and stored in the index.",静态特征：这些特征可以在推文创建时直接从推文中计算出来，比如它是否有URL、有卡片、有引用等等。这些特征是由索引摄取器在推文生成并存储在索引中时产生的。
"Real-time Features: These per-tweet features can change after the tweet has been indexed. They mostly consist of social engagements like retweet count, favorite count, reply count, and some spam signals that are computed with later activities. The Signal Ingester, which is part of a Heron topology, processes multiple event streams to collect and compute these real-time features.",实时功能：这些每条推文的功能可以在推文被索引后改变。它们主要由社交活动组成，如转发计数、收藏计数、回复计数和一些垃圾邮件信号，这些都是在以后的活动中计算的。信号摄取器是Heron拓扑的一部分，它处理多个事件流来收集和计算这些实时特征。
"User Table Features: These per-user features are obtained from the User Table Updater that processes a stream written by the user service. This input is used to store sparse real-time user information, which is later propagated to the tweet being scored by looking up the author of the tweet.",用户表特性：这些每个用户的特性是从处理用户服务编写的流的用户表更新程序中获得的。这个输入用于存储稀疏的实时用户信息，这些信息随后通过查找推文的作者传播到被评分的推文。
"Search Context Features: These features represent the context of the current searcher, like their UI language, their content consumption, and the current time (implied). They are combined with Tweet Data to compute some of the features used in scoring.",搜索上下文特性：这些特性代表当前搜索者的上下文，比如他们的UI语言、他们的内容消费和当前时间（隐含的）。它们与推特数据相结合，计算评分中使用的一些特征。
These inputs are then processed by the Heavy Ranker to score and rank candidates based on their relevance and likelihood of engagement by the user.,然后，这些输入由重排序器处理，以基于候选的相关性和用户参与的可能性对候选进行评分和排序。
-> **Question**: How can one influence Heavy ranker?,->**问题**：一个人如何影响重排名？
"**Answer**: To influence the Heavy Ranker's output or ranking of content, consider the following actions:",**回答**：要影响重排名者的输出或内容排名，请考虑以下操作：
"Improve content quality: Create high-quality and engaging content that is relevant, informative, and valuable to users. High-quality content is more likely to receive positive user engagement, which the Heavy Ranker considers when ranking content.",提高内容质量：创建对用户相关、信息丰富且有价值的高质量、引人入胜的内容。高质量的内容更有可能获得积极的用户参与，这是重排名者在对内容进行排名时考虑的因素。
"Increase user engagement: Encourage users to interact with content through likes, retweets, replies, and comments. Higher engagement levels can lead to better ranking in the Heavy Ranker's output.",提高用户参与度：鼓励用户通过喜欢、转发、回复和评论与内容互动。更高的参与度可以在排名靠前的产出中获得更好的排名。
"Optimize your user profile: A user's reputation, based on factors such as their follower count and follower-to-following ratio, may impact the ranking of their content. Maintain a good reputation by following relevant users, keeping a reasonable follower-to-following ratio and engaging with your followers.",优化你的用户档案：用户的声誉，基于他们的追随者数量和追随者与追随者的比率等因素，可能会影响他们内容的排名。通过关注相关用户、保持合理的关注者与关注者比率以及与你的关注者互动来保持良好的声誉。
"Enhance content discoverability: Use relevant keywords, hashtags, and mentions in your tweets, making it easier for users to find and engage with your content. This increased discoverability may help improve the ranking of your content by the Heavy Ranker.",增强内容的可发现性：在你的推文中使用相关的关键词、标签和提及，让用户更容易找到并参与你的内容。这种增加的可发现性可能有助于提高你的内容的排名。
"Leverage multimedia content: Experiment with different content formats, such as videos, images, and GIFs, which may capture users' attention and increase engagement, resulting in better ranking by the Heavy Ranker.",利用多媒体内容：尝试不同的内容格式，如视频、图像和gif，这可能会吸引用户的注意力并增加参与度，从而使排名靠前的人排名更好。
"User feedback: Monitor and respond to feedback for your content. Positive feedback may improve your ranking, while negative feedback provides an opportunity to learn and improve.",用户反馈：监控并回应对你的内容的反馈。正面反馈可能会提高你的排名，而负面反馈提供了一个学习和提高的机会。
"Note that the Heavy Ranker uses a combination of machine learning models and various features to rank the content. While the above actions may help influence the ranking, there are no guarantees as the ranking process is determined by a complex algorithm, which evolves over time.",请注意，Heavy Ranker使用机器学习模型和各种功能的组合来对内容进行排名。虽然上述操作可能有助于影响排名，但不能保证排名过程是由复杂的算法决定的，该算法会随着时间的推移而演变。
-> **Question**: why threads and long tweets do so well on the platform?,->**问题**：为什么线程和长推文在平台上表现如此之好？
**Answer**: Threads and long tweets perform well on the platform for several reasons:,**回答**：线程和长推文在平台上表现良好，原因有几个：
"**More content and context**: Threads and long tweets provide more information and context about a topic, which can make the content more engaging and informative for users. People tend to appreciate a well-structured and detailed explanation of a subject or a story, and threads and long tweets can do that effectively.",**更多内容和上下文**：线程和长推文提供了关于某个主题的更多信息和上下文，这可以使内容对用户更有吸引力和信息量。人们倾向于欣赏对一个主题或故事的结构良好和详细的解释，线程和长推文可以有效地做到这一点。
"**Increased user engagement**: As threads and long tweets provide more content, they also encourage users to engage with the tweets through replies, retweets, and likes. This increased engagement can lead to better visibility of the content, as the Twitter algorithm considers user engagement when ranking and surfacing tweets.",**增加用户参与度**：由于线程和长推文提供了更多内容，它们也鼓励用户通过回复、转发和喜欢来参与推文。这种参与度的增加可以提高内容的可见性，因为Twitter算法在对推文进行排名和显示时会考虑用户参与度。
"**Narrative structure**: Threads enable users to tell stories or present arguments in a step-by-step manner, making the information more accessible and easier to follow. This narrative structure can capture users' attention and encourage them to read through the entire thread and interact with the content.",**叙述结构**：线程使用户能够以一步一步的方式讲述故事或提出论点，使信息更容易获取和理解。这种叙述结构可以吸引用户的注意力，并鼓励他们通读整个主题并与内容互动。
"**Expanded reach**: When users engage with a thread, their interactions can bring the content to the attention of their followers, helping to expand the reach of the thread. This increased visibility can lead to more interactions and higher performance for the threaded tweets.",**扩大覆盖范围**：当用户参与到一个主题中时，他们的互动可以将内容吸引到他们的追随者的注意，从而有助于扩大该主题的覆盖范围。这种增加的可见性可以为线程推文带来更多的交互和更高的性能。
"**Higher content quality**: Generally, threads and long tweets require more thought and effort to create, which may lead to higher quality content. Users are more likely to appreciate and interact with high-quality, well-reasoned content, further improving the performance of these tweets within the platform.",**更高的内容质量**：一般来说，线程和长推文需要更多的思考和努力来创建，这可能会导致更高质量的内容。用户更有可能欣赏高质量、理由充分的内容并与之互动，从而进一步提高这些推文在平台内的性能。
"Overall, threads and long tweets perform well on Twitter because they encourage user engagement and provide a richer, more informative experience that users find valuable.",总的来说，线程和长推文在Twitter上表现良好，因为它们鼓励用户参与，并提供用户认为有价值的更丰富、更丰富的体验。
-> **Question**: Are thread and long tweet creators building a following that reacts to only threads?,->**问题**：线程和长推文的创建者是否正在建立一个只对线程做出反应的追随者？
"**Answer**: Based on the provided code and context, there isn't enough information to conclude if the creators of threads and long tweets primarily build a following that engages with only thread-based content. The code provided is focused on Twitter's recommendation and ranking algorithms, as well as infrastructure components like Kafka, partitions, and the Follow Recommendations Service (FRS). To answer your question, data analysis of user engagement and results of specific edge cases would be required.",**回答**：根据提供的代码和上下文，没有足够的信息来得出结论，如果线程和长推文的创建者主要是建立一个只关注基于线程的内容的追随者。提供的代码侧重于Twitter的推荐和排名算法，以及基础设施组件，如Kafka、分区和Follow推荐服务（FRS）。要回答您的问题，需要对用户参与度和特定边缘案例的结果进行数据分析。
-> **Question**: Do you need to follow different strategies to get most followers vs to get most likes and bookmarks per tweet?,->**问题**：你需要遵循不同的策略来获得最多的关注者和获得最多的喜欢和书签吗？
"**Answer**: Yes, different strategies need to be followed to maximize the number of followers compared to maximizing likes and bookmarks per tweet. While there may be some overlap in the approaches, they target different aspects of user engagement.",**回答**：是的，与最大化每条推文的喜欢和书签相比，需要遵循不同的策略来最大化关注者的数量。虽然这些方法可能有一些重叠，但它们针对用户参与的不同方面。
Maximizing followers: The primary focus is on growing your audience on the platform. Strategies include:,最大化追随者：主要焦点是在平台上增加你的受众。战略包括：
Consistently sharing high-quality content related to your niche or industry.,持续分享与你的利基或行业相关的高质量内容。
"Engaging with others on the platform by replying, retweeting, and mentioning other users.",通过回复、转发和提及其他用户与平台上的其他人互动。
Using relevant hashtags and participating in trending conversations.,使用相关标签并参与趋势对话。
Collaborating with influencers and other users with a large following.,与有影响力的人和其他拥有大量追随者的用户合作。
Posting at optimal times when your target audience is most active.,在目标受众最活跃的最佳时间发帖。
"Optimizing your profile by using a clear profile picture, catchy bio, and relevant links.",通过使用清晰的个人资料图片、吸引人的简历和相关链接来优化您的个人资料。
Maximizing likes and bookmarks per tweet: The focus is on creating content that resonates with your existing audience and encourages engagement. Strategies include:,最大化每条推文的赞和书签：重点是创造能引起现有受众共鸣并鼓励参与的内容。战略包括：
Crafting engaging and well-written tweets that encourage users to like or save them.,精心制作引人入胜、写得很好的推文，鼓励用户喜欢或保存它们。
"Incorporating visually appealing elements, such as images, GIFs, or videos, that capture attention.",加入视觉上吸引人的元素，如图像、gif或视频，以吸引注意力。
"Asking questions, sharing opinions, or sparking conversations that encourage users to engage with your tweets.",提问、分享观点或引发对话，鼓励用户参与你的推文。
Using analytics to understand the type of content that resonates with your audience and tailoring your tweets accordingly.,使用分析来了解与你的受众产生共鸣的内容类型，并相应地定制你的推文。
"Posting a mix of educational, entertaining, and promotional content to maintain variety and interest.",发布教育、娱乐和宣传内容的组合，以保持多样性和趣味性。
"Timing your tweets strategically to maximize engagement, likes, and bookmarks per tweet.",战略性地安排你的推文时间，以最大限度地提高每条推文的参与度、赞数和书签数。
"Both strategies can overlap, and you may need to adapt your approach by understanding your target audience's preferences and analyzing your account's performance. However, it's essential to recognize that maximizing followers and maximizing likes and bookmarks per tweet have different focuses and require specific strategies.",这两种策略可能会重叠，您可能需要通过了解目标受众的偏好和分析您的客户表现来调整您的方法。然而，重要的是要认识到，最大化关注者和最大化每条推文的喜欢和书签有不同的重点，需要特定的策略。
-> **Question**: Content meta data and how it impacts virality (e.g. ALT in images).,->**问题**：内容元数据及其如何影响病毒式传播（例如图像中的ALT）。
"**Answer**: There is no direct information in the provided context about how content metadata, such as ALT text in images, impacts the virality of a tweet or post. However, it's worth noting that including ALT text can improve the accessibility of your content for users who rely on screen readers, which may lead to increased engagement for a broader audience. Additionally, metadata can be used in search engine optimization, which might improve the visibility of the content, but the context provided does not mention any specific correlation with virality.",**回答**：在提供的上下文中没有关于内容元数据（如图像中的替代文本）如何影响推文或帖子的病毒式传播的直接信息。然而，值得注意的是，对于依赖屏幕阅读器的用户来说，包含替代文本可以提高内容的可访问性，这可能会增加更广泛受众的参与度。此外，元数据可以用于搜索引擎优化，这可能会提高内容的可见性，但提供的上下文没有提到与病毒式传播的任何特定相关性。
-> **Question**: What are some unexpected fingerprints for spam factors?,->**问题**：垃圾邮件因素有哪些意想不到的指纹？
"**Answer**: In the provided context, an unusual indicator of spam factors is when a tweet contains a non-media, non-news link. If the tweet has a link but does not have an image URL, video URL, or news URL, it is considered a potential spam vector, and a threshold for user reputation (tweepCredThreshold) is set to MIN_TWEEPCRED_WITH_LINK.",**回答**：在提供的上下文中，垃圾邮件因素的一个不寻常指标是推文包含非媒体、非新闻链接。如果推文有链接，但没有图像URL、视频URL或新闻URL，则它被认为是潜在的垃圾邮件载体，并且用户信誉阈值（tweepCredThreshold）被设置为MIN_TWEEPCRED_WITH_LINK。
"While this rule may not cover all possible unusual spam indicators, it is derived from the specific codebase and logic shared in the context.",虽然此规则可能不会涵盖所有可能的异常垃圾邮件指示器，但它是从上下文中共享的特定代码库和逻辑中派生出来的。
-> **Question**: Is there any difference between company verified checkmarks and blue verified individual checkmarks?,->**问题**：公司验证的复选标记和蓝色验证的个人复选标记有什么区别吗？
"**Answer**: Yes, there is a distinction between the verified checkmarks for companies and blue verified checkmarks for individuals. The code snippet provided mentions ""Blue-verified account boost"" which indicates that there is a separate category for blue verified accounts. Typically, blue verified checkmarks are used to indicate notable individuals, while verified checkmarks are for companies or organizations.",**回答**：是的，公司的已验证复选标记和个人的蓝色已验证复选标记是有区别的。提供的代码片段提到了“蓝色验证帐户提升”，这表明蓝色验证帐户有一个单独的类别。通常，蓝色验证复选标记用于表示著名的个人，而验证复选标记用于公司或组织。
Evaluation,评价
`Conceptual Guide <https://docs.langchain.com/docs/use-cases/evaluation>`_,`概念指南<https://docs.langchain.com/docs/use-cases/evaluation>`_
"This section of documentation covers how we approach and think about evaluation in LangChain. Both evaluation of internal chains/agents, but also how we would recommend people building on top of LangChain approach evaluation.",本节文档涵盖了我们如何在LangChain中处理和思考评估。既包括对内部链/代理的评估，也包括我们如何推荐人们在LangChain方法评估的基础上进行建设。
The Problem,问题
It can be really hard to evaluate LangChain chains and agents. There are two main reasons for this:,评估LangChain链和代理真的很难。这主要有两个原因：
**# 1: Lack of data**,**#1：缺乏数据**
"You generally don't have a ton of data to evaluate your chains/agents over before starting a project. This is usually because Large Language Models (the core of most chains/agents) are terrific few-shot and zero shot learners, meaning you are almost always able to get started on a particular task (text-to-SQL, question answering, etc) without a large dataset of examples. This is in stark contrast to traditional machine learning where you had to first collect a bunch of datapoints before even getting started using a model.",在开始一个项目之前，你通常没有大量的数据来评估你的连锁店/代理。这通常是因为大型语言模型（大多数链/代理的核心）是极好的少镜头和零镜头学习者，这意味着您几乎总是能够在没有大量示例数据集的情况下开始特定的任务（文本到SQL、问题回答等）。这与传统的机器学习形成鲜明对比，在传统的机器学习中，在开始使用模型之前，你必须首先收集一堆数据点。
**# 2: Lack of metrics**,**#2：缺少指标**
"Most chains/agents are performing tasks for which there are not very good metrics to evaluate performance. For example, one of the most common use cases is generating text of some form. Evaluating generated text is much more complicated than evaluating a classification prediction, or a numeric prediction.",大多数连锁店/代理正在执行没有很好的指标来评估性能的任务。例如，最常见的用例之一是生成某种形式的文本。评估生成的文本比评估分类预测或数字预测要复杂得多。
The Solution,解决方案
"LangChain attempts to tackle both of those issues. What we have so far are initial passes at solutions - we do not think we have a perfect solution. So we very much welcome feedback, contributions, integrations, and thoughts on this.",LangChain试图解决这两个问题。到目前为止，我们所拥有的只是解决方案的初步尝试——我们不认为我们有一个完美的解决方案。因此，我们非常欢迎对此的反馈、贡献、整合和想法。
Here is what we have for each problem so far:,到目前为止，我们对每个问题的了解如下：
"We have started `LangChainDatasets <https://huggingface.co/LangChainDatasets>`_ a Community space on Hugging Face. We intend this to be a collection of open source datasets for evaluating common chains and agents. We have contributed five datasets of our own to start, but we highly intend this to be a community effort. In order to contribute a dataset, you simply need to join the community and then you will be able to upload datasets.",我们已经在拥抱脸上创建了“LangChainDatasets<https：//huggingface.co/LangChainDatasets>”社区空间。我们希望这是一个开源数据集的集合，用于评估常见的链和代理。我们已经贡献了五个我们自己的数据集，但是我们非常希望这是一个社区的努力。为了贡献一个数据集，你只需要加入社区，然后你就可以上传数据集了。
"We're also aiming to make it as easy as possible for people to create their own datasets. As a first pass at this, we've added a QAGenerationChain, which given a document comes up with question-answer pairs that can be used to evaluate question-answering tasks over that document down the line. See `this notebook <./evaluation/qa_generation.html>`_ for an example of how to use this chain.",我们还旨在让人们尽可能容易地创建自己的数据集。作为第一步，我们添加了一个QAGenerationChain，它给出了一个文档的问答对，可用于评估该文档的问答任务。有关如何使用此链的示例，请参见`this notebook<./evaluation/qa_generation.html>`_。
We have two solutions to the lack of metrics.,对于缺乏度量标准，我们有两种解决方案。
"The first solution is to use no metrics, and rather just rely on looking at results by eye to get a sense for how the chain/agent is performing. To assist in this, we have developed (and will continue to develop) `tracing <../tracing.html>`_, a UI-based visualizer of your chain and agent runs.",第一个解决方案是不使用度量标准，而是仅仅依靠肉眼观察结果来了解链/代理的表现。为此，我们开发了（并将继续开发）`tracing<../tracing.html>`_，这是一个基于UI的链和代理运行可视化工具。
The second solution we recommend is to use Language Models themselves to evaluate outputs. For this we have a few different chains and prompts aimed at tackling this issue.,我们推荐的第二个解决方案是使用语言模型本身来评估输出。为此，我们有一些不同的链和提示来解决这个问题。
The Examples,例子
"We have created a bunch of examples combining the above two solutions to show how we internally evaluate chains and agents when we are developing. In addition to the examples we've curated, we also highly welcome contributions here. To facilitate that, we've included a `template notebook <./evaluation/benchmarking_template.html>`_ for community members to use to build their own examples.",我们已经创建了一堆结合上述两种解决方案的例子来展示我们在开发时如何在内部评估链和代理。除了我们策划的例子，我们也非常欢迎在这里投稿。为了方便这一点，我们提供了一个“模板笔记本<./evaluation/benchmarking_template.html>”，供社区成员用来构建他们自己的示例。
The existing examples we have are:,我们现有的例子有：
`Question Answering (State of Union) <./evaluation/qa_benchmarking_sota.html>`_: A notebook showing evaluation of a question-answering task over a State-of-the-Union address.,`Question Answering（State of Union）<./evaluation/qa_benchmarking_sota.html>`_：显示对国情咨文问答任务的评估的笔记本。
`Question Answering (Paul Graham Essay) <./evaluation/qa_benchmarking_pg.html>`_: A notebook showing evaluation of a question-answering task over a Paul Graham essay.,`Question Answering（Paul Graham Essay）<./evaluation/qa_benchmarking_pg.html>`_：一个笔记本，显示对一篇Paul Graham文章的问答任务的评估。
`SQL Question Answering (Chinook) <./evaluation/sql_qa_benchmarking_chinook.html>`_: A notebook showing evaluation of a question-answering task over a SQL database (the Chinook database).,`SQL Question Answering（Chinook）<./evaluation/sql_qa_benchmarking_chinook.html>`_：显示对SQL数据库（Chinook数据库）上的问答任务进行评估的笔记本。
`Agent Vectorstore <./evaluation/agent_vectordb_sota_pg.html>`_: A notebook showing evaluation of an agent doing question answering while routing between two different vector databases.,`Agent Vectorstore<./evaluation/agent_vectordb_sota_pg.html>`_：一个笔记本，显示在两个不同的向量数据库之间路由时对代理进行问题回答的评估。
`Agent Search + Calculator <./evaluation/agent_benchmarking.html>`_: A notebook showing evaluation of an agent doing question answering using a Search engine and a Calculator as tools.,`Agent Search+Calculator<./evaluation/agent_benchmarking.html>`_：一个笔记本，显示使用搜索引擎和计算器作为工具回答问题的agent的评估。
"`Evaluating an OpenAPI Chain <./evaluation/openapi_eval.html>`_: A notebook showing evaluation of an OpenAPI chain, including how to generate test data if you don't have any.",“评估OpenAPI链<./evaluation/openapi_eval.html>`_：显示OpenAPI链评估的笔记本，包括如果没有测试数据，如何生成测试数据。
Other Examples,其他例子
"In addition, we also have some more generic resources for evaluation.",此外，我们还有一些更通用的评估资源。
`Question Answering <./evaluation/question_answering.html>`_: An overview of LLMs aimed at evaluating question answering systems in general.,`Question Answering<./evaluation/question_answering.html>`_：旨在评估一般问答系统的LLMs概述。
"`Data Augmented Question Answering <./evaluation/data_augmented_question_answering.html>`_: An end-to-end example of evaluating a question answering system focused on a specific document (a RetrievalQAChain to be precise). This example highlights how to use LLMs to come up with question/answer examples to evaluate over, and then highlights how to use LLMs to evaluate performance on those generated examples.",`Data Augmented Question Answering<./evaluation/data_augmented_question_answering.html>`_：一个针对特定文档（准确地说是RetrievalQAChain）评估问答系统的端到端示例。这个例子强调了如何使用LLM来提出问题/答案示例进行评估，然后强调了如何使用LLM来评估这些生成的示例的性能。
`Hugging Face Datasets <./evaluation/huggingface_datasets.html>`_: Covers an example of loading and using a dataset from Hugging Face for evaluation.,`Hugging Face Datasets<./evaluation/huggingface_datasets.html>`_:介绍了从Hugging Face加载和使用数据集进行评估的示例。
Agent Benchmarking: Search + Calculator,代理基准测试：搜索+计算器
Here we go over how to benchmark performance of an agent on tasks where it has access to a calculator and a search tool.,在这里，我们将讨论如何对代理在可以访问计算器和搜索工具的任务中的性能进行基准测试。
It is highly reccomended that you do any evaluation/benchmarking with tracing enabled. See [here](https://langchain.readthedocs.io/en/latest/tracing.html) for an explanation of what tracing is and how to set it up.,强烈建议您在启用跟踪的情况下进行任何评估/基准测试。有关什么是跟踪以及如何设置跟踪的解释，请参见【此处】（https：//langchain.readthedocs.io/en/latest/tracing.html）。
Loading the data,正在加载数据
"First, let's load the data.",首先，让我们加载数据。
Setting up a chain,建立连锁
Now we need to load an agent capable of answering these questions.,现在我们需要加载一个能够回答这些问题的代理。
Make a prediction,作出预测
"First, we can make predictions one datapoint at a time. Doing it at this level of granularity allows use to explore the outputs in detail, and also is a lot cheaper than running over multiple datapoints",首先，我们可以一次预测一个数据点。在这种粒度级别上执行此操作允许用户详细地探索输出，并且也比在多个数据点上运行要便宜得多
Make many predictions,做许多预测
Now we can make predictions,现在我们可以预测
Evaluate performance,评估绩效
Now we can evaluate the predictions. The first thing we can do is look at them by eye.,现在我们可以评估预测了。我们能做的第一件事就是用眼睛看它们。
"Next, we can use a language model to score them programatically",接下来，我们可以使用一个语言模型对它们进行编程评分
We can add in the graded output to the `predictions` dict and then get a count of the grades.,我们可以将分级输出添加到“预测”字典中，然后得到分数的计数。
We can also filter the datapoints to the incorrect examples and look at them.,我们还可以过滤不正确示例的数据点并查看它们。
Agent VectorDB Question Answering Benchmarking,Agent VectorDB问答基准测试
Here we go over how to benchmark performance on a question answering task using an agent to route between multiple vectordatabases.,在这里，我们将讨论如何使用代理在多个向量数据库之间路由来测试问答任务的性能。
Now we need to create some pipelines for doing question answering. Step one in that is creating indexes over the data in question.,现在我们需要创建一些管道来回答问题。第一步是在有问题的数据上创建索引。
Now we can create a question answering chain.,现在我们可以创建一个问答链。
Now we do the same for the Paul Graham data.,现在我们对保罗·格拉厄姆的数据做同样的事情。
We can now set up an agent to route between them.,我们现在可以设置一个代理在它们之间进行路由。
Benchmarking Template,基准模板
"This is an example notebook that can be used to create a benchmarking notebook for a task of your choice. Evaluation is really hard, and so we greatly welcome any contributions that can make it easier for people to experiment",这是一个示例笔记本，可用于为您选择的任务创建基准测试笔记本。评估真的很难，所以我们非常欢迎任何能让人们更容易进行实验的贡献
This next section should have an example of setting up a chain that can be run on this dataset.,下一节应该有一个设置可以在这个数据集上运行的链的例子。
Now we can make predictions.,现在我们可以做预测了。
Any guide to evaluating performance in a more systematic manner goes here.,任何以更系统的方式评估绩效的指南都在这里。
Data Augmented Question Answering,数据扩充问答
"This notebook uses some generic prompts/language models to evaluate an question answering system that uses other sources of data besides what is in the model. For example, this can be used to evaluate a question answering system over your proprietary data.",本笔记本使用一些通用提示/语言模型来评估除了模型中的数据之外还使用其他数据源的问答系统。例如，这可以用来评估基于您的专有数据的问答系统。
Let's set up an example with our favorite example - the state of the union address.,让我们用我们最喜欢的例子——国情咨文来举个例子。
Examples,例子
Now we need some examples to evaluate. We can do this in two ways:,现在我们需要一些例子来评估。我们可以通过两种方式做到这一点：
Hard code some examples ourselves,我们自己硬编码一些例子
"Generate examples automatically, using a language model",使用语言模型自动生成示例
Evaluate,评估
"Now that we have examples, we can use the question answering evaluator to evaluate our question answering chain.",现在我们有了例子，我们可以使用问答评估器来评估我们的问答链。
Evaluate with Other Metrics,使用其他指标进行评估
"In addition to predicting whether the answer is correct or incorrect using a language model, we can also use other metrics to get a more nuanced view on the quality of the answers. To do so, we can use the [Critique](https://docs.inspiredco.ai/critique/) library, which allows for simple calculation of various metrics over generated text.",除了使用语言模型预测答案是正确的还是不正确的，我们还可以使用其他指标来获得关于答案质量的更细致入微的视图。为此，我们可以使用【Critique】（https：//docs.inspiredco.ai/Critique/）库，该库允许对生成的文本进行各种度量的简单计算。
First you can get an API key from the [Inspired Cognition Dashboard](https://dashboard.inspiredco.ai) and do some setup:,首先，您可以从[Inspired Cognition Dashboard]（https：//dashboard.inspiredco.ai）获取一个API密钥，并进行一些设置：
"Then run the following code to set up the configuration and calculate the [ROUGE](https://docs.inspiredco.ai/critique/metric_rouge.html), [chrf](https://docs.inspiredco.ai/critique/metric_chrf.html), [BERTScore](https://docs.inspiredco.ai/critique/metric_bert_score.html), and [UniEval](https://docs.inspiredco.ai/critique/metric_uni_eval.html) (you can choose [other metrics](https://docs.inspiredco.ai/critique/metrics.html) too):",然后运行以下代码来设置配置并计算[ROUGE](https://docs.inspiredco.ai/critique/metric_rouge.html)、[chrf](https://docs.inspiredco.ai/critique/metric_chrf.html)、[BERTScore](https://docs.inspiredco.ai/critique/metric_bert_score.html）和[UniEval](https://docs.inspiredco.ai/critique/metric_uni_eval.html)（您也可以选择[其他度量
"Finally, we can print out the results. We can see that overall the scores are higher when the output is semantically correct, and also when the output closely matches with the gold-standard answer.",最后，我们可以打印出结果。我们可以看到，总体而言，当输出在语义上正确时，以及当输出与黄金标准答案非常匹配时，分数会更高。
Generic Agent Evaluation,通用代理评估
Good evaluation is key for quickly iterating on your agent's prompts and tools. Here we provide an example of how to use the TrajectoryEvalChain to evaluate your agent.,良好的评估是快速重复代理提示和工具的关键。这里我们提供了一个如何使用TrajectoryEvalChain来评估您的代理的示例。
Let's start by defining our agent.,让我们从定义我们的代理开始。
Testing the Agent,测试代理
Now let's try our agent out on some example queries.,现在，让我们在一些示例查询中尝试我们的代理。
This looks good! Let's try it out on another query.,看上去很不错！让我们在另一个查询中尝试一下。
This doesn't look so good. Let's try running some evaluation.,这看起来不太好。让我们试着做一些评估。
Evaluating the Agent,评估代理
Let's start by defining the TrajectoryEvalChain.,让我们从定义轨迹评估链开始。
Let's try evaluating the first query.,让我们尝试评估第一个查询。
That seems about right. Let's try the second query.,这似乎是对的。让我们尝试第二个查询。
"That also sounds about right. In conclusion, the TrajectoryEvalChain allows us to use GPT-4 to score both our agent's outputs and tool use in addition to giving us the reasoning behind the evaluation.",这听起来也差不多。总之，TrajectoryEvalChain允许我们使用GPT-4对代理的输出和工具使用进行评分，并为我们提供评估背后的推理。
Using Hugging Face Datasets,使用拥抱脸数据集
"This example shows how to use Hugging Face datasets to evaluate models. Specifically, we show how to load examples to evaluate models on from Hugging Face's dataset package.",这个例子展示了如何使用拥抱脸数据集来评估模型。具体来说，我们展示了如何从Hugging Face的数据集包中加载示例来评估模型。
"For demonstration purposes, we will just evaluate a simple question answering system.",出于演示目的，我们将只评估一个简单的问答系统。
"Now we load a dataset from Hugging Face, and then convert it to a list of dictionaries for easier usage.",现在，我们从拥抱脸加载一个数据集，然后将其转换为字典列表，以便于使用。
Predictions,预测
We can now make and inspect the predictions for these questions.,我们现在可以对这些问题进行预测并进行检验。
"Because these answers are more complex than multiple choice, we can now evaluate their accuracy using a language model.",因为这些答案比选择题更复杂，我们现在可以使用语言模型来评估它们的准确性。
LLM Math,数学法学硕士
Evaluating chains that know how to do math.,评估知道如何做数学的链。
Now we need to create some pipelines for doing math.,现在我们需要创建一些做数学的管道。
Evaluating an OpenAPI Chain,评估OpenAPI链
"This notebook goes over ways to semantically evaluate an [OpenAPI Chain](openapi.ipynb), which calls an endpoint defined by the OpenAPI specification using purely natural language.",本笔记本介绍了从语义上评估【OpenAPI链】（openapi.ipynb），它使用纯自然语言调用由OpenAPI规范定义的端点。
Load the API Chain,加载API链
Load a wrapper of the spec (so we can work with it more easily). You can load from a url or from a local file.,加载规范的包装器（这样我们可以更容易地使用它）。您可以从url或本地文件加载。
*Optional*: Generate Input Questions and Request Ground Truth Queries,*可选*：生成输入问题并请求基本事实查询
See [Generating Test Datasets](#Generating-Test-Datasets) at the end of this notebook for more details.,有关更多详细信息，请参见本笔记本末尾的【生成测试数据集】（#生成测试数据集）。
Run the API Chain,运行API链
The two simplest questions a user of the API Chain are:,API链的用户最简单的两个问题是：
Did the chain succesfully access the endpoint?,链是否成功访问端点？
Did the action accomplish the correct result?,这个动作达到正确的结果了吗？
Evaluate the requests chain,评估请求链
The API Chain has two main components:,API链有两个主要组件：
Translate the user query to an API request (request synthesizer),将用户查询转换为API请求（请求合成器）
Translate the API response to a natural language response,将API响应转换为自然语言响应
"Here, we construct an evaluation chain to grade the request synthesizer against selected human queries",这里，我们构建一个评估链，根据选定的人工查询对请求合成器进行分级
Evaluate the Response Chain,评估响应链
The second component translated the structured API response to a natural language response. Evaluate this against the user's original question.,第二个组件将结构化API响应转换为自然语言响应。根据用户最初的问题对此进行评估。
Generating Test Datasets,生成测试数据集
"To evaluate a chain against your own endpoint, you'll want to generate a test dataset that's conforms to the API.",为了根据您自己的端点评估链，您需要生成一个符合API的测试数据集。
This section provides an overview of how to bootstrap the process.,本节概述如何引导该流程。
"First, we'll parse the OpenAPI Spec. For this example, we'll [Speak](https://www.speak.com/)'s OpenAPI specification.",首先，我们将解析OpenAPI规范。对于这个例子，我们将[Speak]（https://www.speak.com/）的OpenAPI规范。
**Now you can use the `ground_truth` as shown above in [Evaluate the Requests Chain](#Evaluate-the-requests-chain)!**,**现在您可以使用上面[Evaluate the Requests Chain](#Evaluate-the-requests-chain）中所示的“ground_truth”！**
Question Answering Benchmarking: Paul Graham Essay,问答基准：保罗·格拉厄姆论文
Here we go over how to benchmark performance on a question answering task over a Paul Graham essay.,在这里，我们将讨论如何通过保罗·格拉厄姆的一篇文章来衡量问答任务的表现。
Now we need to create some pipelines for doing question answering. Step one in that is creating an index over the data in question.,现在我们需要创建一些管道来回答问题。第一步是在有问题的数据上创建一个索引。
Question Answering Benchmarking: State of the Union Address,问答基准：国情咨文
Here we go over how to benchmark performance on a question answering task over a state of the union address.,在这里，我们将讨论如何在国情咨文中对问答任务的性能进行基准测试。
QA Generation,QA生成
"This notebook shows how to use the `QAGenerationChain` to come up with question-answer pairs over a specific document. This is important because often times you may not have data to evaluate your question-answer system over, so this is a cheap and lightweight way to generate it!",本笔记本展示了如何使用“QAGenerationChain”在特定文档上提出问答对。这一点很重要，因为通常你可能没有数据来评估你的问答系统，所以这是一种廉价而轻量级的生成方法！
"This notebook covers how to evaluate generic question answering problems. This is a situation where you have an example containing a question and its corresponding ground truth answer, and you want to measure how well the language model does at answering those questions.",本笔记本涵盖了如何评估一般问答问题。在这种情况下，您有一个包含问题及其相应的基本事实答案的示例，并且您想要测量语言模型在回答这些问题方面做得有多好。
"For demonstration purposes, we will just evaluate a simple question answering system that only evaluates the model's internal knowledge. Please see other notebooks for examples where it evaluates how the model does at question answering over data not present in what the model was trained on.",出于演示目的，我们将只评估一个简单的问答系统，该系统只评估模型的内部知识。请参阅其他笔记本中的示例，其中评估了模型在回答问题时对模型训练内容中不存在的数据的表现。
"For this purpose, we will just use two simple hardcoded examples, but see other notebooks for tips on how to get and/or generate these examples.",为此，我们将只使用两个简单的硬编码示例，但有关如何获取和/或生成这些示例的提示，请参见其他笔记本。
"We can see that if we tried to just do exact match on the answer answers (`11` and `No`) they would not match what the language model answered. However, semantically the language model is correct in both cases. In order to account for this, we can use a language model itself to evaluate the answers.",我们可以看到，如果我们试图对答案（“11”和“否”）进行精确匹配，它们将与语言模型的答案不匹配。然而，从语义上讲，语言模型在这两种情况下都是正确的。为了解释这一点，我们可以使用语言模型本身来评估答案。
Customize Prompt,自定义提示
"You can also customize the prompt that is used. Here is an example prompting it using a score from 0 to 10. The custom prompt requires 3 input variables: ""query"", ""answer"" and ""result"". Where ""query"" is the question, ""answer"" is the ground truth answer, and ""result"" is the predicted answer.",您还可以自定义所使用的提示。下面是一个使用0到10的分数提示它的例子。自定义提示需要3个输入变量：“查询”、“答案”和“结果”。其中“查询”是问题，“答案”是基本事实答案，“结果”是预测答案。
Evaluation without Ground Truth,无事实依据的评价
"Its possible to evaluate question answering systems without ground truth. You would need a `""context""` input that reflects what the information the LLM uses to answer the question. This context can be obtained by any retreival system. Here's an example of how it works:",在没有基本事实的情况下评估问答系统是可能的。您需要一个“上下文”输入来反映LLM用来回答问题的信息。这个上下文可以通过任何检索系统获得。下面是一个如何工作的示例：
Comparing to other evaluation metrics,与其他评价指标比较
"We can compare the evaluation results we get to other common evaluation metrics. To do this, let's load some evaluation metrics from HuggingFace's `evaluate` package.",我们可以将得到的评估结果与其他常见的评估指标进行比较。为此，让我们从HuggingFace的“评估”包中加载一些评估指标。
SQL Question Answering Benchmarking: Chinook,SQL问答基准测试：Chinook
Here we go over how to benchmark performance on a question answering task over a SQL database.,在这里，我们将讨论如何在SQL数据库上对问答任务的性能进行基准测试。
"This uses the example Chinook database. To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the `.db` file in a notebooks folder at the root of this repository.",这将使用示例Chinook数据库。要进行设置，请遵循https：//database.guide/2-sample-databases-sqlite/上的说明，将“.db”文件放在该存储库根目录下的笔记本文件夹中。
"Note that here we load a simple chain. If you want to experiment with more complex chains, or an agent, just create the `chain` object in a different way.",注意，这里我们加载一个简单的链。如果您想尝试更复杂的链或代理，只需以不同的方式创建“链”对象。
Now we can create a SQL database chain.,现在我们可以创建一个SQL数据库链。
"Now we can make predictions. Note that we add a try-except because this chain can sometimes error (if SQL is written incorrectly, etc)",现在我们可以做预测了。请注意，我们添加了一个try-except，因为这个链有时会出错（如果SQL编写不正确，等等）
Now we can evaluate the predictions. We can use a language model to score them programatically,现在我们可以评估预测了。我们可以用一个语言模型给它们编程打分
Extraction,提取
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/extraction),[概念指南](https://docs.langchain.com/docs/use-cases/extraction）
"Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:",大多数API和数据库仍然处理结构化信息。因此，为了更好地处理这些问题，从文本中提取结构化信息是很有用的。这方面的例子包括：
Extracting a structured row to insert into a database from a sentence,从句子中提取要插入到数据库中的结构化行
Extracting multiple rows to insert into a database from a long document,从长文档中提取要插入到数据库中的多行
Extracting the correct API parameters from a user query,从用户查询中提取正确的API参数
"This work is extremely related to [output parsing](../modules/prompts/output_parsers.rst). Output parsers are responsible for instructing the LLM to respond in a specific format. In this case, the output parsers specify the format of the data you would like to extract from the document. Then, in addition to the output format instructions, the prompt should also contain the data you would like to extract information from.",这项工作与[输出解析]（../modules/prompts/output_parsers.rst）密切相关。输出解析器负责指示LLM以特定的格式进行响应。在这种情况下，输出解析器指定您希望从文档中提取的数据的格式。然后，除了输出格式说明之外，提示符还应该包含您想要从中提取信息的数据。
"While normal output parsers are good enough for basic structuring of response data, when doing extraction you often want to extract more complicated or nested structures. For a deep dive on extraction, we recommend checking out [`kor`](https://eyurtsev.github.io/kor/), a library that uses the existing LangChain chain and OutputParser abstractions but deep dives on allowing extraction of more complicated schemas.",虽然普通的输出解析器对于响应数据的基本结构化来说已经足够好了，但是在进行提取时，您通常希望提取更复杂或嵌套的结构。要深入了解提取，我们建议查看【`kor`】（https：//eyurtsev.github.io/kor/），这是一个使用现有LangChain链和OutputParser抽象的库，但深入了解了允许提取更复杂的模式。
Personal Assistants (Agents),个人助理（代理人）
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/personal-assistants),[概念指南](https://docs.langchain.com/docs/use-cases/personal-assistants）
"We use ""personal assistant"" here in a very broad sense. Personal assistants have a few characteristics:",我们在这里使用的“私人助理”是非常广义的。个人助理有几个特点：
They can interact with the outside world,他们可以与外界互动
They have knowledge of your data,他们知道你的数据
They remember your interactions,他们记得你的互动
Really all of the functionality in LangChain is relevant for building a personal assistant. Highlighting specific parts:,实际上，LangChain中的所有功能都与构建个人助理相关。突出显示特定部分：
[Agent Documentation](../modules/agents.rst) (for interacting with the outside world),[代理文档](../modules/agents.rst)（用于与外部世界交互）
[Index Documentation](../modules/indexes.rst) (for giving them knowledge of your data),[索引文档](../modules/indexes.rst)（用于让他们了解您的数据）
[Memory](../modules/memory.rst) (for helping them remember interactions),[Memory](../modules/memory.rst)（用于帮助他们记住交互）
Specific examples of this include:,这方面的具体例子包括：
[AI Plugins](agents/custom_agent_with_plugin_retrieval.ipynb): an implementation of an agent that is designed to be able to use all AI Plugins.,[AI Plugins]（agents/custom_agent_with_plugin_retrieval.ipynb）：一个被设计为能够使用所有AI插件的代理的实现。
[Plug-and-PlAI (Plugins Database)](agents/custom_agent_with_plugin_retrieval_using_plugnplai.ipynb): an implementation of an agent that is designed to be able to use all AI Plugins retrieved from PlugNPlAI.,[Plug-and-PlAI（插件数据库）]（agents/custom_agent_with_plugin_retrieval_using_plugnplai.ipynb）：一个代理的实现，它被设计成能够使用从plugnplai检索到的所有AI插件。
[Wikibase Agent](agents/wikibase_agent.ipynb): an implementation of an agent that is designed to interact with Wikibase.,[Wikibase Agent](agents/wikibase_agent.ipynb)：旨在与Wikibase交互的代理的实现。
[Sales GPT](agents/sales_agent_with_context.ipynb): This notebook demonstrates an implementation of a Context-Aware AI Sales agent.,[Sales GPT]（agents/sales_agent_with_context.ipynb）：本笔记本演示了一个上下文感知AI销售代理的实现。
Question Answering over Docs,通过文档回答问题
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-docs),[概念指南](https://docs.langchain.com/docs/use-cases/qa-docs）
"Question answering in this context refers to question answering over your document data. For question answering over other types of data, please see other sources documentation like [SQL database Question Answering](./tabular.md) or [Interacting with APIs](./apis.md).",在这种情况下，问答是指对文档数据的问答。有关其他类型数据的问题回答，请参见其他源文档，如【SQL数据库问题回答】（。/表格式。md）或【与API交互】（。/API.md）。
"For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).",对于许多文档的问题回答，您几乎总是希望为数据创建一个索引。这可以用来智能地访问给定问题的最相关的文档，使您避免必须将所有文档传递给LLM（节省您的时间和金钱）。
"See [this notebook](../modules/indexes/getting_started.ipynb) for a more detailed introduction to this, but for a super quick start the steps involved are:",请参阅[本笔记本]（../modules/indexes/getting_started.ipynb）以获得更详细的介绍，但要快速入门，所涉及的步骤如下：
**Load Your Documents**,**加载您的文档**
See [here](../modules/indexes/document_loaders.rst) for more information on how to get started with document loading.,有关如何开始文档加载的更多信息，请参见[此处](../modules/indexes/document_loaders.rst）。
**Create Your Index**,**创建索引**
The best and most popular index by far at the moment is the VectorStore index.,目前最好和最受欢迎的索引是VectorStore索引。
**Query Your Index**,**查询索引**
"Alternatively, use `query_with_sources` to also get back the sources involved",或者，使用“query_with_sources”也可以获取所涉及的源
"Again, these high level interfaces obfuscate a lot of what is going on under the hood, so please see [this notebook](../modules/indexes/getting_started.ipynb) for a lower level walkthrough.",同样，这些高级接口混淆了底层的许多内容，因此请参阅[本笔记本]（../modules/indexes/getting_started.ipynb）以获得较低级别的演练。
Document Question Answering,文档问答
"Question answering involves fetching multiple documents, and then asking a question of them. The LLM response will contain the answer to your question, based on the content of the documents.",问答包括获取多个文档，然后向它们提问。根据文档的内容，LLM回复将包含您的问题的答案。
The recommended way to get started using a question answering chain is:,开始使用问答链的推荐方法是：
[Question Answering Notebook](../modules/chains/index_examples/question_answering.ipynb): A notebook walking through how to accomplish this task.,[Question Answering Notebook](../modules/chains/index_examples/question_answering.ipynb)：介绍如何完成此任务的笔记本。
"[VectorDB Question Answering Notebook](../modules/chains/index_examples/vector_db_qa.ipynb): A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don't want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.",[VectorDB问答笔记本](../modules/chains/index_examples/vector_db_qa.ipynb)：一个笔记本，介绍如何通过向量数据库进行问答。当您有很多文档，并且您不想将它们都传递给LLM，而是首先想对嵌入进行一些语义搜索时，这通常很有用。
Adding in sources,添加源
"There is also a variant of this, where in addition to responding with the answer the language model will also cite its sources (eg which of the documents passed in it used).",还有一个变体，除了回答之外，语言模型还会引用它的来源（例如，它使用了哪个传入的文档）。
The recommended way to get started using a question answering with sources chain is:,开始使用源链问答的推荐方法是：
[QA With Sources Notebook](../modules/chains/index_examples/qa_with_sources.ipynb): A notebook walking through how to accomplish this task.,[QA With Sources Notebook](../modules/chains/index_examples/qa_with_sources.ipynb)：介绍如何完成此任务的笔记本。
"[VectorDB QA With Sources Notebook](../modules/chains/index_examples/vector_db_qa_with_sources.ipynb): A notebook walking through how to do question answering with sources over a vector database. This can often be useful for when you have a LOT of documents, and you don't want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.",[VectorDB QA With Sources Notebook](../modules/chains/index_examples/vector_db_qa_with_sources.ipynb)：一个笔记本，介绍如何通过向量数据库使用源回答问题。当您有很多文档，并且您不想将它们都传递给LLM，而是首先想对嵌入进行一些语义搜索时，这通常很有用。
Additional Related Resources,其他相关资源
"[Utilities for working with Documents](/modules/utils/how_to_guides.rst): Guides on how to use several of the utilities which will prove helpful for this task, including Text Splitters (for splitting up long documents) and Embeddings & Vectorstores (useful for the above Vector DB example).",[Utilities for working with Documents]（/modules/utils/how_to_guides.rst）：介绍如何使用几个实用程序的指南，这些实用程序将证明对此任务很有帮助，包括文本拆分器（用于拆分长文档）和Embeddings&Vectorstores（对上面的Vector DB示例很有用）。
[CombineDocuments Chains](/modules/indexes/combine_docs.md): A conceptual overview of specific types of chains by which you can accomplish this task.,[CombineDocuments Chains](/modules/indexes/combine_docs.md)：特定类型链的概念性概述，通过这些链可以完成此任务。
End-to-end examples,端到端示例
"For examples to this done in an end-to-end manner, please see the following resources:",有关以端到端方式完成此操作的示例，请参见以下资源：
[Semantic search over a group chat with Sources Notebook](question_answering/semantic-search-over-chat.ipynb): A notebook that semantically searches over a group chat conversation.,[Semantic search over a group chat with Sources Notebook](question_answering/Semantic Search-over-Chat.ipynb)：对群聊对话进行语义搜索的笔记本。
Question answering over a group chat messages,通过群聊消息回答问题
"In this tutorial, we are going to use Langchain + Deep Lake with GPT4 to semantically search and ask questions over a group chat.",在本教程中，我们将使用Langchain+Deep Lake和GPT 4在群聊中进行语义搜索和提问。
View a working demo [here](https://twitter.com/thisissukh_/status/1647223328363679745),查看工作演示[此处](https://twitter.com/thisissukh_/status/1647223328363679745）
1. Install required packages,1.安装所需的软件包
2. Add API keys,2.添加API密钥
2. Create sample data,2.创建示例数据
You can generate a sample group chat conversation using ChatGPT with this prompt:,您可以使用ChatGPT生成示例群聊对话，提示如下：
I've already generated such a chat in `messages.txt`. We can keep it simple and use this for our example.,我已经在“messages.txt”中生成了这样的聊天。我们可以保持简单，并以此为例。
3. Ingest chat embeddings,3.摄取聊天嵌入
"We load the messages in the text file, chunk and upload to ActiveLoop Vector store.",我们将消息加载到文本文件中，分块并上传到ActiveLoop矢量存储。
4. Ask questions,4.提问
Now we can ask a question and get an answer back with a semantic search:,现在我们可以提出一个问题，并通过语义搜索得到答案：
Summarization,总结
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/summarization),[概念指南](https://docs.langchain.com/docs/use-cases/summarization）
Summarization involves creating a smaller summary of multiple longer documents. This can be useful for distilling long documents into the core pieces of information.,摘要包括创建多个较长文档的较小摘要。这对于将长文档提炼为核心信息非常有用。
The recommended way to get started using a summarization chain is:,开始使用摘要链的推荐方法是：
[Summarization Notebook](../modules/chains/index_examples/summarize.ipynb): A notebook walking through how to accomplish this task.,[Summarization Notebook](../modules/chains/index_examples/summarize.ipynb)：介绍如何完成此任务的笔记本。
"[Utilities for working with Documents](../reference/utils.rst): Guides on how to use several of the utilities which will prove helpful for this task, including Text Splitters (for splitting up long documents).",[用于处理文档的实用程序]（../reference/utils.rst）：介绍如何使用几个实用程序的指南，这些实用程序将证明对此任务很有帮助，包括文本拆分器（用于拆分长文档）。
Querying Tabular Data,查询表格数据
[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-tabular),[概念指南](https://docs.langchain.com/docs/use-cases/qa-tabular）
"Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables. This page covers all resources available in LangChain for working with data in this format.",大量数据和信息存储在表格数据中，无论是CSV、excel表格还是SQL表格。本页涵盖了LangChain中处理这种格式数据的所有可用资源。
Document Loading,文件加载
"If you have text data stored in a tabular format, you may want to load the data into a Document and then index it as you would other text/unstructured data. For this, you should use a document loader like the [CSVLoader](../modules/indexes/document_loaders/examples/csv.ipynb) and then you should [create an index](../modules/indexes.rst) over that data, and [query it that way](../modules/chains/index_examples/vector_db_qa.ipynb).",如果您有以表格格式存储的文本数据，您可能希望将数据加载到文档中，然后像索引其他文本/非结构化数据一样对其进行索引。为此，您应该使用类似于[CSVLoader]（../modules/indexes/document_loaders/examples/csv.ipynb）的文档加载器，然后您应该对该数据[创建索引]（../modules/indexes.rst），并[以这种方式查询]（../modules/chains/index_examples/vector_db_qa.ipynb）。
Querying,查询
"If you have more numeric tabular data, or have a large amount of data and don't want to index it, you should get started by looking at various chains and agents we have for dealing with this data.",如果您有更多的数字表格数据，或者有大量数据但不想对其进行索引，您应该从查看我们处理这些数据的各种链和代理开始。
"If you are just getting started, and you have relatively small/simple tabular data, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you  understand what is happening better.",如果您刚刚开始，并且您有相对较小/简单的表格数据，那么您应该从链开始。链是一系列预先确定的步骤，所以它们是很好的开始，因为它们给你更多的控制，让你更好地理解正在发生的事情。
[SQL Database Chain](../modules/chains/examples/sqlite.ipynb),[SQL数据库链]（../模块/链/示例/sqlite.ipynb）
"Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger databases and more complex schemas.",代理更加复杂，涉及到对LLM的多个查询，以了解要做什么。代理的缺点是你的控制力较弱。好处是它们更强大，这允许您在更大的数据库和更复杂的模式上使用它们。
[SQL Agent](../modules/agents/toolkits/examples/sql_database.ipynb),[SQL代理](../modules/agents/toolkits/examples/sql_database.ipynb）
[Pandas Agent](../modules/agents/toolkits/examples/pandas.ipynb),[Pandas代理]（../模块/代理/工具包/示例/pandas.ipynb）
[CSV Agent](../modules/agents/toolkits/examples/csv.ipynb),[CSV代理]（../模块/代理/工具包/示例/csv.ipynb）
YouTube,YouTube
This is a collection of `LangChain` tutorials and videos on `YouTube`.,这是“YouTube”上的“LangChain”教程和视频的集合。
"Introduction to LangChain with Harrison Chase, creator of LangChain",LangChain的创建者Harrison Chase介绍LangChain
"[Building the Future with LLMs, `LangChain`, & `Pinecone`](https://youtu.be/nMniwlGyX-c) by [Pinecone](https://www.youtube.com/@pinecone-io)",[用LLMs、`LangChain`、&`Pinecone`]构建未来](https://youtu.be/nMniwlGyX-c）作者：[Pinecone](https://www.youtube.com/@pinecone-io）
[LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36](https://youtu.be/lhby7Ql7hbk) by [Weaviate • Vector Database](https://www.youtube.com/@Weaviate),[LangChain and Weaviate with Harrison Chase and Bob van Luijt-Weaviate Podcast#36](https://youtu.be/LHBY 7 QL 7 HBK）by[Weaviate•Vector Database](https://www.youtube.com/@Weaviate）
[LangChain Demo + Q&A with Harrison Chase](https://youtu.be/zaYTXQFR0_s?t=788) by [Full Stack Deep Learning](https://www.youtube.com/@FullStackDeepLearning),[LangChain演示+与Harrison Chase的问答](https://youtu.be/zaytxqfr 0_s？t=788）由[Full Stack Deep Learning]提供(https://www.youtube.com/@FullStackDeepLearning）
Tutorials,教程
[LangChain Crash Course - Build apps with language models](https://youtu.be/LbT1yp6quS8) by [Patrick Loeber](https://www.youtube.com/@patloeber),[LangChain速成班-用语言模型构建应用程序](https://youtu.be/lbt 1 yp 6 qus 8）作者：[Patrick Loeber](https://www.youtube.com/@patloeber）
[LangChain for Gen AI and LLMs](https://www.youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F) by [James Briggs](https://www.youtube.com/@jamesbriggs):,[Gen AI和LLMs的LangChain](https://www.youtube.com/playlist？list=pliuou 7 oqgtliev 9 utifmm 6_4 pxg-hln 6 f）作者：[James Briggs](https://www.youtube.com/@jamesbriggs)：
#1 [Getting Started with `GPT-3` vs. Open Source LLMs](https://youtu.be/nE2skSRWTTs),#1[GPT-3与开源LLMs入门](https://youtu.be/ne 2 sksrwtts）
#2 [Prompt Templates for `GPT 3.5` and other LLMs](https://youtu.be/RflBcK0oDH0),#2[GPT 3.5和其他LLM的提示模板](https://youtu.be/rflbck 0 odh 0）
#3 [LLM Chains using `GPT 3.5` and other LLMs](https://youtu.be/S8j9Tk0lZHU),#3[使用'GPT 3.5'和其他LLM的LLM链](https://youtu.be/s 8 j 9 tk 0 lzhu）
"#4 [Chatbot Memory for `Chat-GPT`, `Davinci` + other LLMs](https://youtu.be/X05uK0TZozM)",#4[`Chat-GPT`、`Davinci`+其他LLM的聊天机器人内存](https://youtu.be/x 05 uk 0 tzozm）
#5 [Chat with OpenAI in LangChain](https://youtu.be/CnAgB3A5OlU),#5[在LangChain中与OpenAI聊天](https://youtu.be/cnagb 3 a 5 olu）
#6 [LangChain Agents Deep Dive with `GPT 3.5`](https://youtu.be/jSP-gSEyVeI),#6[LangChain Agents Deep Dive with`GPT 3.5`](https://youtu.be/jSP-gSEyVeI）
[Prompt Engineering with OpenAI's GPT-3 and other LLMs](https://youtu.be/BP9fi_0XTlw),[使用OpenAI的GPT-3和其他LLM提示工程](https://youtu.be/BP 9 FI_0 XTLW）
[LangChain 101](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5) by [Data Independent](https://www.youtube.com/@DataIndependent):,[LangChain 101](https://www.youtube.com/playlist？list=plqzxakvf 1 bpnqer 9 mlmdbntnfspzddiu 5）由[Data Independent]提供(https://www.youtube.com/@DataIndependent)：
[What Is LangChain? - LangChain + `ChatGPT` Overview](https://youtu.be/_v_fgW2SkkQ),[什么是LangChain？-LangChain+`ChatGPT`Overview](https://youtu.be/_v_fgw 2 skkq）
[Quickstart Guide](https://youtu.be/kYRB-vJFy38),[快速入门指南](https://youtu.be/kyrb-vjfy 38）
[Beginner Guide To 7 Essential Concepts](https://youtu.be/2xxziIWmaSA),[7个基本概念初学者指南](https://youtu.be/2 xxziiwmasa）
[`OpenAI` + `Wolfram Alpha`](https://youtu.be/UijbzCIJ99g),[`OpenAI`+`Wolfram Alpha`](https://youtu.be/uijbzcij 99 g）
[Ask Questions On Your Custom (or Private) Files](https://youtu.be/EnT-ZTrcPrg),[询问有关自定义（或私有）文件的问题](https://youtu.be/EnT-ZTrcPrg）
[Connect `Google Drive Files` To `OpenAI`](https://youtu.be/IqqHqDcXLww),[将“Google Drive Files”连接到“OpenAI”](https://youtu.be/IqqHqDcXLww）
[`YouTube Transcripts` + `OpenAI`](https://youtu.be/pNcQ5XXMgH4),[`YouTube抄本`+`OpenAI`](https://youtu.be/pncq 5 xxmgh 4）
[Question A 300 Page Book (w/ `OpenAI` + `Pinecone`)](https://youtu.be/h0DHDp1FbmQ),[问题一本300页的书(w/`OpenAI`+`Pinecone`)](https://youtu.be/h 0 dhdp 1 fbmq）
[Workaround `OpenAI's` Token Limit With Chain Types](https://youtu.be/f9_BWhCI4Zo),[解决方法'OpenAI's链类型的令牌限制](https://youtu.be/f 9_bwhci 4 zo）
[Build Your Own OpenAI + LangChain Web App in 23 Minutes](https://youtu.be/U_eV8wfMkXU),[在23分钟内构建您自己的OpenAI+LangChain Web应用程序](https://youtu.be/u_ev 8 wfmkxu）
[Working With The New `ChatGPT API`](https://youtu.be/e9P7FLi5Zy8),[使用新的'ChatGPT API'](https://youtu.be/e 9 p 7 fli 5 zy 8）
[OpenAI + LangChain Wrote Me 100 Custom Sales Emails](https://youtu.be/y1pyAQM-3Bo),[OpenAI+LangChain给我写了100封定制销售邮件](https://youtu.be/y 1 pyaqm-3 bo）
[Structured Output From `OpenAI` (Clean Dirty Data)](https://youtu.be/KwAXfey-xQk),[“OpenAI”的结构化输出（清除脏数据）](https://youtu.be/KwAXfey-xQk）
"[Connect `OpenAI` To +5,000 Tools (LangChain + `Zapier`)](https://youtu.be/7tNm0yiDigU)",[将'OpenAI'连接到+5000个工具(LangChain+`Zapier`)](https://youtu.be/7 tnm 0 yidigu）
[Use LLMs To Extract Data From Text (Expert Mode)](https://youtu.be/xZzvwR9jdPA),[使用LLMs从文本中提取数据（专家模式）](https://youtu.be/xzzvwr 9 JDPA）
[LangChain How to and guides](https://www.youtube.com/playlist?list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ) by [Sam Witteveen](https://www.youtube.com/@samwitteveenai):,[LangChain How to and guides](https://www.youtube.com/playlist？list=pl 8 motc 6 aqftk 1 bs 42 ew 45 kwybyj 4 jodiz)，作者[Sam Witteveen](https://www.youtube.com/@samwitteveenai)：
[LangChain Basics - LLMs & PromptTemplates with Colab](https://youtu.be/J_0qvRt4LNk),[LangChain Basics-LLMs&PromptTemplates with Colab](https://youtu.be/j_0 qvrt 4 lnk）
[LangChain Basics - Tools and Chains](https://youtu.be/hI2BY7yl_Ac),[LangChain基础知识-工具和链](https://youtu.be/hi 2 x 7 yl_ac）
[`ChatGPT API` Announcement & Code Walkthrough with LangChain](https://youtu.be/phHqvLHCwH4),[`ChatGPT API`公告&使用LangChain的代码演练](https://youtu.be/phhqvlhcwh 4）
[Conversations with Memory (explanation & code walkthrough)](https://youtu.be/X550Zbz_ROE),[与内存对话（解释和代码演练）](https://youtu.be/x 550 zbz_roe）
[Chat with `Flan20B`](https://youtu.be/VW5LBavIfY4),[与‘flan 20 b’聊天](https://youtu.be/vw 5 lbavify 4）
[Using `Hugging Face Models` locally (code walkthrough)](https://youtu.be/Kn7SX2Mx_Jk),[在本地使用“拥抱面部模型”（代码演练）](https://youtu.be/kn 7 sx 2 mx_jk）
[`PAL` : Program-aided Language Models with LangChain code](https://youtu.be/dy7-LvDu-3s),[`PAL`:具有LangChain代码的程序辅助语言模型](https://youtu.be/dy 7-lvdu-3 s）
[Building a Summarization System with LangChain and `GPT-3` - Part 1](https://youtu.be/LNq_2s_H01Y),[用LangChain和‘GPT-3’构建摘要系统-第1部分](https://youtu.be/lnq_2 s_h01 y）
[Building a Summarization System with LangChain and `GPT-3` - Part 2](https://youtu.be/d-yeHDLgKHw),[用LangChain和‘GPT-3’构建摘要系统-第2部分](https://youtu.be/d-yeHDLgKHw）
[Microsoft's `Visual ChatGPT` using LangChain](https://youtu.be/7YEiEyfPF5U),[Microsoft使用LangChain的“Visual ChatGPT”](https://youtu.be/7 yeieyfpf 5 u）
[LangChain Agents - Joining Tools and Chains with Decisions](https://youtu.be/ziu87EXZVUE),[LangChain Agents-通过决策连接工具和链](https://youtu.be/ziu 87 exzvue）
[Comparing LLMs with LangChain](https://youtu.be/rFNG0MIEuW0),[将LLMs与LangChain进行比较](https://youtu.be/rfng 0 mieuw 0）
[Using `Constitutional AI` in LangChain](https://youtu.be/uoVqNFDwpX4),[在LangChain中使用‘宪法AI’](https://youtu.be/uovqnfdwpx 4）
[Talking to `Alpaca` with LangChain - Creating an Alpaca Chatbot](https://youtu.be/v6sF8Ed3nTE),[用LangChain与“羊驼”交谈-创建羊驼聊天机器人](https://youtu.be/v 6 sf 8 ed 3 nte）
[Talk to your `CSV` & `Excel` with LangChain](https://youtu.be/xQ3mZhw69bc),[使用LangChain与您的“CSV”和“Excel”对话](https://youtu.be/xq 3 mzhw 69 bc）
[`BabyAGI`: Discover the Power of Task-Driven Autonomous Agents!](https://youtu.be/QBcDLSE2ERA),[`BabyAGI`:发现任务驱动的自主代理的强大功能！](https://youtu.be/qbcdlse 2时代）
[Improve your `BabyAGI` with LangChain](https://youtu.be/DRgPyOXZ-oE),[使用LangChain改进您的“BabyAGI”](https://youtu.be/DRgPyOXZ-oE）
[LangChain](https://www.youtube.com/playlist?list=PLVEEucA9MYhOu89CX8H3MBZqayTbcCTMr) by [Prompt Engineering](https://www.youtube.com/@engineerprompt):,[LangChain](https://www.youtube.com/playlist？list=plveeuca 9 myhou 89 cx 8 h 3 mbzqaytbcctmr）由[Prompt Engineering]提供(https://www.youtube.com/@engineerprompt)：
[LangChain Crash Course - All You Need to Know to Build Powerful Apps with LLMs](https://youtu.be/5-fc4Tlgmro),[LangChain速成班-使用LLMs构建功能强大的应用程序所需了解的一切](https://youtu.be/5-FC 4 TLGMro）
[Working with MULTIPLE `PDF` Files in LangChain: `ChatGPT` for your Data](https://youtu.be/s5LhRdh5fu4),[使用LangChain中的多个“PDF”文件：为您的数据“ChatGPT”](https://youtu.be/s 5 lhrdh 5 fu 4）
[`ChatGPT` for YOUR OWN `PDF` files with LangChain](https://youtu.be/TLf90ipMzfE),[“ChatGPT”用于使用LangChain的您自己的“PDF”文件](https://youtu.be/tlf 90 ipmzfe）
[Talk to YOUR DATA without OpenAI APIs: LangChain](https://youtu.be/wrD-fZvT6UI),[不使用OpenAI API与数据对话：LangChain](https://youtu.be/wrd-fzvt 6 UI）
LangChain by [Chat with data](https://www.youtube.com/@chatwithdata),LangChain by[Chat with data](https://www.youtube.com/@chatwithdata）
[LangChain Beginner's Tutorial for `Typescript`/`Javascript`](https://youtu.be/bH722QgRlhQ),[“Typescript”/“Javascript”的LangChain初学者教程](https://youtu.be/BH 722 QGRLHQ）
[`GPT-4` Tutorial: How to Chat With Multiple `PDF` Files (~1000 pages of Tesla's 10-K Annual Reports)](https://youtu.be/Ix9WIZpArm0),[“GPT-4”教程：如何与多个“PDF”文件聊天（特斯拉10-K年度报告约1000页）](https://youtu.be/ix 9 wizparm 0）
[`GPT-4` & LangChain Tutorial: How to Chat With A 56-Page `PDF` Document (w/`Pinecone`)](https://youtu.be/ih9PBGVVOO4),[`GPT-4`&LangChain教程：如何使用56页的“PDF”文档聊天(w/`Pinecone`)](https://youtu.be/ih 9 pbgvvoo 4）
Videos (sorted by views),视频（按视图排序）
[Building AI LLM Apps with LangChain (and more?) - LIVE STREAM](https://www.youtube.com/live/M-2Cj_2fzWI?feature=share) by [Nicholas Renotte](https://www.youtube.com/@NicholasRenotte),[使用LangChain构建AI LLM应用程序（以及更多？）-直播流](https://www.youtube.com/LIVE/m-2 CJ_2 FZWI？feature=share）作者[Nicholas Renotte](https://www.youtube.com/@NicholasRenotte）
[First look - `ChatGPT` + `WolframAlpha` (`GPT-3.5` and Wolfram|Alpha via LangChain by James Weaver)](https://youtu.be/wYGbY811oMo) by [Dr Alan D. Thompson](https://www.youtube.com/@DrAlanDThompson),[First look-`ChatGPT`+`WolframAlpha`(`GPT-3.5`和WolframAlpha via LangChain by James Weaver)](https://youtu.be/wygby 811 omo）作者[Dr Alan D.Thompson](https://www.youtube.com/@DrAlanDThompson）
[LangChain explained - The hottest new Python framework](https://youtu.be/RoR4XJw8wIc) by [AssemblyAI](https://www.youtube.com/@AssemblyAI),[LangChain解释-最热门的新Python框架](https://youtu.be/ror 4 xjw 8 wic）由[AssemblyAI](https://www.youtube.com/@AssemblyAI）
"[Chatbot with INFINITE MEMORY using `OpenAI` & `Pinecone` - `GPT-3`, `Embeddings`, `ADA`, `Vector DB`, `Semantic`](https://youtu.be/2xNzB7xq8nk) by [David Shapiro ~ AI](https://www.youtube.com/@DavidShapiroAutomator)",[具有无限内存的聊天机器人，使用‘OpenAI’和‘Pinecone’-‘GPT-3’，‘Embeddings’，‘ADA’，‘Vector DB’，‘Semantic`](https://youtu.be/2 xnzb 7 xq 8 nk）作者：[David Shapiro~AI](https://www.youtube.com/@DavidShapiroAutomator）
[LangChain for LLMs is... basically just an Ansible playbook](https://youtu.be/X51N9C-OhlE) by [David Shapiro ~ AI](https://www.youtube.com/@DavidShapiroAutomator),[LangChain for LLMs是...基本上只是一个可回答的剧本]（https://youtu.be/x 51 n 9 c-ohle）作者：[David Shapiro~AI]（https://www.youtube.com/@DavidShapiroAutomator）
[Build your own LLM Apps with LangChain & `GPT-Index`](https://youtu.be/-75p09zFUJY) by [1littlecoder](https://www.youtube.com/@1littlecoder),[使用LangChain&`GPT-Index`]构建您自己的LLM应用程序](https://youtu.be/-75 p 09 zfujy）由[1 littlecoder]提供(https://www.youtube.com/@1 littlecoder）
[`BabyAGI` - New System of Autonomous AI Agents with LangChain](https://youtu.be/lg3kJvf1kXo) by [1littlecoder](https://www.youtube.com/@1littlecoder),[`BabyAGI`-具有LangChain的自主AI代理的新系统](https://youtu.be/lg3kJvf1kXo）由[1littlecoder]提供(https://www.youtube.com/@1littlecoder）
[Run `BabyAGI` with Langchain Agents (with Python Code)](https://youtu.be/WosPGHPObx8) by [1littlecoder](https://www.youtube.com/@1littlecoder),[使用Langchain代理运行'BabyAGI'（使用Python代码)](https://youtu.be/wospghpobx 8）由[1 LittleCoder](https://www.youtube.com/@1 LittleCoder）执行
[How to Use Langchain With `Zapier` | Write and Send Email with GPT-3 | OpenAI API Tutorial](https://youtu.be/p9v2-xEa9A0) by [StarMorph AI](https://www.youtube.com/@starmorph),[如何在‘Zapier’中使用Langchain使用GPT-3 OpenAI API编写和发送电子邮件教程](https://youtu.be/p 9 v 2-xea 9 a 0）作者[StarMorph AI](https://www.youtube.com/@StarMorph）
[Use Your Locally Stored Files To Get Response From GPT - `OpenAI` | Langchain | Python](https://youtu.be/NC1Ni9KS-rk) by [Shweta Lodha](https://www.youtube.com/@shweta-lodha),[使用本地存储的文件从GPT-`OpenAI`Langchain Python获得响应](https://youtu.be/nc 1 ni 9 ks-rk）by[Shweta Lodha](https://www.youtube.com/@shweta-lodha）
"[`Langchain JS` | How to Use GPT-3, GPT-4 to Reference your own Data | `OpenAI Embeddings` Intro](https://youtu.be/veV2I-NEjaM) by [StarMorph AI](https://www.youtube.com/@starmorph)",[`Langchain JS`How to Use GPT-3，GPT-4 to Reference your自己的数据`OpenAI Embeddings`Intro](https://youtu.be/vev 2 i-nejam）作者：[StarMorph AI](https://www.youtube.com/@StarMorph）
[The easiest way to work with large language models | Learn LangChain in 10min](https://youtu.be/kmbS6FDQh7c) by [Sophia Yang](https://www.youtube.com/@SophiaYangDS),[使用大型语言模型的最简单方法在10分钟内学习LangChain](https://youtu.be/kmbs 6 fdqh 7 c）作者[Sophia Yang](https://www.youtube.com/@SophiaYangDS）
"[4 Autonomous AI Agents: “Westworld” simulation `BabyAGI`, `AutoGPT`, `Camel`, `LangChain`](https://youtu.be/yWbnH6inT_U) by [Sophia Yang](https://www.youtube.com/@SophiaYangDS)",[4个自主AI代理：“Westworld”模拟“BabyAGI”、“AutoGPT”、“Camel”、“LangChain”](https://youtu.be/ywbnh 6 int_u）作者：[Sophia Yang](https://www.youtube.com/@SophiaYangDS）
[AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT](https://youtu.be/J-GL0htqda8) by [tylerwhatsgood](https://www.youtube.com/@tylerwhatsgood),[AI可以搜索互联网？Langchain Agents+OpenAI ChatGPT](https://youtu.be/j-gl 0 htqda 8）作者[tylerwhatsgood](https://www.youtube.com/@tylerwhatsgood）
[`Weaviate` + LangChain for LLM apps presented by Erika Cardenas](https://youtu.be/7AGj4Td5Lgw) by [`Weaviate` • Vector Database](https://www.youtube.com/@Weaviate),[`Weaviate`•Vector Database](https://youtu.be/7 agj 4 td 5 lgw）由[`Weaviate`•Vector Database](https://www.youtube.com/@Weaviate）提供的[`Weaviate`+LLM应用程序的LangChain for Erika Cardenas演示](https://youtu.be/7 agj 4 td 5 lgw）
[Analyze Custom `CSV` Data with `GPT-4` using Langchain](https://youtu.be/Ew3sGdX8at4) by [Venelin Valkov](https://www.youtube.com/@venelin_valkov),[Venelin Valkov](https://www.youtube.com/@venelin_valkov）[使用Langchain分析带有“GPT-4”的自定义“CSV”数据](https://youtu.be/ew 3 sgdx 8 at 4）
[Langchain Overview - How to Use Langchain & `ChatGPT`](https://youtu.be/oYVYIq0lOtI) by [Python In Office](https://www.youtube.com/@pythoninoffice6568),[Langchain概述-如何使用Langchain&`ChatGPT`](https://youtu.be/oyvyiq 0 loti)[Python In Office](https://www.youtube.com/@pythoninoffice 6568）
[Custom langchain Agent & Tools with memory. Turn any `Python function` into langchain tool with Gpt 3](https://youtu.be/NIG8lXk0ULg) by [echohive](https://www.youtube.com/@echohive),[Custom langchain Agent&Tools with memory。将任何“Python函数”转换为带有Gpt 3的langchain工具](https://youtu.be/nig 8 lxk 0 ulg）由[echohive]提供(https://www.youtube.com/@echohive）
[`ChatGPT` with any `YouTube` video using langchain and `chromadb`](https://youtu.be/TQZfB2bzVwU) by [echohive](https://www.youtube.com/@echohive),[`ChatGPT`with任何`youtube’视频使用langchain和`chromadb`](https://youtu.be/tqzfb 2 bzvwu）由[echohive]提供(https://www.youtube.com/@echohive）
[How to Talk to a `PDF` using LangChain and `ChatGPT`](https://youtu.be/v2i1YDtrIwk) by [Automata Learning Lab](https://www.youtube.com/@automatalearninglab),[如何使用LangChain和‘ChatGPT’与‘PDF’对话](https://youtu.be/v 2 i 1 ydtriwk)[Automata Learning Lab](https://www.youtube.com/@automatalearninglab）
[Langchain Document Loaders Part 1: Unstructured Files](https://youtu.be/O5C0wfsen98) by [Merk](https://www.youtube.com/@merksworld),[Langchain文档加载器第1部分：非结构化文件](https://youtu.be/o 5 c 0 wfsen 98）作者[Merk](https://www.youtube.com/@merksworld）
[LangChain - Prompt Templates (what all the best prompt engineers use)](https://youtu.be/1aRu8b0XNOQ) by [Nick Daigler](https://www.youtube.com/@nick_daigs),[LangChain-提示模板（所有最好的提示工程师都使用）](https://youtu.be/1 aru 8 b 0 xnoq）作者[Nick Daigler](https://www.youtube.com/@nick_daigs）
[LangChain. Crear aplicaciones Python impulsadas por GPT](https://youtu.be/DkW_rDndts8) by [Jesús Conde](https://www.youtube.com/@0utKast),[langchain.Crear aplicaciones Python impulsadas por GPT](https://youtu.be/dkw_rdndts 8）作者[Jesús Conde](https://www.youtube.com/@0 utkast）
[Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial](https://youtu.be/fLy0VenZyGc) by [Rachel Woods](https://www.youtube.com/@therachelwoods),[在您的产品中使用GPT的最简单方法LangChain基础教程](https://youtu.be/fly 0 venzygc）作者[Rachel Woods](https://www.youtube.com/@therachelwoods）
[`BabyAGI` + `GPT-4` Langchain Agent with Internet Access](https://youtu.be/wx1z_hs5P6E) by [tylerwhatsgood](https://www.youtube.com/@tylerwhatsgood),[`BabyAGI`+`GPT-4`Langchain Agent with Internet Access](https://youtu.be/wx 1 z_hs 5 p 6 e）作者[tylerwhatsgood](https://www.youtube.com/@tylerwhatsgood）
"[Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI](https://youtu.be/mb_YAABSplk) by [Arnoldas Kemeklis](https://www.youtube.com/@processusAI)",[学习LLM代理。它实际上是如何工作的？LangChain，AutoGPT&OpenAI](https://youtu.be/mb_YAABSplk）作者：[Arnoldas Kemeklis](https://www.youtube.com/@processusAI）
[Get Started with LangChain in `Node.js`](https://youtu.be/Wxx1KUWJFv4) by [Developers Digest](https://www.youtube.com/@DevelopersDigest),[Developers Digest](https://www.youtube.com/@DevelopersDigest）的[Node.js`中的LangChain入门](https://youtu.be/wxx 1 kuwjfv 4）
[LangChain + `OpenAI` tutorial: Building a Q&A system w/ own text data](https://youtu.be/DYOU_Z0hAwo) by [Samuel Chan](https://www.youtube.com/@SamuelChan),[LangChain+`OpenAI`tutorial：Building a Q&A system w/own text data](https://youtu.be/dyou_z 0 hawo）作者[Samuel Chan](https://www.youtube.com/@SamuelChan）
[Langchain + `Zapier` Agent](https://youtu.be/yribLAb-pxA) by [Merk](https://www.youtube.com/@merksworld),[Langchain+`Zapier`Agent](https://youtu.be/yribLAb-pxA）作者[Merk](https://www.youtube.com/@merksworld）
[Connecting the Internet with `ChatGPT` (LLMs) using Langchain And Answers Your Questions](https://youtu.be/9Y0TBC63yZg) by [Kamalraj M M](https://www.youtube.com/@insightbuilder),[使用Langchain与“ChatGPT”(LLMs）连接Internet并回答您的问题](https://youtu.be/9 y 0 tbc 63 yzg）作者：[Kamalraj M M](https://www.youtube.com/@insightbuilder）
[Build More Powerful LLM Applications for Business’s with LangChain (Beginners Guide)](https://youtu.be/sp3-WLKEcBg) by[ No Code Blackbox](https://www.youtube.com/@nocodeblackbox),[使用LangChain为企业构建更强大的LLM应用程序（初学者指南）](https://youtu.be/sp3-WLKEcBg）由[无代码黑盒]提供(https://www.youtube.com/@nocodeblackbox）
